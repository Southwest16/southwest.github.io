<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hive/Spark SQL常用函数-窗口分析函数、行列转换、JSON处理</title>
      <link href="2020/10/11/Hive-Spark-SQL%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3%E5%88%86%E6%9E%90%E5%87%BD%E6%95%B0%E3%80%81%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2%E3%80%81JSON%E5%A4%84%E7%90%86/"/>
      <url>2020/10/11/Hive-Spark-SQL%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3%E5%88%86%E6%9E%90%E5%87%BD%E6%95%B0%E3%80%81%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2%E3%80%81JSON%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1 id="窗口分析函数"><a href="#窗口分析函数" class="headerlink" title="窗口分析函数"></a>窗口分析函数</h1><p>函数中用到的表数据如下图：<br><img src="01.png" alt="alt"></p><h3 id="1-分析函数"><a href="#1-分析函数" class="headerlink" title="1. 分析函数"></a>1. 分析函数</h3><ul><li>row_number()</li><li>rank()</li><li>dense_rank()</li></ul><p>这3个函数通常用在组内排序中，但实现的效果却不相同，用法如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">    <span class="keyword">name</span>,subject,score,</span><br><span class="line">    row_number() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> score) rn, </span><br><span class="line">    <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> score) rk,</span><br><span class="line">    <span class="keyword">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> score) dr</span><br><span class="line"><span class="keyword">from</span> grade;</span><br></pre></td></tr></table></figure><p>排序之后编号对比， 如下图：<br><img src="02.png" alt="alt"></p><h3 id="2-聚合函数"><a href="#2-聚合函数" class="headerlink" title="2. 聚合函数"></a>2. 聚合函数</h3><ul><li>count()组内计数</li><li>sum()组内求和</li><li>avg()组内求平均值</li><li>max()&amp;min()组内求最大最小值</li></ul><p>下面SQL以sum函数为例展示聚合函数的用法，其他函数的用法类似。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line"><span class="keyword">name</span>,subject,score,</span><br><span class="line"><span class="keyword">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> subject) <span class="keyword">as</span> sum1, <span class="comment">-- 分组内起始行到当前行的和</span></span><br><span class="line"><span class="keyword">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> subject <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> sum2, <span class="comment">--分组内当前行与前一行的和</span></span><br><span class="line"><span class="keyword">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> subject <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="number">2</span> <span class="keyword">following</span>) <span class="keyword">as</span> sum3, <span class="comment">--分组内当前行与后两行的和</span></span><br><span class="line"><span class="keyword">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> subject <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">unbounded</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> sum4, <span class="comment">--分组内起始行到当前行的和</span></span><br><span class="line"><span class="keyword">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> subject <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">unbounded</span> <span class="keyword">following</span>) <span class="keyword">as</span> sum5 <span class="comment">---分组内当前行到终止行的和</span></span><br><span class="line"><span class="keyword">from</span> grade;</span><br></pre></td></tr></table></figure><p>计算结果如下图：<br><img src="03.png" alt="alt"></p><h3 id="3-窗口函数"><a href="#3-窗口函数" class="headerlink" title="3. 窗口函数"></a>3. 窗口函数</h3><ul><li>lag(col, n, default)  表示分组内列(col)的当前行之前的第n行, default为默认值</li><li>lead(col, n, default)  表示分组内列(col)的当前行之后的第n行, default为默认值</li></ul><p>用法如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">    <span class="keyword">name</span>,subject,score,</span><br><span class="line">    lag(score, <span class="number">1</span>, <span class="number">0</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> score) lag, <span class="comment">-- lag(score, 1, 0)表示分组内列(score)的当前行之前的第1行, 0为默认值</span></span><br><span class="line">    <span class="keyword">lead</span>(score, <span class="number">1</span>, <span class="number">0</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> score) <span class="keyword">lead</span> <span class="comment">-- lead(score, 1, 0)表示分组内列(score)的当前行之后的第1行, 0为默认值</span></span><br><span class="line"><span class="keyword">from</span> grade;</span><br></pre></td></tr></table></figure><p>结果如下图：<br><img src="04.png" alt="alt"></p><ul><li>first_value(col) 组内排序第一个值</li><li>last_value(col) 组内排序最后一个值</li></ul><p>用法如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">    <span class="keyword">name</span>,subject,score,</span><br><span class="line">    <span class="keyword">first_value</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> score) f</span><br><span class="line"><span class="keyword">from</span> grade;</span><br></pre></td></tr></table></figure><p><img src="05.png" alt="alt"></p><h1 id="行列转换"><a href="#行列转换" class="headerlink" title="行列转换"></a>行列转换</h1><h3 id="1-行转列"><a href="#1-行转列" class="headerlink" title="1. 行转列"></a>1. 行转列</h3><p>原始数据如下图：<br><img src="06.png" alt="alt"><br>转换SQL：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>, collect_list(score) <span class="keyword">from</span> grade <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span>;</span><br></pre></td></tr></table></figure><p>结果：<br><img src="07.png" alt="alt"></p><h3 id="2-列转行"><a href="#2-列转行" class="headerlink" title="2. 列转行"></a>2. 列转行</h3><p>原始数据：<br><img src="08.png" alt="alt"><br>转换SQL：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>, score <span class="keyword">from</span> grade <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span> (scores) tmp <span class="keyword">as</span> score</span><br></pre></td></tr></table></figure><p>结果：<br><img src="09.png" alt="alt"></p><h1 id="JSON处理"><a href="#JSON处理" class="headerlink" title="JSON处理"></a>JSON处理</h1><h2 id="1-JSON对象"><a href="#1-JSON对象" class="headerlink" title="1. JSON对象"></a>1. JSON对象</h2><p>JSON对象的处理可以用get_json_object()函数或json_tuple()函数。</p><p>字段field的值是一个JSONObject：{“status”:0,”version”:”v1.0”}</p><ul><li>get_json_object()  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">    get_json_object(<span class="keyword">field</span>, <span class="string">&quot;$.status&quot;</span>),</span><br><span class="line">    get_json_object(<span class="keyword">field</span>, <span class="string">&quot;$.version&quot;</span>)</span><br><span class="line"><span class="keyword">from</span> db.table;</span><br></pre></td></tr></table></figure></li><li>json_tuple()<br>如果需要获取多个key的值，建议用json_tuple函数，性能优于get_json_object()。  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">    json.status, json.version</span><br><span class="line"><span class="keyword">from</span> qjdods.cif_credit_report t</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> json_tuple(report_value, <span class="string">&#x27;status&#x27;</span>, <span class="string">&#x27;version&#x27;</span>) <span class="keyword">json</span> <span class="keyword">as</span> <span class="keyword">status</span>, <span class="keyword">version</span> <span class="keyword">limit</span> <span class="number">1</span>;</span><br></pre></td></tr></table></figure><h2 id="2-JSON数组"><a href="#2-JSON数组" class="headerlink" title="2. JSON数组"></a>2. JSON数组</h2><h3 id="1-Hive-SQL实现"><a href="#1-Hive-SQL实现" class="headerlink" title="1. Hive SQL实现"></a>1. Hive SQL实现</h3>Hive中的处理思路：</li></ul><ol><li>把JSON对象之间的逗号(，)替换成特殊字符，比如^*，因为之后要以这个特殊字符串来切分</li><li>替换掉中括号([])，为空</li><li>以步骤1中的特殊字符串切分处理后的JSON数组</li><li>结合 lateral view explode()函数，使得JSON数组转成多行JSON对象</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">    <span class="keyword">id</span>, <span class="keyword">json</span></span><br><span class="line"><span class="keyword">from</span> db.table</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(</span><br><span class="line">    <span class="keyword">split</span>(</span><br><span class="line">        regexp_replace(regexp_replace(json_array, <span class="string">&quot;&#125;,&quot;</span>, <span class="string">&quot;&#125;^*^*&quot;</span>), <span class="string">&quot;\\[|\\]&quot;</span>, <span class="string">&quot;&quot;</span>), <span class="string">&quot;\\^\\*\\^\\*&quot;</span></span><br><span class="line">    )</span><br><span class="line">) t <span class="keyword">as</span> <span class="keyword">json</span></span><br></pre></td></tr></table></figure><h3 id="3-Spark-SQL实现"><a href="#3-Spark-SQL实现" class="headerlink" title="3. Spark SQL实现"></a>3. Spark SQL实现</h3><p>使用Hive SQL处理JSON数组有一个弊端，如果JSON数组里面有嵌套数组的时候，单纯的替换掉中括号得出的结果就是错误的。而Spark SQL提供了一个内建函数substring_index(str: Column, delim: String, count: Int)，这个函数可以从指定的索引位置，并指定指定分隔符来切分字符串，这样就可以实现只替换JSON数组中的首尾中括号。当然，在Hive SQL也可以自己写一个UDF来实现这个功能。</p><p>实现代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Json数组行转列</span></span><br><span class="line"><span class="function">def <span class="title">explodeFunc</span><span class="params">(spark: SparkSession, df: Dataset[Row])</span>: Dataset[Row] </span>= &#123;</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    df.select($<span class="string">&quot;user_id&quot;</span>,</span><br><span class="line">        explode(</span><br><span class="line">            split(</span><br><span class="line">                substring_index(</span><br><span class="line">                    substring_index(</span><br><span class="line">                        regexp_replace($<span class="string">&quot;json_array&quot;</span>, <span class="string">&quot;&#125;,&quot;</span>, <span class="string">&quot;&#125;^*^*&quot;</span>),</span><br><span class="line">                        <span class="string">&quot;[&quot;</span>, -<span class="number">1</span>),</span><br><span class="line">                    <span class="string">&quot;]&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="string">&quot;\\^\\*\\^\\*&quot;</span></span><br><span class="line">            )</span><br><span class="line">        ).as(<span class="string">&quot;json&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="时间处理"><a href="#时间处理" class="headerlink" title="时间处理"></a>时间处理</h1><ul><li>获取当前时间，并格式化(yyyy-MM-dd HH:mm:ss)  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from_unixtime(unix_timestamp(), &#x27;yyyy-MM-dd HH:mm:ss&#x27;)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
            <tag> Spark SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CDH集群安装</title>
      <link href="2020/10/11/CDH%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"/>
      <url>2020/10/11/CDH%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<p>在云计算时代，可能很多公司为了成本的考虑，会采用第三方厂商提供的大数据集群，比如阿里的Maxcompute、华为的FusionInsight等。但选择安装开源的软件，能让你更加清楚其内部的原理，也能更好的针对自己公司的业务需求来定制软件进行二次开发。</p><a id="more"></a><p>下面针对自己在公司安装CDH集群的经历，整理了以下安装步骤，希望能对有需要的同学有所帮助！</p><p><font color=red>注：以下步骤中，从第1步到第8步，除了元数据库的安装之外，其他步骤都是要在集群所有节点上执行的！</font></p><h1 id="1-配置主机名"><a href="#1-配置主机名" class="headerlink" title="1. 配置主机名"></a>1. 配置主机名</h1><p>在文件/etc/hosts的最后加入以下配置(集群所有节点都需要配置)<br>10.1.12.61bigdatadev01<br>10.1.12.62bigdatadev02<br>10.1.12.63bigdatadev03<br>10.1.12.64bigdatadev04<br>10.1.12.65bigdatadev05</p><p>依次修改每个节点文件/etc/hostname中的主机名为bigdata01，bigdata02…修改完之后需要重启主机reboot，才能生效。</p><h1 id="2-时间同步"><a href="#2-时间同步" class="headerlink" title="2. 时间同步"></a>2. 时间同步</h1><p>使用chrony与外网进行时间同步，不需要配置从节点向主节点同步。</p><ol><li>安装chrony服务：yum -y install chrony</li><li>启动服务：systemctl start chronyd</li><li>查看状态：systemctl status chronyd</li><li>设置开机启动：systemctl enable chronyd</li></ol><h1 id="3-ssh免密登录"><a href="#3-ssh免密登录" class="headerlink" title="3. ssh免密登录"></a>3. ssh免密登录</h1><ol><li><p>在集群各节点上产生公钥和私钥<br>ssh-keygen -t rsa<br>注：ssh-keygen为ssh生成、管理和转换认证密钥, 它支持RSA和DSA两种认证密钥。-t选项: 指定要创建的密钥类型。</p></li><li><p>拷贝被访问节点的公钥到访问节点<br>拷贝主节点的公钥到所有节点，需要主节点能访问所有节点包括主节点本身。<br>ssh-copy-id master</p><p>命令格式：ssh-copy-id [ -i [identity_file] ] [user@]machine<br>ssh-copy-id是把本地主机的公钥复制到远程主机的authorized_keys文件上, 也会给远程主机的用户主目录(home)和~/.ssh和~/.ssh/authorized_keys设置合适的权限。-i选项用来把本地的ssh公钥文件安装到远程主机对应账户下。<br>例如：ssh-copy-id user@server 或 ssh-copy-id -i ~/.ssh/id_rsa.pub user@server</p></li></ol><h1 id="4-关闭防火墙"><a href="#4-关闭防火墙" class="headerlink" title="4. 关闭防火墙"></a>4. 关闭防火墙</h1><ol><li>查看防火墙服务状态: systemctl status firewalld</li><li>关闭防火墙: systemctl stop firewalld</li><li>禁止开机启动：systemctl disable firewalld</li></ol><h1 id="5-禁用SELinux"><a href="#5-禁用SELinux" class="headerlink" title="5. 禁用SELinux"></a>5. 禁用SELinux</h1><ol><li>查看SElLinux状态: sestatus -v 或 getenforce</li><li>永久关闭SELinux：<ol><li>编辑vi /etc/selinux/config</li><li>修改SELINUX=disabled</li><li>重启主机</li></ol></li></ol><h1 id="6-安装Java环境"><a href="#6-安装Java环境" class="headerlink" title="6. 安装Java环境"></a>6. 安装Java环境</h1><p>查看是否安装了jdk：rpm -qa | grep jdk<br>如果没安装，则通过rpm安装JDK：rpm -ivh jdk-8u172-linux-x64.rpm </p><h1 id="7-元数据库安装"><a href="#7-元数据库安装" class="headerlink" title="7. 元数据库安装"></a>7. 元数据库安装</h1><h2 id="卸载mariadb"><a href="#卸载mariadb" class="headerlink" title="卸载mariadb"></a>卸载mariadb</h2><p>centos默认安装mariadb，需要先卸载以避免冲突。</p><ol><li>查看已安装MariaDB相关包<br>rpm -qa | grep -i mariadb</li><li>查看已安装的MariaDB相关yum包，包需要根据rpm命令的结果判断<br>yum list mariadb-libs</li><li>移除已安装的MariaDB相关的yum包，包名需根据yum list命令结果判断<br>yum remove mariadb-libs</li></ol><h2 id="安装MySQL"><a href="#安装MySQL" class="headerlink" title="安装MySQL"></a>安装MySQL</h2><ol><li><p>下载MySQL rpm包<br>官网下载地址：<a href="https://dev.mysql.com/downloads/mysql/5.7.html#downloads">https://dev.mysql.com/downloads/mysql/5.7.html#downloads</a></p></li><li><p>依次执行以下命令(包之间有前后依赖关系，务必按以下顺序安装)<br> rpm -ivh mysql-community-common-5.7.17-1.el7.x86_64.rpm<br> rpm -ivh mysql-community-libs-5.7.17-1.el7.x86_64.rpm<br> rpm -ivh mysql-community-client-5.7.17-1.el7.x86_64.rpm<br> rpm -ivh mysql-community-server-5.7.17-1.el7.x86_64.rpm</p><p> 如果执行上面第四个命令报错，执行如下命令：</p><ol><li>yum -y install perl</li><li>下载libaio rpm包：<a href="http://mirror.centos.org/centos/6/os/x86_64/Packages/libaio-0.3.107-10.el6.x86_64.rpm">http://mirror.centos.org/centos/6/os/x86_64/Packages/libaio-0.3.107-10.el6.x86_64.rpm</a></li><li>rpm -ivh libaio-0.3.107-10.el6.x86_64.rpm</li><li>yum -y install net-tools</li></ol></li><li><p>数据库初始化</p><ol><li><p>执行mysqld –initialize –user=mysql初始化，之后会在/var/log/mysqld.log中生成一个root账号密码</p></li><li><p>启动数据库 systemctl start mysqld，并设置mysql开机自启动 systemctl enable mysqld</p></li><li><p>登录 mysql -uroot -p</p></li><li><p>设置密码 alter user ‘root’@’localhost’ identified by ‘yourpasswd’;</p></li><li><p>创建集群组件所必须的元数据库，并给每个数据库设置用户名和密码<br> create database hive    default charset utf8 collate utf8_general_ci;<br> create database oozie  default charset utf8 collate utf8_general_ci;<br> create database hue    default charset utf8 collate utf8_general_ci;<br> create database amon default charset utf8 collate utf8_general_ci;</p><p> grant all on hive.* ‘hive’@’%’ identified by ‘123456’;<br> grant all on oozie .* ‘oozie ‘@’%’ identified by ‘123456’;<br> grant all on hue .* ‘hue ‘@’%’ identified by ‘123456’;<br> grant all on amon .* ‘amon ‘@’%’ identified by ‘123456’;</p></li><li><p>修改 /etc/my.cnf<br> my.cnf中配置参考<a href="https://www.cloudera.com/documentation/enterprise/5-15-x/topics/cm_ig_mysql.html#cmig_topic_5_5">https://www.cloudera.com/documentation/enterprise/5-15-x/topics/cm_ig_mysql.html#cmig_topic_5_5</a>中步骤4。</p></li></ol></li></ol><h1 id="8-安装-Cloudera-Manager"><a href="#8-安装-Cloudera-Manager" class="headerlink" title="8. 安装 Cloudera Manager"></a>8. 安装 Cloudera Manager</h1><ol><li><p>下载cloudera manger和CDH<br> 下载<a href="https://archive.cloudera.com/cm5/cm/5/">https://archive.cloudera.com/cm5/cm/5/</a>，版本选择 cloudera-manager-centos7-cm5.15.0_x86_64.tar.gz。</p><p> 下载<a href="https://archive.cloudera.com/cdh5/parcels/5.15.0/">https://archive.cloudera.com/cdh5/parcels/5.15.0/</a>，版本选择CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel 和 CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel.sha1</p></li><li><p>解压压缩包，并进行必要的配置</p><ol><li>将步骤1中下载的CM压缩包copy到/opt目录下；</li><li>解压 tar -zxvf cloudera-manager-centos7-cm5.15.0_x86_64.tar.gz；</li><li>配置CM server的主机名：vi cm-5.15.0/etc/cloudera-scm-agent/config.ini ；</li><li>在所有节点上创建用户，执行 useradd –system –home-dir /opt/cm-5.15.0/run/cloudera-scm-server/ </li></ol></li></ol><p>–no-create-home –shell=/bin/false –comment “Cloudera SCM User” cloudera-scm；<br>    5. 将mysql数据库驱动 mysql-connector-java-5.1.42-bin.jar 上传到所有节点的/opt/cm-5.15.0/share/cmf/lib/ 目录下；<br>    6. 为CM创建数据库 /opt/cm-5.15.0/share/cmf/schema/scm_prepare_database.sh  mysql -uroot<br>-pyourpasswd  scm yourscm（yourpasswd是新用户yourscm的密码）</p><h1 id="9-启动CM相关服务"><a href="#9-启动CM相关服务" class="headerlink" title="9. 启动CM相关服务"></a>9. 启动CM相关服务</h1><ol><li>集群主节点上启动Cloudera server： /opt/cm-5.15.0/etc/init.d/cloudera-scm-server start；</li><li>所有节点启动Cloudera agent：/opt/cm-5.15.0/etc/init.d/cloudera-scm-agent start；</li></ol><h1 id="10-Web端操作"><a href="#10-Web端操作" class="headerlink" title="10. Web端操作"></a>10. Web端操作</h1><ol><li><p>如果之前操作没出现异常，那么在浏览器中访问 hostname:7180（默认用户名密码都为admin）就会出现以下界面：<br><img src="01.png" alt="alt"></p></li><li><p>使用默认账户密码登录，之后出现如下界面：<br><img src="02.png" alt="alt"><br>选择免费的那个版本即可，当然也可以购买使用Cloudera提供的高级功能。</p></li><li><p>选好版本之后，点继续，出现以下界面：<br><img src="03.png" alt="alt"><br>将你配置的所有节点都勾选上，然后继续。</p></li><li><p>选择CDH版本<br><img src="04.png" alt="alt"></p></li><li><p>集群组件配置<br><img src="05.png" alt="alt"><br><img src="06.png" alt="alt"><br>因为有些组件是没必要在所有节点上都安装的，建议将组件均衡的安装到集群节点上，而不是集中在某几个节点上，否则可能会造成某些节点资源占用过多。</p></li><li><p>安装组件<br>选择好要安装的组件，点下一步，就会依次安装这些组件。如果安装过程中没出什么问题，那就大功告成了！<br>这里贴出本人在安装过程中遇到的问题：</p><ol><li>在测试Hive、Ooozie、Hue等的数据库连接时，Hue报错 Unexpected error. Unable to verify database connection<br>解决：选择把Hue安装在MySQL所安装的主机上，并安装以下包：<br>rpm -ivh mysql-community-libs-compat-5.7.20-1.el7.x86_64.rpm<br>yum install python-lxml</li><li>安装在过程中出现 java.lang.ClassNotFoundException: com.mysql.jdbc.Driver<br>将mysql驱动包<a href="https://repo1.maven.org/maven2/mysql/mysql-connector-java/5.1.42/mysql-connector-java-5.1.42.jar">mysql-connector-java-5.1.42.jar</a>拷贝到以下目录：<br>/opt/cloudera/parcels/CDH-5.15.0-1.cdh5.15.0.p0.21/lib/hive/lib<br>/opt/cloudera/parcels/CDH-5.15.0-1.cdh5.15.0.p0.21/lib/oozie/lib<br>/opt/cloudera/parcels/CDH-5.15.0-1.cdh5.15.0.p0.21/lib/oozie/libext </li><li>Cloudera Manager Web界面出现 Hue Load Balancer 运行状况不良<br>yum -y install httpd<br>yum -y install mod_ssl</li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Cloudera </tag>
            
            <tag> CDH </tag>
            
            <tag> 集群安装部署 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>手把手教你维度数据建模</title>
      <link href="2020/10/11/%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E7%BB%B4%E5%BA%A6%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1/"/>
      <url>2020/10/11/%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E7%BB%B4%E5%BA%A6%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1/</url>
      
        <content type="html"><![CDATA[<p>在这篇文章中，你将会学到如何一步步地进行维度数据建模，你将看到如何在真实的场景中使用维度模型。</p><a id="more"></a><h1 id="什么是维度数据建模"><a href="#什么是维度数据建模" class="headerlink" title="什么是维度数据建模"></a>什么是维度数据建模</h1><p>维度数据建模是在进行数仓设计时的一种数据建模方法。这种建模方法的主要目标是为了提高数据检索效率，对select查询操作进行了优化。维度数据建模最适合数仓星型模型和雪花模型。</p><p>数仓中的维度数据建模不同于ER建模(Entity-Relationship Model，关系-实体模型)，ER建模的主要目标是通过减少数据的冗余来规范化数据， 而维度数据建模使得数据一旦存储在数仓中后，能被更容易地获取。维度模型是许多OLAP系统的底层数据模型。</p><p>维度模型是被传奇人物Ralph Kimball提出的，你可以读读他的这本书<a href="https://www.amazon.in/Data-Warehouse-Toolkit-Complete-Dimensional/dp/8126544279/ref=as_li_ss_tl?ie=UTF8&keywords=kimball&qid=1470065827&ref_=sr_1_1&s=books&sr=1-1&linkCode=ll1&tag=dwgeecom-21&linkId=fa8b801a6414c3020dad1a1d29a429f6">The Data Warehouse Toolkit</a></p><h1 id="维度数据建模的步骤"><a href="#维度数据建模的步骤" class="headerlink" title="维度数据建模的步骤"></a>维度数据建模的步骤</h1><p>接下来我们通过一个示例来了解维度数据建模的步骤。场景：您希望存储某个MedPlus商店每天销售多少片paracetamol 和 diclofenac 的信息。建模过程中，所有数据都归为两类：维度表和事实表。事实表中包含度量信息，维度表中包含限定度量的信息。</p><p>下面是数据仓库维度建模示例的步骤：</p><h2 id="第一步：选择业务目标"><a href="#第一步：选择业务目标" class="headerlink" title="第一步：选择业务目标"></a>第一步：选择业务目标</h2><p>在我们的例子中，业务目标就是存储单个商店每天paracetamol 和diclofenac 的销售数据。</p><h2 id="第二步：确定粒度"><a href="#第二步：确定粒度" class="headerlink" title="第二步：确定粒度"></a>第二步：确定粒度</h2><p>粒度是表中存储的最低级别的信息。例如，如果表包含每日销售数据，则粒度为“每日”。</p><p>在我们的例子中，假设一个特定的MedPlus商店在特定的一天销售1000片paracetamol ，那么粒度是每天，而在特定的月份销售10000片，那么粒度是每月。</p><p>设定粒度信息是非常重要的，我们的例子采用的是“每日”的粒度。</p><h2 id="第三步：确定维度和维度属性"><a href="#第三步：确定维度和维度属性" class="headerlink" title="第三步：确定维度和维度属性"></a>第三步：确定维度和维度属性</h2><p>在我们的例子中，可以确定三个维度：商店、药品(paracetamol 和diclofenac)和日期。下面是维度表的结构。<br><strong>Medicine</strong><br>| SK | NAME |<br>|–| –|<br>| 1 | Paracetamol |<br>| 2 | Diclofenac |</p><p><strong>Shop</strong><br>| SK | NAME |<br>|–| –|<br>| 1 | shop1|<br>| 2 | shop2|<br>| 3 | shop3|</p><p><strong>Day</strong><br>| SK | NAME |<br>|–| –|<br>| 1 | 2019-01-01|<br>| 2 | 2019-01-02|<br>| 3 | 2019-01-03|</p><h2 id="第四步：确定事实表"><a href="#第四步：确定事实表" class="headerlink" title="第四步：确定事实表"></a>第四步：确定事实表</h2><p>事实表包含的是一些可度量的东西。例子中，药片的销售量就是一个度量，我们可以创建单独的事实表来存储这些度量。</p><p>例子中，粒度是每天销售的药片，我将Medicine、Shop和Day这三张表的SK列添加到下面的事实表中。<br><img src="01.png" alt="alt"><br>在这个例子中，我们创建了3个维表和1个事实表，维度表通过外键连接到事实表上，这个模型看起来像个星型，也被称为<strong>星型模型</strong>。</p><h1 id="维度建模的优势"><a href="#维度建模的优势" class="headerlink" title="维度建模的优势"></a>维度建模的优势</h1><ol><li>提升了数据查询效率：通过冗余存储减少了关联查询</li><li>简化了业务报表逻辑</li><li>更容易理解：维度模型中的数据不是维度就是事实</li><li>可扩展性：维度模型可以更新新的维度。</li></ol><h1 id="维度数据建模工具"><a href="#维度数据建模工具" class="headerlink" title="维度数据建模工具"></a>维度数据建模工具</h1><p>建议可以尝试被广泛使用的erwin数据模型工具。</p>]]></content>
      
      
      <categories>
          
          <category> 数仓建模 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据仓库 </tag>
            
            <tag> 维度建模 </tag>
            
            <tag> 星型模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop集群数据迁移</title>
      <link href="2020/10/11/Hadoop%E9%9B%86%E7%BE%A4%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB/"/>
      <url>2020/10/11/Hadoop%E9%9B%86%E7%BE%A4%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Python读写Phoenix</title>
      <link href="2020/10/11/Python%E8%AF%BB%E5%86%99Phoenix/"/>
      <url>2020/10/11/Python%E8%AF%BB%E5%86%99Phoenix/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>MySQL表同步到Hive</title>
      <link href="2020/10/11/MySQL%E8%A1%A8%E5%90%8C%E6%AD%A5%E5%88%B0Hive/"/>
      <url>2020/10/11/MySQL%E8%A1%A8%E5%90%8C%E6%AD%A5%E5%88%B0Hive/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>单词统计WordCount (Scala/Python/Java)</title>
      <link href="2020/10/11/%E5%8D%95%E8%AF%8D%E7%BB%9F%E8%AE%A1WordCount-Scala-Python-Java/"/>
      <url>2020/10/11/%E5%8D%95%E8%AF%8D%E7%BB%9F%E8%AE%A1WordCount-Scala-Python-Java/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>YARN/Hadoop/HDFS常用命令</title>
      <link href="2020/10/11/YARN-Hadoop-HDFS%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>2020/10/11/YARN-Hadoop-HDFS%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark connector连接器之整合读写MySQL及问题汇总</title>
      <link href="2020/10/11/Spark-connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99MySQL%E5%8F%8A%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/"/>
      <url>2020/10/11/Spark-connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99MySQL%E5%8F%8A%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>pyspark中遇到的坑 (持续更新)</title>
      <link href="2020/10/11/pyspark%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0/"/>
      <url>2020/10/11/pyspark%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>pyhive依赖模块sasl安装(pip install sasl)报错Microsoft Visual C++ 14.0 is required</title>
      <link href="2020/10/11/pyhive%E4%BE%9D%E8%B5%96%E6%A8%A1%E5%9D%97sasl%E5%AE%89%E8%A3%85-pip-install-sasl-%E6%8A%A5%E9%94%99Microsoft-Visual-C-14-0-is-required/"/>
      <url>2020/10/11/pyhive%E4%BE%9D%E8%B5%96%E6%A8%A1%E5%9D%97sasl%E5%AE%89%E8%A3%85-pip-install-sasl-%E6%8A%A5%E9%94%99Microsoft-Visual-C-14-0-is-required/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark Connector连接器之整合读写HBase</title>
      <link href="2020/10/11/Spark-Connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99HBase/"/>
      <url>2020/10/11/Spark-Connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99HBase/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink和Spark Streaming背压</title>
      <link href="2020/10/11/Flink%E5%92%8CSpark-Streaming%E8%83%8C%E5%8E%8B/"/>
      <url>2020/10/11/Flink%E5%92%8CSpark-Streaming%E8%83%8C%E5%8E%8B/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库架构、维度数据建模、雪花模型和星型模型</title>
      <link href="2020/10/11/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E3%80%81%E7%BB%B4%E5%BA%A6%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E3%80%81%E9%9B%AA%E8%8A%B1%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%98%9F%E5%9E%8B%E6%A8%A1%E5%9E%8B/"/>
      <url>2020/10/11/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E3%80%81%E7%BB%B4%E5%BA%A6%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E3%80%81%E9%9B%AA%E8%8A%B1%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%98%9F%E5%9E%8B%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark Dataframe转成HashMap</title>
      <link href="2020/10/11/Spark-Dataframe%E8%BD%AC%E6%88%90HashMap/"/>
      <url>2020/10/11/Spark-Dataframe%E8%BD%AC%E6%88%90HashMap/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark内存管理(静态内存管理和统一内存管理)</title>
      <link href="2020/10/11/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86-%E9%9D%99%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%92%8C%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
      <url>2020/10/11/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86-%E9%9D%99%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%92%8C%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark性能优化指南(官网文档)</title>
      <link href="2020/10/11/Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97-%E5%AE%98%E7%BD%91%E6%96%87%E6%A1%A3/"/>
      <url>2020/10/11/Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97-%E5%AE%98%E7%BD%91%E6%96%87%E6%A1%A3/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>JSON数据转成Spark Dataset/DataFrame</title>
      <link href="2020/10/11/JSON%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%88%90Spark-Dataset-DataFrame/"/>
      <url>2020/10/11/JSON%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%88%90Spark-Dataset-DataFrame/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>图解正则表达式中的贪婪模式和非贪婪模式</title>
      <link href="2020/10/11/%E5%9B%BE%E8%A7%A3%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%B8%AD%E7%9A%84%E8%B4%AA%E5%A9%AA%E6%A8%A1%E5%BC%8F%E5%92%8C%E9%9D%9E%E8%B4%AA%E5%A9%AA%E6%A8%A1%E5%BC%8F/"/>
      <url>2020/10/11/%E5%9B%BE%E8%A7%A3%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%B8%AD%E7%9A%84%E8%B4%AA%E5%A9%AA%E6%A8%A1%E5%BC%8F%E5%92%8C%E9%9D%9E%E8%B4%AA%E5%A9%AA%E6%A8%A1%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Impala与Hive的语法区别(持续更新中...)</title>
      <link href="2020/10/11/Impala%E4%B8%8EHive%E7%9A%84%E8%AF%AD%E6%B3%95%E5%8C%BA%E5%88%AB-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E4%B8%AD/"/>
      <url>2020/10/11/Impala%E4%B8%8EHive%E7%9A%84%E8%AF%AD%E6%B3%95%E5%8C%BA%E5%88%AB-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E4%B8%AD/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Java死锁产生的条件及死锁判断排查</title>
      <link href="2020/10/11/Java%E6%AD%BB%E9%94%81%E4%BA%A7%E7%94%9F%E7%9A%84%E6%9D%A1%E4%BB%B6%E5%8F%8A%E6%AD%BB%E9%94%81%E5%88%A4%E6%96%AD%E6%8E%92%E6%9F%A5/"/>
      <url>2020/10/11/Java%E6%AD%BB%E9%94%81%E4%BA%A7%E7%94%9F%E7%9A%84%E6%9D%A1%E4%BB%B6%E5%8F%8A%E6%AD%BB%E9%94%81%E5%88%A4%E6%96%AD%E6%8E%92%E6%9F%A5/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark DataFrame导出为Excel文件</title>
      <link href="2020/10/11/Spark-DataFrame%E5%AF%BC%E5%87%BA%E4%B8%BAExcel%E6%96%87%E4%BB%B6/"/>
      <url>2020/10/11/Spark-DataFrame%E5%AF%BC%E5%87%BA%E4%B8%BAExcel%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>15分钟搞定 Spark集群安装</title>
      <link href="2020/10/11/15%E5%88%86%E9%92%9F%E6%90%9E%E5%AE%9A-Spark%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"/>
      <url>2020/10/11/15%E5%88%86%E9%92%9F%E6%90%9E%E5%AE%9A-Spark%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<p>快速安装Spark集群。</p><a id="more"></a><p><font color=red>注：以下步骤中，从第1步到第8步，除了元数据库的安装之外，其他步骤都是要在集群所有节点上执行的！</font></p><h1 id="1-官网下载Spark安装包"><a href="#1-官网下载Spark安装包" class="headerlink" title="1. 官网下载Spark安装包"></a>1. 官网下载Spark安装包</h1><p>下载要安装版本的安装包：<a href="https://archive.apache.org/dist/spark/">https://archive.apache.org/dist/spark/</a>。</p><p>我这里下载的是：<a href="https://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.6.tgz%E3%80%82%E5%BB%BA%E8%AE%AE%E7%94%A8%E8%BF%85%E9%9B%B7%E4%B8%8B%E8%BD%BD%EF%BC%8C%E4%BC%9A%E6%AF%94%E8%BE%83%E5%BF%AB%E3%80%82">https://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.6.tgz。建议用迅雷下载，会比较快。</a></p><h1 id="2-解压安装包"><a href="#2-解压安装包" class="headerlink" title="2. 解压安装包"></a>2. 解压安装包</h1><p>将下载好的安装包上传到集群某个节点的/opt目录，解压<br>tar -zxvf spark-2.3.1-bin-hadoop2.6.tgz spark</p><h1 id="3-Spark配置"><a href="#3-Spark配置" class="headerlink" title="3. Spark配置"></a>3. Spark配置</h1><p>进入到Spark安装路径conf目录下：cd ../spark/conf/。复制以下3个文件：<br>cp spark-env.sh.template spark-env.sh<br>cp spark-defaults.conf.template spark-defaults.conf<br>cp log4j.properties.template log4j.properties(INFO改为WARN)</p><h3 id="1-配置spark-env-sh"><a href="#1-配置spark-env-sh" class="headerlink" title="1. 配置spark-env.sh"></a>1. 配置spark-env.sh</h3><p>这个文件主要是配置Spark的环境变量。</p><p>文件中添加以下环境变量，vi spark-env.sh：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/etc/hadoop/conf <span class="comment"># Hadoop配置文件所在目录</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop <span class="comment"># Hadoop安装路径</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=\<span class="variable">$HADOOP_HOME</span>/lib/native:\<span class="variable">$LD_LIBRARY_PATH</span></span><br></pre></td></tr></table></figure><p>主要让Spark找到Hadoop的安装目录。</p><h3 id="2-配置spark-defaults-conf"><a href="#2-配置spark-defaults-conf" class="headerlink" title="2. 配置spark-defaults.conf"></a>2. 配置spark-defaults.conf</h3><p>这个文件主要是一些Spark相关的配置项。<br>以下以Spark on YARN为例，vi spark-defaults.conf：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.masteryarn <span class="comment"># 指定yarn作为资源调度框架</span></span><br><span class="line">spark.eventLog.enabled<span class="literal">true</span> <span class="comment"># 启动事件日志, 以便后续日志分析</span></span><br><span class="line">spark.eventLog.dirhdfs://master-1:8021/spark_eventlog</span><br><span class="line">spark.serializerorg.apache.spark.serializer.KryoSerializer <span class="comment"># Kryo序列化性能要好于默认的Java序列化</span></span><br><span class="line">spark.driver.memory2g <span class="comment"># 指定默认Driver端内存</span></span><br><span class="line">spark.yarn.archivehdfs://master-1:8021/archive <span class="comment"># 将Spark程序运行所需的jar打包放到这个目录，避免每次提交任务都要从本地Spark安装目录下拷贝jar包</span></span><br></pre></td></tr></table></figure><p>Spark有不计其数的配置项，可根据个人需求进行配置。具体可参考官网<a href="https://spark.apache.org/docs/latest/configuration.html">https://spark.apache.org/docs/latest/configuration.html</a>。</p><h3 id="3-配置log4j-properties"><a href="#3-配置log4j-properties" class="headerlink" title="3. 配置log4j.properties"></a>3. 配置log4j.properties</h3><p>可以配置Spark任务的日志级别。</p><h1 id="4-scp安装文件"><a href="#4-scp安装文件" class="headerlink" title="4. scp安装文件"></a>4. scp安装文件</h1><p>将解压配置好的Spark安装文件拷贝到其他节点：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp username@hostname:/opt/spark username@hostname:/opt</span><br></pre></td></tr></table></figure><h1 id="5-测试"><a href="#5-测试" class="headerlink" title="5. 测试"></a>5. 测试</h1><ul><li><p>启动spark shell命令行，写一段简单的Spark程序：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark<span class="literal">-shell</span> -<span class="literal">-master</span> yarn -<span class="literal">-num</span><span class="literal">-executors</span> <span class="number">2</span> -<span class="literal">-executor</span><span class="literal">-cores</span> <span class="number">1</span> -<span class="literal">-executor</span><span class="literal">-memory</span> <span class="number">512</span>M</span><br></pre></td></tr></table></figure></li><li><p>或启动spark sql命令行，执行一些简单的SQL语句：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark<span class="literal">-sql</span> -<span class="literal">-master</span> yarn -<span class="literal">-num</span><span class="literal">-executors</span> <span class="number">2</span> -<span class="literal">-executor</span><span class="literal">-cores</span> <span class="number">1</span> -<span class="literal">-executor</span><span class="literal">-memory</span> <span class="number">512</span>M</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vim编辑器常用操作</title>
      <link href="2020/10/11/Vim%E7%BC%96%E8%BE%91%E5%99%A8%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"/>
      <url>2020/10/11/Vim%E7%BC%96%E8%BE%91%E5%99%A8%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p>Vim常用命令</p><a id="more"></a><h1 id="光标移动"><a href="#光标移动" class="headerlink" title="光标移动"></a>光标移动</h1><ul><li>快速移动到行首：^</li><li>快速移动到行尾：$</li><li>快速移动到文件第一行：连按两下g键</li><li>快速移动到文件最后一行：shift+g组合键</li></ul><h1 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h1><ol><li>非编辑模式下输入斜杠(/)，之后紧挨着输入要查找的内容</li><li>按回车键定位到要查找的内容</li><li>按n键查找下一个；shift+n键查找上一个</li></ol><h1 id="行号"><a href="#行号" class="headerlink" title="行号"></a>行号</h1><ul><li><h2 id="显示行号"><a href="#显示行号" class="headerlink" title="显示行号"></a>显示行号</h2></li></ul><ol><li>vim进入待编辑文件</li><li>输入英文的冒号</li><li>输入set number，然后按回车键即会显示行号</li></ol><ul><li><h2 id="取消行号"><a href="#取消行号" class="headerlink" title="取消行号"></a>取消行号</h2></li></ul><ol><li>非编辑模式下输入英文的冒号</li><li>输入set nonumber，然后按回车键即会取消行号</li></ol><h1 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h1><ul><li><h2 id="批量注释"><a href="#批量注释" class="headerlink" title="批量注释"></a>批量注释</h2></li></ul><ol><li>vim进入待编辑文件</li><li>键盘上下键移动到要注释文本的第一行</li><li>组合键ctrl + v 进入 VISUAL BLOCK 模式</li><li>键盘上下键移动到要注释文本的最后一行</li><li>组合键shift + i 进入编辑模式</li><li>输入注释符号(#或者//等)</li><li>连续按两次ESC键，此时会看到选中的行被注释</li></ol><ul><li><h2 id="批量取消注释"><a href="#批量取消注释" class="headerlink" title="批量取消注释"></a>批量取消注释</h2></li></ul><ol><li>vim进入待编辑文件</li><li>键盘上下键移动到要取消注释文本的第一行</li><li>组合键ctrl + v 进入 VISUAL BLOCK 模式</li><li>键盘上下键移动到要取消注释文本的最后一行</li><li>按下键盘的d键，此时会看到选中的行被取消注释</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Vim </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux Sed命令使用指南</title>
      <link href="2020/10/11/Linux-Sed%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
      <url>2020/10/11/Linux-Sed%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<h1 id="Sed命令"><a href="#Sed命令" class="headerlink" title="Sed命令"></a>Sed命令</h1><p>sed编辑器被称为流编辑器，它会执行下列操作：</p><ol><li>一次从输入读取一行数据，</li><li>根据所提供的编辑器命令匹配数据，</li><li>按照命令修改流中的数据，</li><li>将新的数据输出到STDOUT。</li></ol><p>sed命令的格式：sed options [script] [file]</p><a id="more"></a><h1 id="常用options"><a href="#常用options" class="headerlink" title="常用options"></a>常用options</h1><ul><li><p><strong>-i</strong><br>替换文件中每一行的aa为a，默认情况下它只替换每行中出现的第一处:</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">&#x27;s/aa/a/g&#x27;</span> data.txt</span><br></pre></td></tr></table></figure></li><li><p><strong>-e</strong><br>此选项可执行多个命令:</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -e <span class="string">&#x27;s/brown/green/; s/dog/cat/&#x27;</span> data.txt</span><br><span class="line">或sed -e <span class="string">&#x27;s/brown/green/ s/dog/cat/&#x27;</span> data.txt</span><br></pre></td></tr></table></figure></li><li><p><strong>-f</strong><br>此选项在sed命令中指定文件。如果有大量要处理的sed命令，那么可以将它们放进一个单独的文件中：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -f script.sed data.txt </span><br></pre></td></tr></table></figure></li></ul><h1 id="替换标记"><a href="#替换标记" class="headerlink" title="替换标记"></a>替换标记</h1><p>让替换命令能够替换一行中不同地方出现的文本，格式：s/pattern/replacement/flags。<br>有4种可用的替换标记：</p><ol><li><p><strong>数字</strong>：表明新文本将替换第几处模式匹配的地方。</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">&#x27;s/test/trial/2&#x27;</span> data.txt</span><br></pre></td></tr></table></figure></li><li><p><strong>g</strong>：表明新文本将会替换所有匹配的文本。</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">&#x27;s/test/trial/g&#x27;</span> data.txt</span><br></pre></td></tr></table></figure></li><li><p><strong>p</strong>：表明原先行的内容要打印出来。</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sed -n <span class="string">&#x27;s/test/trial/p&#x27;</span> data.txt</span><br><span class="line"></span><br><span class="line">-n表示禁止sed编辑器输出，但p替换标记会输出修改过的行，将二者配合使用的效果就是只输出被替换命令修改过的行。</span><br></pre></td></tr></table></figure></li><li><p><strong>w file</strong>：将替换的结果写带文件中。</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">&#x27;s/test/trial/w out.txt&#x27;</span> data.txt</span><br></pre></td></tr></table></figure></li></ol><h1 id="行寻址"><a href="#行寻址" class="headerlink" title="行寻址"></a>行寻址</h1><p>sed编辑器中使用的命令会作用于文本数据的所有行，如果只想讲命令作用于特定行，则必须用行寻址。</p><p>有以下两种形式：</p><ol><li>以数字形式表示行区间<ul><li>替换第二行：sed ‘2s/dog/cat/‘ data.txt</li><li>替换第二行到第三行：sed ‘2,3s/dog/cat/‘ data.txt</li><li>替换某行开始的所有行：sed ‘2,$s/dog/cat/‘ data.txt</li></ul></li><li>用文本模式来过滤行<br>sed ‘/Samantha/s/bash/csh/‘ /etc/passwd命令组合，如果需要在单行上执行多条命令，可以用花括号将多余命令组合在一起: sed  ‘2{s/fox/elephant/ s/dog/cat/}’ data.txt</li></ol><h1 id="删除操作"><a href="#删除操作" class="headerlink" title="删除操作"></a>删除操作</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">&#x27;d&#x27;</span> data.txt   流中的所有文本行都会被删除</span><br><span class="line">sed <span class="string">&#x27;3d&#x27;</span> data.txt删除第三行</span><br><span class="line">sed <span class="string">&#x27;2,3d&#x27;</span> data.txt删除第2到第三行</span><br><span class="line">sed <span class="string">&#x27;3,\$d&#x27;</span> data.txt删除第三及之后所有行</span><br><span class="line">sed <span class="string">&#x27;/dog/d&#x27;</span> data.txt删除包含匹配指定模式的行</span><br><span class="line">sed <span class="string">&#x27;/^\$/d&#x27;</span> data.txt删除空白行</span><br></pre></td></tr></table></figure><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>打印匹配行到最后一行的内容</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -n <span class="string">&#x27;/c/,$&#123;//!p&#125;&#x27;</span> file</span><br></pre></td></tr></table></figure></li><li><p>多个空格替换为指定字符(^)</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">&#x27;s/[[:space:]][[:space:]]*/^/g&#x27;</span> data.txt</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> sed </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Connector连接器之整合读写Phoenix</title>
      <link href="2020/10/11/Spark-Connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99Phoenix/"/>
      <url>2020/10/11/Spark-Connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99Phoenix/</url>
      
        <content type="html"><![CDATA[<p>Phoenix为NoSQL数据库HBase提供了标准SQL和JDBC API的强大功能，且具备完整的ACID事务处理能力。对于小数据量的查询，其性能可以达到毫秒级别；对于数千万行的数据，其性能也可以达到秒级。</p><a id="more"></a><p>要使用phoenix-spark插件，需要在项目中添加如下依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-spark<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.1-HBase-1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Spark版本为2.3.1，HBase版本为1.2.0，Phoenix版本为4.14.1。</p><h2 id="Spark加载Phoenix表"><a href="#Spark加载Phoenix表" class="headerlink" title="Spark加载Phoenix表"></a>Spark加载Phoenix表</h2><p>方法一：使用数据源API加载Phoenix表为一个DataFrame</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">readPhoenix</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val df = spark.read</span><br><span class="line">        .format(<span class="string">&quot;org.apache.phoenix.spark&quot;</span>)</span><br><span class="line">        .options(Map(<span class="string">&quot;table&quot;</span> -&gt; <span class="string">&quot;TEST&quot;</span>, <span class="string">&quot;zkUrl&quot;</span> -&gt; <span class="string">&quot;host:2181&quot;</span>))</span><br><span class="line">        .load()</span><br><span class="line"></span><br><span class="line">    df.show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>方法二：使用Configuration对象加载Phoenix表为一个DataFrame</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.phoenix.spark._</span><br><span class="line"></span><br><span class="line"><span class="function">def <span class="title">readPhoenix</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val conf = <span class="keyword">new</span> Configuration()</span><br><span class="line">    conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hostname:2181&quot;</span>)</span><br><span class="line">    val df = spark.sqlContext.phoenixTableAsDataFrame(</span><br><span class="line">        <span class="string">&quot;test&quot;</span>, <span class="comment">//表名</span></span><br><span class="line">        Seq(<span class="string">&quot;NAME&quot;</span>, <span class="string">&quot;AGE&quot;</span>), <span class="comment">//指定要加载的列名</span></span><br><span class="line">        predicate = Some(<span class="string">&quot;PHONE = 13012340000&quot;</span>), <span class="comment">//可设置where条件</span></span><br><span class="line">        conf = conf)</span><br><span class="line"></span><br><span class="line">    df.show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>方法三：使用Zookeeper URL加载Phoenix表为一个RDD</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.phoenix.spark._</span><br><span class="line"></span><br><span class="line"><span class="function">def <span class="title">readPhoenix</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val rdd =spark.sparkContext.phoenixTableAsRDD(</span><br><span class="line">        <span class="string">&quot;TEST&quot;</span>,</span><br><span class="line">        Seq(<span class="string">&quot;NAME&quot;</span>, <span class="string">&quot;AGE&quot;</span>),</span><br><span class="line">        predicate = Some(<span class="string">&quot;PHONE = 13012340000&quot;</span>),</span><br><span class="line">        zkUrl = Some(<span class="string">&quot;hostname:2181&quot;</span>) <span class="comment">//Zookeeper URL来连接Phoenix</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    rdd.map(_.get(<span class="string">&quot;&quot;</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Spark持久化数据到Phoenix"><a href="#Spark持久化数据到Phoenix" class="headerlink" title="Spark持久化数据到Phoenix"></a>Spark持久化数据到Phoenix</h2><p>创建要导入的Phoenix表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">TEST</span>(<span class="keyword">id</span> <span class="built_in">BIGINT</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> PRIMARY <span class="keyword">KEY</span>, col1 <span class="built_in">VARCHAR</span>, col2 <span class="built_in">INTEGER</span>);</span><br></pre></td></tr></table></figure><h3 id="保存RDD到Phoenix"><a href="#保存RDD到Phoenix" class="headerlink" title="保存RDD到Phoenix"></a>保存RDD到Phoenix</h3><p>代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.phoenix.spark._</span><br><span class="line"></span><br><span class="line"><span class="function">def <span class="title">writePhoenix</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val dataSet = List((<span class="number">1L</span>, <span class="string">&quot;1&quot;</span>, <span class="number">1</span>), (<span class="number">2L</span>, <span class="string">&quot;2&quot;</span>, <span class="number">2</span>), (<span class="number">3L</span>, <span class="string">&quot;3&quot;</span>, <span class="number">3</span>))</span><br><span class="line">    spark.sparkContext.parallelize(dataSet)</span><br><span class="line">        .saveToPhoenix(</span><br><span class="line">            <span class="string">&quot;TEST&quot;</span>, <span class="comment">//表名</span></span><br><span class="line">            Seq(<span class="string">&quot;ID&quot;</span>,<span class="string">&quot;COL1&quot;</span>,<span class="string">&quot;COL2&quot;</span>), <span class="comment">//列命名</span></span><br><span class="line">            zkUrl = Some(<span class="string">&quot;host:2181&quot;</span>) <span class="comment">//Zookeeper URL</span></span><br><span class="line">        )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="保存DataFrame到Phoenix"><a href="#保存DataFrame到Phoenix" class="headerlink" title="保存DataFrame到Phoenix"></a>保存DataFrame到Phoenix</h3><p>方法一：使用saveToPhoenix()方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val df: DataFrame = ...</span><br><span class="line"></span><br><span class="line">df.saveToPhoenix(Map(<span class="string">&quot;table&quot;</span> -&gt; <span class="string">&quot;TEST&quot;</span>, <span class="string">&quot;zkUrl&quot;</span> -&gt; <span class="string">&quot;host:2181&quot;</span>))</span><br></pre></td></tr></table></figure><p>方法二：使用format()方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val df: DataFrame = ...</span><br><span class="line"></span><br><span class="line">df.write</span><br><span class="line">    .format(<span class="string">&quot;org.apache.phoenix.spark&quot;</span>)</span><br><span class="line">    .mode(<span class="string">&quot;overwrite&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;table&quot;</span>, <span class="string">&quot;OUTPUT_TABLE&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;zkUrl&quot;</span>, <span class="string">&quot;host:2181&quot;</span>)</span><br><span class="line">    .save()</span><br></pre></td></tr></table></figure><p>参考</p><ol><li><a href="http://phoenix.apache.org/phoenix_spark.html">http://phoenix.apache.org/phoenix_spark.html</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
            <tag> Spark </tag>
            
            <tag> Phoenix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Connector连接器之整合读写Elasticsearch</title>
      <link href="2020/10/11/Spark-Connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99Elasticsearch/"/>
      <url>2020/10/11/Spark-Connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99Elasticsearch/</url>
      
        <content type="html"><![CDATA[<p>Elasticsearch提供了对Spark的支持，可以将ES中的索引（一行行的JSON数据）加载为RDD或DataFrame。</p><a id="more"></a><p>在使用elasticsearch-spark插件之前，需要在项目中添加依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.elasticsearch<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>elasticsearch-spark-20_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>7.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h1 id="1-ES索引加载为RDD"><a href="#1-ES索引加载为RDD" class="headerlink" title="1. ES索引加载为RDD"></a>1. ES索引加载为RDD</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.elasticsearch.spark._</span><br><span class="line"></span><br><span class="line">val spark = SparkSession</span><br><span class="line">.builder()</span><br><span class="line">.config(<span class="string">&quot;hive.metastore.uris&quot;</span>, <span class="string">&quot;thrift://master-1:9083&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//es配置</span></span><br><span class="line">.config(<span class="string">&quot;es.nodes&quot;</span>, <span class="string">&quot;node-1&quot;</span>) </span><br><span class="line">.config(<span class="string">&quot;es.port&quot;</span>, <span class="string">&quot;9200&quot;</span>)</span><br><span class="line">.config(<span class="string">&quot;pushdown&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line"></span><br><span class="line">.appName(<span class="string">&quot;Spark ES&quot;</span>)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line">val rdd = spark.sparkContext.esJsonRDD(<span class="string">&quot;index_name&quot;</span>)</span><br><span class="line"></span><br><span class="line">spark.close()</span><br></pre></td></tr></table></figure><p>配置项”es.nodes”的值只要给出ES集群中的一个数据节点即可，只要给出一个节点，ES就能找到集群中的其他节点。数据节点就是ES安装目录下配置文件(../elasticsearch-7.1.0/config/elasticsearch.yml)中”node.master”为true的节点。</p><p>Spark从ES加载出来的数据是JSON String类型的RDD，根据请求体的结构就可以取出来具体的数据。</p><h1 id="2-ES索引加载为DataFrame"><a href="#2-ES索引加载为DataFrame" class="headerlink" title="2. ES索引加载为DataFrame"></a>2. ES索引加载为DataFrame</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">val spark = SparkSession</span><br><span class="line">.builder()</span><br><span class="line">.config(<span class="string">&quot;hive.metastore.uris&quot;</span>, <span class="string">&quot;thrift://master-1:9083&quot;</span>)</span><br><span class="line">.appName(<span class="string">&quot;Spark ES&quot;</span>)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line">val options = Map(</span><br><span class="line"><span class="string">&quot;es.nodes&quot;</span> -&gt; <span class="string">&quot;node-1&quot;</span>,</span><br><span class="line"><span class="string">&quot;es.port&quot;</span> -&gt; <span class="string">&quot;9200&quot;</span>,</span><br><span class="line"><span class="string">&quot;pushdown&quot;</span> -&gt; <span class="string">&quot;true&quot;</span>)</span><br><span class="line"></span><br><span class="line">val df = spark.read.format(<span class="string">&quot;org.elasticsearch.spark.sql&quot;</span>)</span><br><span class="line">.options(options)</span><br><span class="line">.load(<span class="string">&quot;index_name&quot;</span>)</span><br><span class="line">.where(<span class="string">&quot;col = 100&quot;</span>)</span><br><span class="line">.select(<span class="string">&quot;col1&quot;</span>, <span class="string">&quot;col2&quot;</span>, <span class="string">&quot;col3&quot;</span>)</span><br><span class="line"></span><br><span class="line">spark.close()</span><br></pre></td></tr></table></figure><p>将ES索引加载成DataFrame后便可以方便地进行表操作了。</p><p>最后注意提交Spark任务时加上第三方依赖包elasticsearch-spark-20_2.11-7.1.0.jar。</p><p>参考</p><ol><li><a href="https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html">https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Elasticsearch </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python使用POP3协议客户端poplib登录邮箱并解压缩zip、rar压缩包</title>
      <link href="2020/10/11/Python%E4%BD%BF%E7%94%A8POP3%E5%8D%8F%E8%AE%AE%E5%AE%A2%E6%88%B7%E7%AB%AFpoplib%E7%99%BB%E5%BD%95%E9%82%AE%E7%AE%B1%E5%B9%B6%E8%A7%A3%E5%8E%8B%E7%BC%A9zip%E3%80%81rar%E5%8E%8B%E7%BC%A9%E5%8C%85/"/>
      <url>2020/10/11/Python%E4%BD%BF%E7%94%A8POP3%E5%8D%8F%E8%AE%AE%E5%AE%A2%E6%88%B7%E7%AB%AFpoplib%E7%99%BB%E5%BD%95%E9%82%AE%E7%AE%B1%E5%B9%B6%E8%A7%A3%E5%8E%8B%E7%BC%A9zip%E3%80%81rar%E5%8E%8B%E7%BC%A9%E5%8C%85/</url>
      
        <content type="html"><![CDATA[<h1 id="1-项目背景"><a href="#1-项目背景" class="headerlink" title="1. 项目背景"></a>1. 项目背景</h1><p>目前在做的一个小项目，需要到登录到邮箱获取压缩包，解压压缩包获取文件，并从文件中抽取出有效数据入库，后续会做一些二次加工，最后用到业务风控中。</p><a id="more"></a><h1 id="2-poplib模块"><a href="#2-poplib模块" class="headerlink" title="2. poplib模块"></a>2. poplib模块</h1><p>Python内建了poplib模块来实现登录邮箱，这个模块提供了一个类POP3_SSL，它支持使用SSL(Secure Socket Layer，安全套阶层，在OSI模型处于会话层)作为底层协议连接POP3服务器。</p><h1 id="3-邮箱登录"><a href="#3-邮箱登录" class="headerlink" title="3. 邮箱登录"></a>3. 邮箱登录</h1><p>邮箱登录代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> poplib</span><br><span class="line"></span><br><span class="line">\<span class="comment"># 通过主机名、端口生成POP3对象</span></span><br><span class="line">pop = poplib.POP3_SSL(host=<span class="string">&quot;&quot;</span>, port=<span class="string">&quot;&quot;</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    \<span class="comment"># 通过用户名、密码登录邮箱</span></span><br><span class="line">    pop.user(<span class="string">&quot;&quot;</span>)</span><br><span class="line">    pop.pass_(<span class="string">&quot;&quot;</span>)</span><br><span class="line"><span class="keyword">except</span> poplib.error_proto <span class="keyword">as</span> e:</span><br><span class="line">    logger.error(<span class="string">&quot;Login failed: &quot;</span> + e)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    parse(pop, sys.argv)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    \<span class="comment"># 退出邮箱</span></span><br><span class="line">    pop.quit()</span><br></pre></td></tr></table></figure><p>注意，最后程序不管是正常结束还是非正常结束都一定要退出邮箱。</p><h1 id="4-获取邮件内容"><a href="#4-获取邮件内容" class="headerlink" title="4. 获取邮件内容"></a>4. 获取邮件内容</h1><p>获取指定邮件有多种方式，可以通过邮件发件人、邮件发件邮箱地址、邮件主题，还可以用邮件索引，使用邮件索引速度最快，这种获取方式就跟在散列表中根据key获取对应value很相似。</p><h2 id="1-获取邮件基本信息"><a href="#1-获取邮件基本信息" class="headerlink" title="1.  获取邮件基本信息"></a>1.  获取邮件基本信息</h2><p>登录邮箱之后，可以获取很多与邮件相关的基本信息，比如邮箱邮件列表、邮件主题、邮件内容、邮件发件人及发件地址、邮件接收时间等等</p><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> email.parser <span class="keyword">import</span> Parser</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_mail_list</span>(<span class="params">pop</span>):</span></span><br><span class="line">    <span class="comment"># 查找附件, 并返回下载文件路径</span></span><br><span class="line">    emails = pop.list()[<span class="number">1</span>]  <span class="comment"># 邮箱邮件列表</span></span><br><span class="line">    email_num = len(emails)  <span class="comment"># 邮件数量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>]:</span><br><span class="line">        lines = pop.retr(i)[<span class="number">1</span>]  <span class="comment"># 获取指定的邮件</span></span><br><span class="line">        line_bytes = <span class="string">b&#x27;\r\n&#x27;</span>.join(lines)  <span class="comment"># 换行符分割邮件信息</span></span><br><span class="line">        msg_content = line_bytes.decode(<span class="string">&#x27;utf-8&#x27;</span>)  <span class="comment"># 解码</span></span><br><span class="line">        msg = Parser().parsestr(msg_content)  <span class="comment"># 每封邮件信息</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 发件人和邮箱地址</span></span><br><span class="line">        header, address = parseaddr(msg.get(<span class="string">&quot;From&quot;</span>, <span class="string">&quot;&quot;</span>)) </span><br><span class="line">        name, address = decode_str(header), decode_str(address)</span><br><span class="line">        <span class="comment"># 邮件主题</span></span><br><span class="line">        email_subject = decode_str(msg.get(<span class="string">&#x27;Subject&#x27;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 邮件接收时间</span></span><br><span class="line">        date_tuple = parsedate_tz(msg.get(<span class="string">&quot;Date&quot;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line">        date_formatted = datetime.fromtimestamp(mktime_tz(date_tuple)).strftime(<span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        compressed_files = list()</span><br><span class="line">        <span class="keyword">for</span> compressed_file <span class="keyword">in</span> get_attachment(msg, compressed_files):</span><br><span class="line">            <span class="keyword">yield</span> compressed_file</span><br></pre></td></tr></table></figure><h2 id="2-获取邮件中的附件"><a href="#2-获取邮件中的附件" class="headerlink" title="2. 获取邮件中的附件"></a>2. 获取邮件中的附件</h2><p>获取邮件附件中的zip或rar压缩包，<font color=red>注意：这里要考虑到一封邮件中有多个压缩包的情况。</font></p><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_attachment</span>(<span class="params">msg, ompressed_files: list</span>):</span></span><br><span class="line">    <span class="keyword">if</span> msg.is_multipart() <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">        \<span class="comment"># 分层信息</span></span><br><span class="line">        parts = msg.get_payload()</span><br><span class="line">        <span class="keyword">for</span> n, part <span class="keyword">in</span> enumerate(parts):</span><br><span class="line">            res = get_attachment(part, compressed_files)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        content_type = msg.get_content_type()</span><br><span class="line">        <span class="keyword">if</span> content_type == <span class="string">&#x27;application/octet-stream&#x27;</span>:</span><br><span class="line">            <span class="keyword">for</span> subpart <span class="keyword">in</span> msg.walk():  \<span class="comment"># 遍历消息树(深度优先搜索算法), 获取每个子节点</span></span><br><span class="line">                file_name_encoder = subpart.get_filename()</span><br><span class="line">                file_name = decode_str(file_name_encoder)  \<span class="comment"># 解码获取文件名</span></span><br><span class="line">                \<span class="comment"># 判断是否是zip或rar格式文件</span></span><br><span class="line">                <span class="keyword">if</span> file_name.split(<span class="string">&quot;.&quot;</span>)[<span class="number">-1</span>] <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;zip&#x27;</span>, <span class="string">&quot;rar&quot;</span>]:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                data = msg.get_payload(decode=<span class="literal">True</span>)  \<span class="comment"># 附件二进制</span></span><br><span class="line">                file_data = base64.b64encode(data).decode()  \<span class="comment"># base64编码</span></span><br><span class="line">                compressed_files.append(&#123;<span class="string">&quot;file_data&quot;</span>: file_data, <span class="string">&quot;name&quot;</span>: file_name&#125;)  \<span class="comment"># 保存到列表</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> compressed_files</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode_str</span>(<span class="params">s</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    解码</span></span><br><span class="line"><span class="string">    :param s:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    value, charset = decode_header(s)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> charset:</span><br><span class="line">        value = value.decode(charset)</span><br><span class="line">    <span class="keyword">return</span> value</span><br></pre></td></tr></table></figure><h1 id="5-解压zip-rar压缩包"><a href="#5-解压zip-rar压缩包" class="headerlink" title="5. 解压zip/rar压缩包"></a>5. 解压zip/rar压缩包</h1><p>在获取到邮件附件的二进制内容之后，就可以使用zipfile模块和rarfile模块解解压zip和rar压缩包了。</p><h2 id="1-打开zip-rar压缩包"><a href="#1-打开zip-rar压缩包" class="headerlink" title="1. 打开zip/rar压缩包"></a>1. 打开zip/rar压缩包</h2><p>处理步骤4中获取的邮件附件，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> rarfile</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">pop</span>):</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> get_mail_list(pop):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> data[<span class="string">&#x27;name&#x27;</span>].endswith(<span class="string">&quot;.zip&quot;</span>):  <span class="comment"># zip格式的压缩文件</span></span><br><span class="line">                zip_obj = zipfile.ZipFile(BytesIO(base64.b64decode(data[<span class="string">&#x27;file_data&#x27;</span>].encode())))</span><br><span class="line">                <span class="keyword">for</span> record <span class="keyword">in</span> get_files_in_zip(zip_obj, item):</span><br><span class="line">                    <span class="keyword">pass</span>  <span class="comment"># 数据处理</span></span><br><span class="line">            <span class="keyword">elif</span> data[<span class="string">&#x27;name&#x27;</span>].endswith(<span class="string">&quot;.rar&quot;</span>):  <span class="comment"># rar格式的压缩文件</span></span><br><span class="line">                rar_obj = rarfile.RarFile(BytesIO(base64.b64decode(data[<span class="string">&#x27;file_data&#x27;</span>].encode())))</span><br><span class="line">                <span class="keyword">for</span> record <span class="keyword">in</span> get_files_in_rar(rar_obj, item):</span><br><span class="line"><span class="keyword">pass</span>  <span class="comment"># 数据处理</span></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            logger.error(traceback.format_exc())</span><br></pre></td></tr></table></figure><h2 id="2-获取压缩包中文件"><a href="#2-获取压缩包中文件" class="headerlink" title="2. 获取压缩包中文件"></a>2. 获取压缩包中文件</h2><p>打开压缩包之后，便可迭代获取压缩包中文件。不过这里要注意的是，zip/rar压缩中可能会嵌套压缩包，比如zip压缩包中嵌套zip压缩包或rar压缩包，那么就需要使用递归进行解压缩。</p><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> rarfile</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_files_in_zip</span>(<span class="params">zip_obj</span>):</span></span><br><span class="line">    <span class="comment"># 获取压缩包中文件列表, 同时过滤一些空目录</span></span><br><span class="line">    files = [file <span class="keyword">for</span> file <span class="keyword">in</span> zip_obj.namelist() <span class="keyword">if</span> re.search(<span class="string">&quot;\\..*$&quot;</span>, file)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历每个压缩包中的所有文件</span></span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">        <span class="comment"># 压缩包中嵌套压缩包</span></span><br><span class="line">        <span class="keyword">if</span> file.endswith(<span class="string">&quot;zip&quot;</span>):</span><br><span class="line">            inner_zip_obj = zipfile.ZipFile(BytesIO(base64.b64decode(base64.b64encode(zip_obj.read(file)).decode().encode())))</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> get_files_in_zip(inner_zip_obj)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">elif</span> file.endswith(<span class="string">&quot;rar&quot;</span>):</span><br><span class="line">            inner_rar_obj = rarfile.RarFile(BytesIO(base64.b64decode(base64.b64encode(zip_obj.read(file)).decode().encode())))</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> get_files_in_rar(inner_rar_obj)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line"><span class="keyword">pass</span>  <span class="comment"># 压缩中文件处理</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_files_in_rar</span>(<span class="params">rar_obj</span>):</span></span><br><span class="line">    <span class="comment"># 获取压缩包中文件列表, 同时过滤一些空目录</span></span><br><span class="line">    files = [file <span class="keyword">for</span> file <span class="keyword">in</span> rar_obj.namelist() <span class="keyword">if</span> re.search(<span class="string">&quot;\\..*$&quot;</span>, file)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析每个文件内容</span></span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">        <span class="comment"># 压缩包中嵌套压缩包</span></span><br><span class="line">        <span class="keyword">if</span> file.endswith(<span class="string">&quot;rar&quot;</span>):</span><br><span class="line">            inner_rar_obj = rarfile.RarFile(BytesIO(base64.b64decode(base64.b64encode(rar_obj.read(file)).decode().encode())))</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> get_files_in_rar(inner_rar_obj)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">elif</span> file.endswith(<span class="string">&quot;zip&quot;</span>):</span><br><span class="line">            inner_zip_obj = zipfile.ZipFile(BytesIO(base64.b64decode(base64.b64encode(rar_obj.read(file)).decode().encode())))</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> get_files_in_zip(inner_zip_obj)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line"><span class="keyword">pass</span>  <span class="comment"># 压缩中文件处理</span></span><br></pre></td></tr></table></figure><p>解压获取压缩包中文件之后，就可以从文件(这里主要是pdf、html、excel、csv等格式的文件)中提取数据了。</p><p>不同格式的文件如何解析，请参考我的<a href="https://blog.csdn.net/lovetechlovelife/article/details/107499638">另一篇文章</a>。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 邮箱 </tag>
            
            <tag> POP </tag>
            
            <tag> ZIP </tag>
            
            <tag> RAR </tag>
            
            <tag> 压缩包 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python解析Excel、PDF、HTML、CSV...文件</title>
      <link href="2020/10/11/Python%E8%A7%A3%E6%9E%90Excel%E3%80%81PDF%E3%80%81HTML%E3%80%81CSV-%E6%96%87%E4%BB%B6/"/>
      <url>2020/10/11/Python%E8%A7%A3%E6%9E%90Excel%E3%80%81PDF%E3%80%81HTML%E3%80%81CSV-%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<p>Python在文件解析方面有很多丰富的三方模块，接下来我们就看如何使用Python解析Excel、PDF、HTML、CSV这几种常见格式的文件。</p><a id="more"></a><h1 id="1-Excel文件解析"><a href="#1-Excel文件解析" class="headerlink" title="1. Excel文件解析"></a>1. Excel文件解析</h1><p>标准的Excel文件其实就相当于一张表，有表头和对应的数据。一般常见的Excel文件都以.xls、.xlsx和.et结尾。</p><p>以下是使用xlrd模块实现解析Excel文件的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> xlrd</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xls_parse</span>(<span class="params">file_data</span>):</span></span><br><span class="line">    <span class="comment"># 打开一个电子表格文件进行数据提取</span></span><br><span class="line">    data = xlrd.open_workbook(file_contents=a bytes object, encoding_override=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="comment"># 一个Excel文件中所有sheet的名称</span></span><br><span class="line">    sheets = data.sheet_names()</span><br><span class="line">    <span class="comment"># 通过sheet名获取对应的sheet表内容</span></span><br><span class="line">    table = data.sheet_by_name(sheets[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    start_row = <span class="number">0</span>  <span class="comment"># 起始行</span></span><br><span class="line">    rows = table.nrows  <span class="comment"># 表格总行数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假如第一行是表头</span></span><br><span class="line">header = table.row_values(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历表记录</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, rows):</span><br><span class="line">        <span class="comment"># 表记录值与对应的表头组成字典</span></span><br><span class="line">        col_dic = dict(zip(header, table.row_values(i)))</span><br><span class="line">        </span><br><span class="line">        raw_item = json.dumps(col_dic, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">yield</span> raw_item  <span class="comment"># 返回json格式</span></span><br></pre></td></tr></table></figure><p>&nbsp;</p><h1 id="2-PDF文件解析"><a href="#2-PDF文件解析" class="headerlink" title="2. PDF文件解析"></a>2. PDF文件解析</h1><p>PDF相对来说是比较难的解析，除非PDF中有类似Excel格式的数据，否则解析出来的数据很难进行规范化。Python中也有很多解析pdf文件的第三方库。比如tabula等</p><p>tabula的解析代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> tabula</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pdf_parse</span>(<span class="params">file_data</span>):</span></span><br><span class="line">    <span class="comment"># 提取PDF文件中的表格</span></span><br><span class="line">    tables = tabula.read_pdf(file_data, pages=<span class="string">&quot;all&quot;</span>)  <span class="comment"># file_data为BytesIO对象</span></span><br><span class="line">    <span class="comment"># 遍历提取出的所有表格</span></span><br><span class="line">    <span class="keyword">for</span> df <span class="keyword">in</span> tables:</span><br><span class="line">        <span class="comment"># 提取表头</span></span><br><span class="line">        columns = list(df.columns)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 按列切分表格记录</span></span><br><span class="line">        df_new = pd.DataFrame(columns=columns)</span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> columns:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment"># DataFrame转JSON</span></span><br><span class="line">        json_array = json.loads(df_new.to_json(orient=<span class="string">&quot;records&quot;</span>, force_ascii=<span class="literal">False</span>))</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> json_array:</span><br><span class="line">            <span class="keyword">yield</span> json.dumps(item, ensure_ascii=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>除了tabula之外，本人还是测试了很多其他的第三方模块，比如pdfminer.six、pdfminer3k、pdfplumber、PyPDF2、slate3k、camelot。要使用哪个模块，主要依赖于pdf文件内容的规范程度，总的来说推荐使用pdfplumber或者tabula。</p><p>&nbsp;</p><h1 id="3-HTML文件解析"><a href="#3-HTML文件解析" class="headerlink" title="3. HTML文件解析"></a>3. HTML文件解析</h1><p>HTML格式的文件相当于是半结构的数据，解析相对来说比较简单。</p><p>html文件的解析代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> lxml.etree <span class="keyword">import</span> HTML</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">html_parse</span>(<span class="params">file_data</span>):</span></span><br><span class="line">    <span class="comment"># 从一个字符串常量中解析HTML文档</span></span><br><span class="line">    html = HTML(file_data.read())</span><br><span class="line">    html_tables = html.findall(<span class="string">&quot;body/table&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将HTML中的所有&lt;table&gt;都放到一个列表中, 以便后续统一处理</span></span><br><span class="line">    table_rows = []</span><br><span class="line">    <span class="keyword">for</span> html_table <span class="keyword">in</span> html_tables:</span><br><span class="line">        table_rows.extend(list(html_table))</span><br><span class="line"></span><br><span class="line">    rows = len(table_rows)  <span class="comment"># 表格总行数</span></span><br><span class="line">    start_row = <span class="number">0</span>  <span class="comment"># 起始行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设第一行是表头</span></span><br><span class="line">    header = [col.text <span class="keyword">for</span> col <span class="keyword">in</span> table_rows[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历表记录</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(start_row, rows):</span><br><span class="line">        values = [col.text <span class="keyword">for</span> col <span class="keyword">in</span> table_rows[i]]</span><br><span class="line">        <span class="comment"># 表记录值与对应的列名组成字典</span></span><br><span class="line">        col_dic = dict(zip(header, values))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 字典转json</span></span><br><span class="line">        raw_item = json.dumps(col_dic, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">yield</span> raw_item</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>&nbsp;</p><h1 id="4-CSV文件解析"><a href="#4-CSV文件解析" class="headerlink" title="4. CSV文件解析"></a>4. CSV文件解析</h1><p>csv是默认以逗号(,)分割的一行行数据组成的文件。通常情况，csv文件都可以通过Excel软件打开，正常显示为表格。</p><p>csv文件解析代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">csv_parse</span>(<span class="params">file_data</span>):</span></span><br><span class="line">    file_bytes_obj = file_data.read()  <span class="comment"># file_data为BytesIO对象</span></span><br><span class="line"><span class="comment"># 根据文件内容检测编码格式有时候会不准确, 这时可以尝试更范围的编码格式</span></span><br><span class="line"><span class="comment"># 范围: gb18030 &gt; gb2312 &gt; gbk</span></span><br><span class="line">    encode = chardet.detect(file_bytes_obj)[<span class="string">&quot;encoding&quot;</span>]</span><br><span class="line"></span><br><span class="line">    file_string_io = StringIO(file_bytes_obj.decode(encode).replace(<span class="string">&#x27;\r&#x27;</span>, <span class="string">&#x27;\r\n&#x27;</span>))</span><br><span class="line">    csv_data = list(csv.reader(file_string_io))</span><br><span class="line"></span><br><span class="line">    start_row = <span class="number">-1</span>  <span class="comment"># 起始行</span></span><br><span class="line">    rows = len(csv_data)  <span class="comment"># 表格总行数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 表头</span></span><br><span class="line">    header = csv_data[i]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历表记录</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(start_row, rows):</span><br><span class="line">        col_dic = dict(zip(header, csv_data[i]))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 字典转json</span></span><br><span class="line">        raw_item = json.dumps(col_dic, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">yield</span> raw_item</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如有遇到其他格式的文件，之后会持续更新到这篇文章中！</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python读写Elasticsearch、HBase、Hive</title>
      <link href="2020/10/11/Python%E8%AF%BB%E5%86%99Elasticsearch%E3%80%81HBase%E3%80%81Hive/"/>
      <url>2020/10/11/Python%E8%AF%BB%E5%86%99Elasticsearch%E3%80%81HBase%E3%80%81Hive/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Elasticsearch"><a href="#1-Elasticsearch" class="headerlink" title="1. Elasticsearch"></a>1. Elasticsearch</h2><p>安装Elasticsearch模块：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install elasticsearch==7.1.0</span><br></pre></td></tr></table></figure><p>读取ES索引的代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> elasticsearch <span class="keyword">import</span> Elasticsearch</span><br><span class="line"></span><br><span class="line">es = Elasticsearch(hosts=[&#123;<span class="string">&quot;host&quot;</span>: <span class="string">&quot;host&quot;</span>, <span class="string">&quot;port&quot;</span>: <span class="number">9200</span>&#125;])</span><br><span class="line">result = es.search(index=<span class="string">&quot;index_name&quot;</span>, body=&#123;<span class="string">&quot;query&quot;</span>: &#123;<span class="string">&quot;match_all&quot;</span>: &#123;&#125;&#125;&#125;)</span><br><span class="line"><span class="keyword">for</span> hit <span class="keyword">in</span> result[<span class="string">&#x27;hits&#x27;</span>][<span class="string">&#x27;hits&#x27;</span>]:</span><br><span class="line">    print(hit[<span class="string">&quot;_source&quot;</span>])</span><br></pre></td></tr></table></figure><h2 id="2-HBase"><a href="#2-HBase" class="headerlink" title="2. HBase"></a>2. HBase</h2><p>安装Python连接HBase的模块：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install happybase</span><br></pre></td></tr></table></figure><p>读取HBase表的代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> happybase</span><br><span class="line"></span><br><span class="line">connection = happybase.Connection(<span class="string">&#x27;host&#x27;</span>)</span><br><span class="line">connection.open()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印所有的表</span></span><br><span class="line">print(connection.tables())</span><br><span class="line"></span><br><span class="line">table = connection.table(<span class="string">&#x27;table_name&#x27;</span>)</span><br><span class="line">print(table.families())</span><br><span class="line"></span><br><span class="line">row = table.row(<span class="string">&#x27;row_key&#x27;</span>, columns=[<span class="string">b&#x27;cf:col1&#x27;</span>, <span class="string">b&#x27;cf:col2&#x27;</span>, <span class="string">b&#x27;cf:col3&#x27;</span>])</span><br><span class="line">print(bytes(row[<span class="string">b&#x27;cf:col1&#x27;</span>]).decode(<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line"></span><br><span class="line">connection.close()</span><br></pre></td></tr></table></figure><h2 id="3-Hive"><a href="#3-Hive" class="headerlink" title="3. Hive"></a>3. Hive</h2><p>安装Python连接Hive的模块：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install pyhive</span><br></pre></td></tr></table></figure><p>如果安装pyhive过程出错，请参考<a href="https://blog.csdn.net/lovetechlovelife/article/details/97128463">pyhive依赖模块sasl安装(pip install sasl)报错Microsoft Visual C++ 14.0 is required</a>。</p><p>读取Hive表的代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyhive <span class="keyword">import</span> hive</span><br><span class="line"></span><br><span class="line">conn = hive.Connection(host=<span class="string">&quot;master-1&quot;</span>, port=<span class="number">10000</span>, username=<span class="string">&quot;root&quot;</span>, auth=<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">cursor.execute(<span class="string">&quot;select * from db.table limit 10&quot;</span>)</span><br><span class="line">print(cursor.fetchall())</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Elasticsearch </tag>
            
            <tag> HBase </tag>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pymysql + DBUtils实现数据库连接池及数据批量读写</title>
      <link href="2020/10/11/pymysql-DBUtils%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E6%B1%A0%E5%8F%8A%E6%95%B0%E6%8D%AE%E6%89%B9%E9%87%8F%E8%AF%BB%E5%86%99/"/>
      <url>2020/10/11/pymysql-DBUtils%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E6%B1%A0%E5%8F%8A%E6%95%B0%E6%8D%AE%E6%89%B9%E9%87%8F%E8%AF%BB%E5%86%99/</url>
      
        <content type="html"><![CDATA[<h2 id="1-安装pymysql、DBUtils"><a href="#1-安装pymysql、DBUtils" class="headerlink" title="1. 安装pymysql、DBUtils"></a>1. 安装pymysql、DBUtils</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple pymysql</span><br><span class="line">pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple DBUtils</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="2-创建连接池"><a href="#2-创建连接池" class="headerlink" title="2. 创建连接池"></a>2. 创建连接池</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> DBUtils.PooledDB <span class="keyword">import</span> PooledDB, PooledDBError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_pool</span>():</span></span><br><span class="line">pool = PooledDB(</span><br><span class="line">    creator=pymysql,  <span class="comment"># 数据库连接对象</span></span><br><span class="line">    host=<span class="string">&quot;host&quot;</span>,</span><br><span class="line">    user=<span class="string">&quot;user&quot;</span>,</span><br><span class="line">    password=<span class="string">&quot;password&quot;</span>,</span><br><span class="line">    database=<span class="string">&quot;database&quot;</span>,</span><br><span class="line">    charset=<span class="string">&quot;charset&quot;</span>,</span><br><span class="line">    blocking=<span class="literal">True</span>,  <span class="comment"># 超过最大连接数时该如何处理后来的任务。设置为true就表示阻塞等待直达有空闲连接</span></span><br><span class="line">    maxconnections=<span class="number">2</span>,  <span class="comment"># 连接池所能允许的最大连接数</span></span><br><span class="line">    mincached=<span class="number">1</span>,  <span class="comment"># 最小初始化空闲连接数</span></span><br><span class="line">    autocommit=<span class="literal">True</span>  <span class="comment"># 是否要自动提交</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">return</span> pool.connection()</span><br></pre></td></tr></table></figure><h2 id="3-读数据"><a href="#3-读数据" class="headerlink" title="3. 读数据"></a>3. 读数据</h2><h3 id="1-只读一行"><a href="#1-只读一行" class="headerlink" title="1. 只读一行"></a>1. 只读一行</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_one</span>(<span class="params">self, conn, sql</span>):</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line">        cursor.execute(sql)</span><br><span class="line">        <span class="keyword">return</span> cursor.fetchone()[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        conn.close()</span><br><span class="line">        cursor.close()</span><br></pre></td></tr></table></figure><h3 id="2-批量读取"><a href="#2-批量读取" class="headerlink" title="2. 批量读取"></a>2. 批量读取</h3><p>批量读取的场景，如果要返回的数据量比较大，为了避免给数据库造成太大负载和占用过多带宽资源，可以选择分批读取数据。比如要读取10万条数据，每批只读2000条。</p><p>思路：</p><ol><li>先通过一条SQL获取要读取的数据行数</li><li>每读取一批之后，自增主键减去2000</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">sql_count = <span class="string">&quot;select id from db.table where id between 25000 and 125000&quot;</span></span><br><span class="line">sql = <span class="string">&quot;select col1, col2, col3 from db.table&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_many</span>(<span class="params">self, conn, sql_count, sql, size</span>):</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算要读取的数据行数</span></span><br><span class="line">        cursor.execute(sql_count)</span><br><span class="line">        cursor.fetchall()</span><br><span class="line">        row_count = cursor.rowcount</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分配读取数据</span></span><br><span class="line">        cursor.execute(sql)</span><br><span class="line">        <span class="keyword">while</span> row_count &gt; <span class="number">0</span>:</span><br><span class="line">            records = cursor.fetchmany(size)</span><br><span class="line">            row_count -= size</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> records</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        conn.close()</span><br><span class="line">        cursor.close()</span><br></pre></td></tr></table></figure><h2 id="4-写数据"><a href="#4-写数据" class="headerlink" title="4. 写数据"></a>4. 写数据</h2><p>如果可以，尽量进行批量写，因为executemany()方法在多行插入和更新的场景下提升写性能。否则，就等于是循环执行execute()方法一条条的写，性能较差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">execute_many</span>(<span class="params">self, conn, sql, args</span>):</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 从连接池中获取一个连接</span></span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line">        cursor.executemany(sql, args)</span><br><span class="line">    <span class="keyword">except</span> PooledDBError <span class="keyword">as</span> e:</span><br><span class="line">        logger.error(<span class="string">&quot;Failed to insert records into MySQL table &#123;&#125;&quot;</span>.format(e))</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        conn.close()</span><br><span class="line">        cursor.close()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
            <tag> Python </tag>
            
            <tag> 连接池 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL SELECT查询非分组聚合列（sql_mode=only_full_group_by）</title>
      <link href="2020/10/11/MySQL-SELECT%E6%9F%A5%E8%AF%A2%E9%9D%9E%E5%88%86%E7%BB%84%E8%81%9A%E5%90%88%E5%88%97%EF%BC%88sql-mode-only-full-group-by%EF%BC%89/"/>
      <url>2020/10/11/MySQL-SELECT%E6%9F%A5%E8%AF%A2%E9%9D%9E%E5%88%86%E7%BB%84%E8%81%9A%E5%90%88%E5%88%97%EF%BC%88sql-mode-only-full-group-by%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>业务中有这样的逻辑：通过某一列col1分组之后查询col1，col2，col3这三个列，SQL语句如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> company_name, department, employee <span class="keyword">from</span> company <span class="keyword">group</span> <span class="keyword">by</span> company_name</span><br></pre></td></tr></table></figure><p>如果是MySQL 5.7以前的版本，是可以执行成功的；如果是MySQL 5.7及以上版本，就会报错：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR 1140 (42000): In aggregated query without GROUP BY, expression... of <span class="keyword">SELECT</span> <span class="keyword">list</span> contains nonaggregated <span class="keyword">column</span> <span class="string">&#x27;col_name&#x27;</span>; this is incompatible <span class="keyword">with</span> sql_mode=only_full_group_by</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h1><p>在标准的 SQL-92中，上述的查询是不被支持的。也就是说，select列表、having条件和order by列表中是不允许出现非聚合列的，所谓非聚合列就是group by子句中的列。换句话说，就是select中查询的列，必须出现在group by子句中，否则就必须在非聚合列上使用聚合函数（比如sum()、max()等）。</p><p>为什么不允许这样查询呢？主要是因为分组之后，要查询非聚合列，其实是不知道该取那个值的。</p><p>但是MySQL对标准的SQL-92做了扩展，使得select列表、having条件和order by列表中可以引用非聚合列，这个功能可以带来更好的性能，因为它需要对非聚合列进行分组排序。不过在分组之后，查询非聚合列时，其取值是随机的，也就是组内随机取一个，就算是先通过order by进行排序之后，也是随机取值的。</p><p>因为MySQL 5.7之前，配置项ONLY_FULL_GROUP_BY是默认禁用的，也就是支持在select列表、having条件和order by列表中引用非聚合列且不适用聚合函数；而MySQL 5.7及之后的版本中，配置项ONLY_FULL_GROUP_BY默认是开启的，则不支持上述功能。</p><p>可以通过下面SQL查看ONLY_FULL_GROUP_BY是否启用：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询的结果中包含ONLY_FULL_GROUP_BY就说明是启用的，否则就是被禁用的</span></span><br><span class="line"><span class="keyword">select</span> @@global.sql_mode;</span><br></pre></td></tr></table></figure><p>由于本人使用的是MySQL 5.7，所以在select中查询非聚合列就出现了上述的报错</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>了解了问题原因和原理之后，解决起来就比较简单了。</p><p>通过下面SQL将sql_mode中的ONLY_FULL_GROUP_BY替换成空即可：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> <span class="keyword">global</span> sql_mode=(<span class="keyword">select</span> <span class="keyword">replace</span>(@@sql_mode, <span class="string">&#x27;ONLY_FULL_GROUP_BY&#x27;</span>, <span class="string">&#x27;&#x27;</span>));</span><br></pre></td></tr></table></figure><p>当然也可以设置会话级别的：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> <span class="keyword">session</span> sql_mode=(<span class="keyword">select</span> <span class="keyword">replace</span>(@@sql_mode, <span class="string">&#x27;ONLY_FULL_GROUP_BY&#x27;</span>, <span class="string">&#x27;&#x27;</span>));</span><br></pre></td></tr></table></figure><p>或者是通过MySQL配置文件my.cnf进行修改（需重启mysql实例）。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://dev.mysql.com/doc/refman/5.6/en/group-by-handling.html">https://dev.mysql.com/doc/refman/5.6/en/group-by-handling.html</a></li><li><a href="https://dev.mysql.com/doc/refman/5.7/en/group-by-handling.html">https://dev.mysql.com/doc/refman/5.7/en/group-by-handling.html</a></li><li><a href="https://dev.mysql.com/doc/refman/5.7/en/sql-mode.html#sqlmode_only_full_group_by">https://dev.mysql.com/doc/refman/5.7/en/sql-mode.html#sqlmode_only_full_group_by</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python for循环中添加字典到列表，结果列表中全是循环中的最后一个值</title>
      <link href="2020/10/11/Python-for%E5%BE%AA%E7%8E%AF%E4%B8%AD%E6%B7%BB%E5%8A%A0%E5%AD%97%E5%85%B8%E5%88%B0%E5%88%97%E8%A1%A8%EF%BC%8C%E7%BB%93%E6%9E%9C%E5%88%97%E8%A1%A8%E4%B8%AD%E5%85%A8%E6%98%AF%E5%BE%AA%E7%8E%AF%E4%B8%AD%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AA%E5%80%BC/"/>
      <url>2020/10/11/Python-for%E5%BE%AA%E7%8E%AF%E4%B8%AD%E6%B7%BB%E5%8A%A0%E5%AD%97%E5%85%B8%E5%88%B0%E5%88%97%E8%A1%A8%EF%BC%8C%E7%BB%93%E6%9E%9C%E5%88%97%E8%A1%A8%E4%B8%AD%E5%85%A8%E6%98%AF%E5%BE%AA%E7%8E%AF%E4%B8%AD%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AA%E5%80%BC/</url>
      
        <content type="html"><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>问题的是在Python解析Excel文件出现的。</p><a id="more"></a><p>下面是出现问题的一段代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此函数实现了解析Excel文件，并将文件中的每行数据以字典的形式返回(key为列名，value为对应的值)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span>():</span></span><br><span class="line">...</span><br><span class="line"><span class="keyword">yield</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将解析返回的每行数据追加到一个列表中</span></span><br><span class="line">list = []</span><br><span class="line"><span class="keyword">for</span> dic <span class="keyword">in</span> func():</span><br><span class="line">list.append(dic)</span><br><span class="line"></span><br><span class="line">print(list)</span><br></pre></td></tr></table></figure><p>上面代码执行之后，输出的list中，发现所有元素的内容都是相同的，再去对比源文件，发现列表的元素都为Excel文件中最后一行内容。很明显，这肯定是不对的！</p><h1 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h1><p>出现这个问题之后，首先想到的就是逐步调试这段循环代码，同时观察dic的值和list中元素的值。</p><ul><li>第一次循环时，list中对应的确实是Excel中第一行的内容，</li><li>第二次循环时，在dic已改变，但还没往list中追加第二个元素之前，发现第一次追加到list中的的元素的值已经发生了改变，而且改变之后的内容就和第二次循环时dic的内容相同。</li></ul><p>到这里，就恍然大悟了，原来每次往列表中追加的都是这个字典的引用，而不是字典的内容，所以才会在每次循环时，出现只要dic的内容发生了变化，list中之前追加进去的dic也随着发生改变的情况。</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>知道了这个原因，我们就知道在追加列表时，针对Excel的每行数据，都应该追加不同的引用。</p><p>那么具体的实现就是用到字典的copy()方法，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">list = []</span><br><span class="line"><span class="keyword">for</span> dic <span class="keyword">in</span> func():</span><br><span class="line">list.append(dic.copy())</span><br></pre></td></tr></table></figure><p>copy()方法对字典dic进行了一个浅拷贝，也就是说对字典创建了一个新的引用。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://docs.python.org/3.7/library/copy.html">https://docs.python.org/3.7/library/copy.html</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Excel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL是如何通过变量实现组内排序的(MySQL 8.0之前版本，非row_number函数)</title>
      <link href="2020/10/11/MySQL%E6%98%AF%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E5%8F%98%E9%87%8F%E5%AE%9E%E7%8E%B0%E7%BB%84%E5%86%85%E6%8E%92%E5%BA%8F%E7%9A%84-MySQL-8-0%E4%B9%8B%E5%89%8D%E7%89%88%E6%9C%AC%EF%BC%8C%E9%9D%9Erow-number%E5%87%BD%E6%95%B0/"/>
      <url>2020/10/11/MySQL%E6%98%AF%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E5%8F%98%E9%87%8F%E5%AE%9E%E7%8E%B0%E7%BB%84%E5%86%85%E6%8E%92%E5%BA%8F%E7%9A%84-MySQL-8-0%E4%B9%8B%E5%89%8D%E7%89%88%E6%9C%AC%EF%BC%8C%E9%9D%9Erow-number%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>业务中要实现这样的逻辑：有一张用户表，每个用户有一个或多个银行账户，每个账户有对应的流水数据，现在想要获取每个用户的每个账户下，交易金额最大的前10条流水数据，该如何实现？</p><a id="more"></a><h1 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h1><p>组内排序取Top N在业务当中是一个会经常用到的功能。如果你是在大数据框架中取组内Top N，一般的大数据框架都提供了现成的函数，比如Hive中的row_number() + partitionBy()，实现起来会比较简单。而如果你用的是关系型数据库，比如MySQL，就没有类似现成的函数来实现这个功能了(MySQL 8.0开始也提供row_number函数，但是很多公司都还是使用的MySQL5.6/5.7)。</p><p>那么在MySQL 8.0 之前如何实现组内排序呢？答案就是，借助变量。</p><p>表(bank_transaction)中部分列：<br>|列名| 类型 | 含义 |<br>|–|–|–|<br>| user_id | varchar(64) | 用户ID |<br>| bank_account | varchar(64) | 银行账户 |<br>| transaction_amount | decimal(17, 2) | 交易金额 |</p><p>SQL实现组内求Top N：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">user_id,</span><br><span class="line">bank_account,</span><br><span class="line">transaction_amount</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">user_id,</span><br><span class="line">bank_account,</span><br><span class="line">transaction_amount,</span><br><span class="line">@row_num := <span class="keyword">if</span>(@group01 = user_id <span class="keyword">and</span> @group02 = bank_account, @row_num + <span class="number">1</span>, <span class="number">1</span>) <span class="keyword">as</span> rn,</span><br><span class="line">@group01 := user_id,</span><br><span class="line">@group02 := bank_account</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">user_id,</span><br><span class="line">bank_account,</span><br><span class="line">transaction_amount</span><br><span class="line"><span class="keyword">from</span> bank_transaction </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> user_id, bank_account, transaction_amount <span class="keyword">desc</span></span><br><span class="line">) t1, (<span class="keyword">select</span> @row_num := <span class="number">0</span>, @group01 := <span class="string">&quot;&quot;</span>, @group02 := <span class="string">&quot;&quot;</span>)t2</span><br><span class="line">) t</span><br><span class="line"><span class="keyword">where</span> t.rn &lt;= <span class="number">6</span></span><br></pre></td></tr></table></figure><p>实现思路：</p><ol><li>第一步：对数据进行组内排序，t1这个子查询里面就是对每个用户的每个账户下的所有交易金额进行降序排列</li><li>第二步：设置变量，@row_num变量保存的是组内排序时的编号，@group01和@group02保存的是要分组列的临时值</li><li>第三步：遍历已排序数据，每次拿后一行与前一行进行比较，如果后一行分组列的值和前一行的相等（对应上面SQL语句中的if条件），就表明后一行与前一行属于同一个组，那么变量@row_num的值就加一；如果不相等，就从1重新开始。以此，给所有数据编号。</li><li>第四步：给所有数据编号之后，通过where条件即可取出Top N的记录。</li></ol><p>另外，SQL也可以这样写，主要是变量的使用方式不同：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> @row_num := <span class="number">0</span>, @group01 := <span class="string">&quot;&quot;</span>, @group02 := <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">user_id,</span><br><span class="line">bank_account,</span><br><span class="line">transaction_amount</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">user_id,</span><br><span class="line">bank_account,</span><br><span class="line">transaction_amount,</span><br><span class="line">@row_num := <span class="keyword">if</span>(@group01 = user_id <span class="keyword">and</span> @group02 = bank_account, @row_num + <span class="number">1</span>, <span class="number">1</span>) <span class="keyword">as</span> rn,</span><br><span class="line">@group01 := user_id,</span><br><span class="line">@group02 := bank_account</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">user_id,</span><br><span class="line">bank_account,</span><br><span class="line">transaction_amount</span><br><span class="line"><span class="keyword">from</span> bank_transaction </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> user_id, bank_account, transaction_amount <span class="keyword">desc</span></span><br><span class="line">) t1</span><br><span class="line">) t</span><br><span class="line"><span class="keyword">where</span> t.rn &lt;= <span class="number">6</span></span><br></pre></td></tr></table></figure><p>Hive中如何实现组内排序求Top N，可以参考<a href="https://editor.csdn.net/md/?articleId=107349381">这里</a>。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>本地Spark连接远程集群Hive(Scala/Python)</title>
      <link href="2020/10/11/%E6%9C%AC%E5%9C%B0Spark%E8%BF%9E%E6%8E%A5%E8%BF%9C%E7%A8%8B%E9%9B%86%E7%BE%A4Hive-Scala-Python/"/>
      <url>2020/10/11/%E6%9C%AC%E5%9C%B0Spark%E8%BF%9E%E6%8E%A5%E8%BF%9C%E7%A8%8B%E9%9B%86%E7%BE%A4Hive-Scala-Python/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark中一行代码转换DataFrame所有列的类型</title>
      <link href="2020/10/10/Spark%E4%B8%AD%E4%B8%80%E8%A1%8C%E4%BB%A3%E7%A0%81%E8%BD%AC%E5%8C%96DataFrame%E6%89%80%E6%9C%89%E5%88%97%E7%9A%84%E7%B1%BB%E5%9E%8B/"/>
      <url>2020/10/10/Spark%E4%B8%AD%E4%B8%80%E8%A1%8C%E4%BB%A3%E7%A0%81%E8%BD%AC%E5%8C%96DataFrame%E6%89%80%E6%9C%89%E5%88%97%E7%9A%84%E7%B1%BB%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>当需要把DataFrame中所有列的类型转换成另外一种类型，并且这个DataFrame中字段很多，一个一个地转换要写很多冗余代码，那么就可以使用如下这两种转换方式。</p><a id="more"></a><h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><h2 id="1-foldLeft-函数"><a href="#1-foldLeft-函数" class="headerlink" title="1. foldLeft() 函数"></a>1. foldLeft() 函数</h2><p>代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.DataFrame</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.col</span><br><span class="line"></span><br><span class="line">val df: DataFrame = ...</span><br><span class="line">val columns: Array[String] = df.columns</span><br><span class="line">val df2: DataFrame = columns.foldLeft(df)&#123;(currentDF, column) =&gt; currentDF.withColumn(column, col(column).cast(<span class="string">&quot;string&quot;</span>))&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>变量columns</strong>：是一个String类型的数组，数组中的元素为df中的列名。</li><li><strong>foldLeft函数</strong>：此函数以df为初始值，从左向右遍历columns数组，并把df的每一行和columns的每个元素作为参数传入foldLeft后面的函数中(也就是foldLeft后面的大括号中)。</li><li>**withColumn()**：将每一列转换成String类型并赋值给当前列。如果存在同名的列，withColumn函数默认会进行覆盖。</li></ul><h2 id="2-map-函数"><a href="#2-map-函数" class="headerlink" title="2. map() 函数"></a>2. map() 函数</h2><p>代码如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;Column, DataFrame&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.col</span><br><span class="line"></span><br><span class="line">val df: DataFrame = ...</span><br><span class="line">val columns: Array[String] = df.columns</span><br><span class="line">val arrayColumn: Array[Column] = columns.map(column =&gt; col(column).cast(<span class="string">&quot;string&quot;</span>))</span><br><span class="line">val df2: DataFrame = df.select(arrayColumn :_*)</span><br></pre></td></tr></table></figure><p>通过map函数将columns中的每一列转换成String类型，并返回一个Column类型的数组，然后，将arrayColumn数组中的每个元素作为参数传入select函数中，就相当于df.select(col1, col2, col3, …)。</p><p>除此之外，这种写法还有一个很有用的场景：比如要在一个DataFrame中select出很多列(假如有几十个几百个)，如果要一个个显示写出来，既不方便又会让代码显得很冗余，那么就可以使用这种写法。</p><ol><li>先通过df.columns得到这个DataFrame中的所有列，返回一个包含所有列的数组；</li><li>再使用Scala中的这种语法进行查询df.select(arrayColumn :_*)，非常简洁明了。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL行记录大小超过限制 Row size too large</title>
      <link href="2020/10/10/MySQL%E8%A1%8C%E8%AE%B0%E5%BD%95%E5%A4%A7%E5%B0%8F%E8%B6%85%E8%BF%87%E9%99%90%E5%88%B6%20Row%20size%20too%20large/"/>
      <url>2020/10/10/MySQL%E8%A1%8C%E8%AE%B0%E5%BD%95%E5%A4%A7%E5%B0%8F%E8%B6%85%E8%BF%87%E9%99%90%E5%88%B6%20Row%20size%20too%20large/</url>
      
        <content type="html"><![CDATA[<h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>问题出现在Spark写MySQL的场景：要写入MySQL的DataFrame中有90多个列，其中有10多个列为字符串类型，且长度较长(大于1000)；对应的要写入的MySQL表使用的是InnoDB引擎，这些较大的字符串所对应的列在MySQL中设置为text类型。最终在写MySQL的时候，出现这样的报错：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: <span class="function">Row size too <span class="title">large</span> <span class="params">(&gt; <span class="number">8126</span>)</span>. Changing some columns to TEXT or BLOB or using ROW_FORMAT</span>=DYNAMIC or ROW_FORMAT=COMPRESSED may help. In current row format, BLOB prefix of <span class="number">768</span> bytes is stored inline.</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="MySQL版本"><a href="#MySQL版本" class="headerlink" title="MySQL版本"></a>MySQL版本</h1><p>本人使用的MySQL版本为5.6、InnoDB引擎，以下内容也是以这个前提来展开的，MyISAM暂不做介绍。</p><p>注：</p><ul><li>MySQL 5.6版本默认InnoDB文件格式为Antelope，相应配置innodb_file_format=Antelope，此种文件格式不支持COMPRESSED和DYNAMIC的行格式。</li><li>MySQL 5.7版本默认InnoDB文件格式为Barracuda，相应配置innodb_file_format=Barracuda，此种文件格式支持COMPRESSED和DYNAMIC的行格式。</li></ul><h1 id="名词术语"><a href="#名词术语" class="headerlink" title="名词术语"></a>名词术语</h1><p>要理解上面报错的本质，首先要了解以下概念：</p><ol><li>MySQL中的可变长度类型</li><li>页(page)、页大小(page size)、off-page column、overflow page？</li><li>页（Page）、行大小、行格式三种之间的关系</li></ol><h2 id="1-可变长度类型"><a href="#1-可变长度类型" class="headerlink" title="1. 可变长度类型"></a>1. 可变长度类型</h2><p>MySQL中的可变长度类型：VARCHAR、VARBINARY、BLOB和TEXT类型。</p><p>InnoDB将长度大于等于768字节的fixed-length字段当作可变长度字段，可以存储在off-page。</p><h2 id="2-页-page-、页大小-Page-size-、off-page-column、overflow-page"><a href="#2-页-page-、页大小-Page-size-、off-page-column、overflow-page" class="headerlink" title="2. 页(page)、页大小(Page size)、off-page column、overflow page"></a>2. 页(page)、页大小(Page size)、off-page column、overflow page</h2><h3 id="i-页-Page"><a href="#i-页-Page" class="headerlink" title="i. 页(Page)"></a>i. 页(Page)</h3><p>page代表InnoDB每次在磁盘和内存之间传输多少数据的一个单元。一个page可以包含一行或多行数据，这主要取决于每行数据的大小。如果一行记录不能全部放入到一个page中，InnoDB会用一个指针来引用这行数据。</p><p>可以使用COMPRESSED格式来使每个page容纳更多的数据。对于blob或者text类型的字段，COMPACT格式允许大长度的列和其他列分开存储，以便减少查询时的I/O负载和内存占用。</p><p>当InnoDB以批处理的方式读写一组page以增加I/O吞吐量时，它会一次读写一个区段的page。</p><h3 id="ii-页大小-Page-size"><a href="#ii-页大小-Page-size" class="headerlink" title="ii. 页大小(Page size)"></a>ii. 页大小(Page size)</h3><p>在MySQL 5.6版本之前，每个InnoDB page的大小都是固定的16KB，这是一个各方面取舍平衡的值：16KB能足以容纳大多数的行数据，同时也足够小到可以最小化将不必要的数据传输到内存的性能开销。</p><p>从MySQL 5.6开始，InnoDB page的大小可以是4KB、8KB或16KB，可通过innodb_page_size配置进行设置。在MySQL5.7.6中，InnoDB支持更大的page size(32KB和64KB)，但是这两种page size并不支持ROW_FORMAT=COMPRESSED， 并且最大记录大小为16KB。</p><h3 id="iii-off-page-column"><a href="#iii-off-page-column" class="headerlink" title="iii. off-page column"></a>iii. off-page column</h3><p>一个可变长度列(比如BLOB和VARCHAR)中的数据因为太大而不能放入一个B-tree page中，那么数据就会存储在overflow pages中。</p><h3 id="iiii-overflow-page"><a href="#iiii-overflow-page" class="headerlink" title="iiii. overflow page"></a>iiii. overflow page</h3><p>专门分配的磁盘pages，用来存储那些因为数据太长而不能放入B-tree page的可变长度列，这些可变长度列就是上面提到的off-page column。</p><h2 id="3-行格式"><a href="#3-行格式" class="headerlink" title="3. 行格式"></a>3. 行格式</h2><p>表的行格式决定了表中行是如何在物理层面上被存储的，这反过来又会影响增删查改操作的性能。当越多的行能被存储在单个page中时，那查询操作和索引的查找都会更高效，buffer pool就需要越少的缓存，更新操作就需要越少的I/O。</p><p>每个表中的数据都是被划分为很多个page的，这些page都是保存在B-tree这种数据结构中的，表中的数据和二级索引都是使用的这种数据结构。</p><p>长度较长的可变长度列由于无法存储到单个B-tree page中，只能存储到单独分配的磁盘页(overflow pagess)上。这些列也被称为off-page column。off-page columns的值存储在overflow pages的单链表中，而且每一列都有自己的列表，从这个列表中可以知道这一列的值都存储在哪些overflow page中。根据列长度的不同，会将变长列的全部值或前缀存储在B-tree中，这样就能避免page的浪费，也避免了要读取多个page的情况。</p><p>MySQL中常用的InnoDB引擎支持4中行格式：</p><ol><li>REDUNDANT</li><li>COMPACT </li><li>DYNAMIC</li><li>COMPRESSED</li></ol><p>更多关于InnoDB Row Formats的细节，参考<a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-row-format.html">这里</a>。</p><h2 id="4-页（Page）、行大小、行格式三种之间的关系"><a href="#4-页（Page）、行大小、行格式三种之间的关系" class="headerlink" title="4. 页（Page）、行大小、行格式三种之间的关系"></a>4. 页（Page）、行大小、行格式三种之间的关系</h2><p>MySQL表中行的最大长度被限制为65535字节，即使使用的存储引擎能够支持更大的行，也不能超过这个限制。</p><p>表中行的最大长度略少于数据库page大小的一半，例如，对于默认的InnoDB page大小16KB，所对应的行最大长度为略小于8KB，这个值是通过配置项innodb_page_size来设定的。</p><p>如果表中一行没有超过半个page的限制，那么整行数据都是存储在page中的；如果超过了半个page大小，那么对于可变长度列，超过限制的数据会被存储在外部off-page storage(就是上面提到的overflow page)。</p><p>而可变长度列是如何存储在off-page storage中的，又跟行格式的不同而不同：</p><ul><li>COMPACT 和 REDUNDANT行格式<br>在使用这两种行格式的情况下，当一个可变长度列被存储到外部的off-page storage中时，InnoDB引擎会把这一列的前768个字节存储在page中，剩下的数据存储在overflow pages中。每一个存储在overflow pages中的可变长度列都有一个自己的overflow pages列表。这768个字节中，有20字节用来存储这个列的真实长度和指向包含指向overflow list的指针。</li><li>DYNAMIC和COMPRESSED行格式<br>在使用这两种行格式的情况下，当一个可变长度列被存储到外部的off-page storage中时，InnoDB引擎会在page中存储一个20字节的指针，列中的剩余数据会全部存储到overflow pages中。</li></ul><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>在使用InnoDB建表时，默认的行格式为COMPACT（可通过show variables like “table_name”查看），这种行格式对应的默认page大小为16KB，那么相应每行的大小不能超过8KB。如果表中有20个列都为text类型，而且每个text类型列的值都超过了768字节，那么20 * 768字节=15360字节=15KB远大于8KB，所以必然会报错！<br>那么解决这个问题的方法就是修改行格式，以下是启用DYNAMIC行格式的步骤：</p><ol><li><p>首先是MySQL配置文件my.cnf中添加两个配置项：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">innodb_file_per_table=1 //</span><br><span class="line">innodb_file_format = Barracuda //DYNAMIC行格式只有在Barracuda文件格式下才支持</span><br></pre></td></tr></table></figure></li><li><p>修改表行格式ROW_FORMAT</p> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name ROW_FORMAT=DYNAMIC;</span><br></pre></td></tr></table></figure><p>修改之后，执行 show table status like ‘table_name’，可以看到Row_format这一列对应的值已经变成了dynamic，再写入数据的时候就不会报错了。</p></li></ol><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://dev.mysql.com/doc/mysql-reslimits-excerpt/5.7/en/column-count-limit.html">https://dev.mysql.com/doc/mysql-reslimits-excerpt/5.7/en/column-count-limit.html</a></li><li><a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-row-format.html">https://dev.mysql.com/doc/refman/5.7/en/innodb-row-format.html</a></li><li><a href="https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_dynamic_row_format">https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_dynamic_row_format</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
