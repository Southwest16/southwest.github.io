<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>15分钟搞定 Spark集群安装</title>
    <url>/2020/10/11/15%E5%88%86%E9%92%9F%E6%90%9E%E5%AE%9A-Spark%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Flink和Spark Streaming背压</title>
    <url>/2020/10/11/Flink%E5%92%8CSpark-Streaming%E8%83%8C%E5%8E%8B/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>CDH集群安装</title>
    <url>/2020/10/11/CDH%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Hadoop集群数据迁移</title>
    <url>/2020/10/11/Hadoop%E9%9B%86%E7%BE%A4%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Hive/Spark SQL常用函数(窗口分析函数、行列转换、JSON处理)</title>
    <url>/2020/10/11/Hive-Spark-SQL%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3%E5%88%86%E6%9E%90%E5%87%BD%E6%95%B0%E3%80%81%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2%E3%80%81JSON%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>JSON数据转成Spark Dataset/DataFrame</title>
    <url>/2020/10/11/JSON%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%88%90Spark-Dataset-DataFrame/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Linux Sed命令使用指南</title>
    <url>/2020/10/11/Linux-Sed%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Impala与Hive的语法区别(持续更新中...)</title>
    <url>/2020/10/11/Impala%E4%B8%8EHive%E7%9A%84%E8%AF%AD%E6%B3%95%E5%8C%BA%E5%88%AB-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E4%B8%AD/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Java死锁产生的条件及死锁判断排查</title>
    <url>/2020/10/11/Java%E6%AD%BB%E9%94%81%E4%BA%A7%E7%94%9F%E7%9A%84%E6%9D%A1%E4%BB%B6%E5%8F%8A%E6%AD%BB%E9%94%81%E5%88%A4%E6%96%AD%E6%8E%92%E6%9F%A5/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>MySQL SELECT查询非分组聚合列（sql_mode=only_full_group_by）</title>
    <url>/2020/10/11/MySQL-SELECT%E6%9F%A5%E8%AF%A2%E9%9D%9E%E5%88%86%E7%BB%84%E8%81%9A%E5%90%88%E5%88%97%EF%BC%88sql-mode-only-full-group-by%EF%BC%89/</url>
    <content><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>业务中有这样的逻辑：通过某一列col1分组之后查询col1，col2，col3这三个列，SQL语句如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> company_name, department, employee <span class="keyword">from</span> company <span class="keyword">group</span> <span class="keyword">by</span> company_name</span><br></pre></td></tr></table></figure>
<p>如果是MySQL 5.7以前的版本，是可以执行成功的；如果是MySQL 5.7及以上版本，就会报错：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ERROR 1140 (42000): In aggregated query without GROUP BY, expression... of <span class="keyword">SELECT</span> <span class="keyword">list</span> contains nonaggregated <span class="keyword">column</span> <span class="string">&#x27;col_name&#x27;</span>; this is incompatible <span class="keyword">with</span> sql_mode=only_full_group_by</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h1 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h1><p>在标准的 SQL-92中，上述的查询是不被支持的。也就是说，select列表、having条件和order by列表中是不允许出现非聚合列的，所谓非聚合列就是group by子句中的列。换句话说，就是select中查询的列，必须出现在group by子句中，否则就必须在非聚合列上使用聚合函数（比如sum()、max()等）。</p>
<p>为什么不允许这样查询呢？主要是因为分组之后，要查询非聚合列，其实是不知道该取那个值的。</p>
<p>但是MySQL对标准的SQL-92做了扩展，使得select列表、having条件和order by列表中可以引用非聚合列，这个功能可以带来更好的性能，因为它需要对非聚合列进行分组排序。不过在分组之后，查询非聚合列时，其取值是随机的，也就是组内随机取一个，就算是先通过order by进行排序之后，也是随机取值的。</p>
<p>因为MySQL 5.7之前，配置项ONLY_FULL_GROUP_BY是默认禁用的，也就是支持在select列表、having条件和order by列表中引用非聚合列且不适用聚合函数；而MySQL 5.7及之后的版本中，配置项ONLY_FULL_GROUP_BY默认是开启的，则不支持上述功能。</p>
<p>可以通过下面SQL查看ONLY_FULL_GROUP_BY是否启用：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询的结果中包含ONLY_FULL_GROUP_BY就说明是启用的，否则就是被禁用的</span></span><br><span class="line"><span class="keyword">select</span> @@global.sql_mode;</span><br></pre></td></tr></table></figure>

<p>由于本人使用的是MySQL 5.7，所以在select中查询非聚合列就出现了上述的报错</p>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>了解了问题原因和原理之后，解决起来就比较简单了。</p>
<p>通过下面SQL将sql_mode中的ONLY_FULL_GROUP_BY替换成空即可：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> <span class="keyword">global</span> sql_mode=(<span class="keyword">select</span> <span class="keyword">replace</span>(@@sql_mode, <span class="string">&#x27;ONLY_FULL_GROUP_BY&#x27;</span>, <span class="string">&#x27;&#x27;</span>));</span><br></pre></td></tr></table></figure>

<p>当然也可以设置会话级别的：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> <span class="keyword">session</span> sql_mode=(<span class="keyword">select</span> <span class="keyword">replace</span>(@@sql_mode, <span class="string">&#x27;ONLY_FULL_GROUP_BY&#x27;</span>, <span class="string">&#x27;&#x27;</span>));</span><br></pre></td></tr></table></figure>
<p>或者是通过MySQL配置文件my.cnf进行修改（需重启mysql实例）。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="https://dev.mysql.com/doc/refman/5.6/en/group-by-handling.html">https://dev.mysql.com/doc/refman/5.6/en/group-by-handling.html</a></li>
<li><a href="https://dev.mysql.com/doc/refman/5.7/en/group-by-handling.html">https://dev.mysql.com/doc/refman/5.7/en/group-by-handling.html</a></li>
<li><a href="https://dev.mysql.com/doc/refman/5.7/en/sql-mode.html#sqlmode_only_full_group_by">https://dev.mysql.com/doc/refman/5.7/en/sql-mode.html#sqlmode_only_full_group_by</a></li>
</ol>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL行记录大小超过限制 Row size too large</title>
    <url>/2020/10/10/MySQL%E8%A1%8C%E8%AE%B0%E5%BD%95%E5%A4%A7%E5%B0%8F%E8%B6%85%E8%BF%87%E9%99%90%E5%88%B6%20Row%20size%20too%20large/</url>
    <content><![CDATA[<h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>问题出现在Spark写MySQL的场景：要写入MySQL的DataFrame中有90多个列，其中有10多个列为字符串类型，且长度较长(大于1000)；对应的要写入的MySQL表使用的是InnoDB引擎，这些较大的字符串所对应的列在MySQL中设置为text类型。最终在写MySQL的时候，出现这样的报错：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: <span class="function">Row size too <span class="title">large</span> <span class="params">(&gt; <span class="number">8126</span>)</span>. Changing some columns to TEXT or BLOB or using ROW_FORMAT</span>=DYNAMIC or ROW_FORMAT=COMPRESSED may help. In current row format, BLOB prefix of <span class="number">768</span> bytes is stored inline.</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h1 id="MySQL版本"><a href="#MySQL版本" class="headerlink" title="MySQL版本"></a>MySQL版本</h1><p>本人使用的MySQL版本为5.6、InnoDB引擎，以下内容也是以这个前提来展开的，MyISAM暂不做介绍。</p>
<p>注：</p>
<ul>
<li>MySQL 5.6版本默认InnoDB文件格式为Antelope，相应配置innodb_file_format=Antelope，此种文件格式不支持COMPRESSED和DYNAMIC的行格式。</li>
<li>MySQL 5.7版本默认InnoDB文件格式为Barracuda，相应配置innodb_file_format=Barracuda，此种文件格式支持COMPRESSED和DYNAMIC的行格式。</li>
</ul>
<h1 id="名词术语"><a href="#名词术语" class="headerlink" title="名词术语"></a>名词术语</h1><p>要理解上面报错的本质，首先要了解以下概念：</p>
<ol>
<li>MySQL中的可变长度类型</li>
<li>页(page)、页大小(page size)、off-page column、overflow page？</li>
<li>页（Page）、行大小、行格式三种之间的关系</li>
</ol>
<h2 id="1-可变长度类型"><a href="#1-可变长度类型" class="headerlink" title="1. 可变长度类型"></a>1. 可变长度类型</h2><p>MySQL中的可变长度类型：VARCHAR、VARBINARY、BLOB和TEXT类型。</p>
<p>InnoDB将长度大于等于768字节的fixed-length字段当作可变长度字段，可以存储在off-page。</p>
<h2 id="2-页-page-、页大小-Page-size-、off-page-column、overflow-page"><a href="#2-页-page-、页大小-Page-size-、off-page-column、overflow-page" class="headerlink" title="2. 页(page)、页大小(Page size)、off-page column、overflow page"></a>2. 页(page)、页大小(Page size)、off-page column、overflow page</h2><h3 id="i-页-Page"><a href="#i-页-Page" class="headerlink" title="i. 页(Page)"></a>i. 页(Page)</h3><p>page代表InnoDB每次在磁盘和内存之间传输多少数据的一个单元。一个page可以包含一行或多行数据，这主要取决于每行数据的大小。如果一行记录不能全部放入到一个page中，InnoDB会用一个指针来引用这行数据。</p>
<p>可以使用COMPRESSED格式来使每个page容纳更多的数据。对于blob或者text类型的字段，COMPACT格式允许大长度的列和其他列分开存储，以便减少查询时的I/O负载和内存占用。</p>
<p>当InnoDB以批处理的方式读写一组page以增加I/O吞吐量时，它会一次读写一个区段的page。</p>
<h3 id="ii-页大小-Page-size"><a href="#ii-页大小-Page-size" class="headerlink" title="ii. 页大小(Page size)"></a>ii. 页大小(Page size)</h3><p>在MySQL 5.6版本之前，每个InnoDB page的大小都是固定的16KB，这是一个各方面取舍平衡的值：16KB能足以容纳大多数的行数据，同时也足够小到可以最小化将不必要的数据传输到内存的性能开销。</p>
<p>从MySQL 5.6开始，InnoDB page的大小可以是4KB、8KB或16KB，可通过innodb_page_size配置进行设置。在MySQL5.7.6中，InnoDB支持更大的page size(32KB和64KB)，但是这两种page size并不支持ROW_FORMAT=COMPRESSED， 并且最大记录大小为16KB。</p>
<h3 id="iii-off-page-column"><a href="#iii-off-page-column" class="headerlink" title="iii. off-page column"></a>iii. off-page column</h3><p>一个可变长度列(比如BLOB和VARCHAR)中的数据因为太大而不能放入一个B-tree page中，那么数据就会存储在overflow pages中。</p>
<h3 id="iiii-overflow-page"><a href="#iiii-overflow-page" class="headerlink" title="iiii. overflow page"></a>iiii. overflow page</h3><p>专门分配的磁盘pages，用来存储那些因为数据太长而不能放入B-tree page的可变长度列，这些可变长度列就是上面提到的off-page column。</p>
<h2 id="3-行格式"><a href="#3-行格式" class="headerlink" title="3. 行格式"></a>3. 行格式</h2><p>表的行格式决定了表中行是如何在物理层面上被存储的，这反过来又会影响增删查改操作的性能。当越多的行能被存储在单个page中时，那查询操作和索引的查找都会更高效，buffer pool就需要越少的缓存，更新操作就需要越少的I/O。</p>
<p>每个表中的数据都是被划分为很多个page的，这些page都是保存在B-tree这种数据结构中的，表中的数据和二级索引都是使用的这种数据结构。</p>
<p>长度较长的可变长度列由于无法存储到单个B-tree page中，只能存储到单独分配的磁盘页(overflow pagess)上。这些列也被称为off-page column。off-page columns的值存储在overflow pages的单链表中，而且每一列都有自己的列表，从这个列表中可以知道这一列的值都存储在哪些overflow page中。根据列长度的不同，会将变长列的全部值或前缀存储在B-tree中，这样就能避免page的浪费，也避免了要读取多个page的情况。</p>
<p>MySQL中常用的InnoDB引擎支持4中行格式：</p>
<ol>
<li>REDUNDANT</li>
<li>COMPACT </li>
<li>DYNAMIC</li>
<li>COMPRESSED</li>
</ol>
<p>更多关于InnoDB Row Formats的细节，参考<a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-row-format.html">这里</a>。</p>
<h2 id="4-页（Page）、行大小、行格式三种之间的关系"><a href="#4-页（Page）、行大小、行格式三种之间的关系" class="headerlink" title="4. 页（Page）、行大小、行格式三种之间的关系"></a>4. 页（Page）、行大小、行格式三种之间的关系</h2><p>MySQL表中行的最大长度被限制为65535字节，即使使用的存储引擎能够支持更大的行，也不能超过这个限制。</p>
<p>表中行的最大长度略少于数据库page大小的一半，例如，对于默认的InnoDB page大小16KB，所对应的行最大长度为略小于8KB，这个值是通过配置项innodb_page_size来设定的。</p>
<p>如果表中一行没有超过半个page的限制，那么整行数据都是存储在page中的；如果超过了半个page大小，那么对于可变长度列，超过限制的数据会被存储在外部off-page storage(就是上面提到的overflow page)。</p>
<p>而可变长度列是如何存储在off-page storage中的，又跟行格式的不同而不同：</p>
<ul>
<li>COMPACT 和 REDUNDANT行格式<br>在使用这两种行格式的情况下，当一个可变长度列被存储到外部的off-page storage中时，InnoDB引擎会把这一列的前768个字节存储在page中，剩下的数据存储在overflow pages中。每一个存储在overflow pages中的可变长度列都有一个自己的overflow pages列表。这768个字节中，有20字节用来存储这个列的真实长度和指向包含指向overflow list的指针。</li>
<li>DYNAMIC和COMPRESSED行格式<br>在使用这两种行格式的情况下，当一个可变长度列被存储到外部的off-page storage中时，InnoDB引擎会在page中存储一个20字节的指针，列中的剩余数据会全部存储到overflow pages中。</li>
</ul>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>在使用InnoDB建表时，默认的行格式为COMPACT（可通过show variables like “table_name”查看），这种行格式对应的默认page大小为16KB，那么相应每行的大小不能超过8KB。如果表中有20个列都为text类型，而且每个text类型列的值都超过了768字节，那么20 * 768字节=15360字节=15KB远大于8KB，所以必然会报错！<br>那么解决这个问题的方法就是修改行格式，以下是启用DYNAMIC行格式的步骤：</p>
<ol>
<li><p>首先是MySQL配置文件my.cnf中添加两个配置项：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">innodb_file_per_table=1 //</span><br><span class="line">innodb_file_format = Barracuda //DYNAMIC行格式只有在Barracuda文件格式下才支持</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改表行格式ROW_FORMAT</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name ROW_FORMAT=DYNAMIC;</span><br></pre></td></tr></table></figure>
<p>修改之后，执行 show table status like ‘table_name’，可以看到Row_format这一列对应的值已经变成了dynamic，再写入数据的时候就不会报错了。</p>
</li>
</ol>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://dev.mysql.com/doc/mysql-reslimits-excerpt/5.7/en/column-count-limit.html">https://dev.mysql.com/doc/mysql-reslimits-excerpt/5.7/en/column-count-limit.html</a></li>
<li><a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-row-format.html">https://dev.mysql.com/doc/refman/5.7/en/innodb-row-format.html</a></li>
<li><a href="https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_dynamic_row_format">https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_dynamic_row_format</a></li>
</ol>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL是如何通过变量实现组内排序的(MySQL 8.0之前版本，非row_number函数)</title>
    <url>/2020/10/11/MySQL%E6%98%AF%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E5%8F%98%E9%87%8F%E5%AE%9E%E7%8E%B0%E7%BB%84%E5%86%85%E6%8E%92%E5%BA%8F%E7%9A%84-MySQL-8-0%E4%B9%8B%E5%89%8D%E7%89%88%E6%9C%AC%EF%BC%8C%E9%9D%9Erow-number%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>业务中要实现这样的逻辑：有一张用户表，每个用户有一个或多个银行账户，每个账户有对应的流水数据，现在想要获取每个用户的每个账户下，交易金额最大的前10条流水数据，该如何实现？</p>
<a id="more"></a>

<h1 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h1><p>组内排序取Top N在业务当中是一个会经常用到的功能。如果你是在大数据框架中取组内Top N，一般的大数据框架都提供了现成的函数，比如Hive中的row_number() + partitionBy()，实现起来会比较简单。而如果你用的是关系型数据库，比如MySQL，就没有类似现成的函数来实现这个功能了(MySQL 8.0开始也提供row_number函数，但是很多公司都还是使用的MySQL5.6/5.7)。</p>
<p>那么在MySQL 8.0 之前如何实现组内排序呢？答案就是，借助变量。</p>
<p>表(bank_transaction)中部分列：<br>|列名| 类型 | 含义 |<br>|–|–|–|<br>| user_id | varchar(64) | 用户ID |<br>| bank_account | varchar(64) | 银行账户 |<br>| transaction_amount | decimal(17, 2) | 交易金额 |</p>
<p>SQL实现组内求Top N：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	user_id,</span><br><span class="line">	bank_account,</span><br><span class="line">	transaction_amount</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">	<span class="keyword">select</span></span><br><span class="line">		user_id,</span><br><span class="line">		bank_account,</span><br><span class="line">		transaction_amount,</span><br><span class="line">		@row_num := <span class="keyword">if</span>(@group01 = user_id <span class="keyword">and</span> @group02 = bank_account, @row_num + <span class="number">1</span>, <span class="number">1</span>) <span class="keyword">as</span> rn,</span><br><span class="line">		@group01 := user_id,</span><br><span class="line">		@group02 := bank_account	</span><br><span class="line">	<span class="keyword">from</span> (</span><br><span class="line">		<span class="keyword">select</span></span><br><span class="line">			user_id,</span><br><span class="line">			bank_account,</span><br><span class="line">			transaction_amount</span><br><span class="line">		<span class="keyword">from</span> bank_transaction </span><br><span class="line">		<span class="keyword">order</span> <span class="keyword">by</span> user_id, bank_account, transaction_amount <span class="keyword">desc</span></span><br><span class="line">	) t1, (<span class="keyword">select</span> @row_num := <span class="number">0</span>, @group01 := <span class="string">&quot;&quot;</span>, @group02 := <span class="string">&quot;&quot;</span>)t2</span><br><span class="line">) t</span><br><span class="line"><span class="keyword">where</span> t.rn &lt;= <span class="number">6</span></span><br></pre></td></tr></table></figure>
<p>实现思路：</p>
<ol>
<li>第一步：对数据进行组内排序，t1这个子查询里面就是对每个用户的每个账户下的所有交易金额进行降序排列</li>
<li>第二步：设置变量，@row_num变量保存的是组内排序时的编号，@group01和@group02保存的是要分组列的临时值</li>
<li>第三步：遍历已排序数据，每次拿后一行与前一行进行比较，如果后一行分组列的值和前一行的相等（对应上面SQL语句中的if条件），就表明后一行与前一行属于同一个组，那么变量@row_num的值就加一；如果不相等，就从1重新开始。以此，给所有数据编号。</li>
<li>第四步：给所有数据编号之后，通过where条件即可取出Top N的记录。</li>
</ol>
<p>另外，SQL也可以这样写，主要是变量的使用方式不同：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> @row_num := <span class="number">0</span>, @group01 := <span class="string">&quot;&quot;</span>, @group02 := <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">	user_id,</span><br><span class="line">	bank_account,</span><br><span class="line">	transaction_amount</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">	<span class="keyword">select</span></span><br><span class="line">		user_id,</span><br><span class="line">		bank_account,</span><br><span class="line">		transaction_amount,</span><br><span class="line">		@row_num := <span class="keyword">if</span>(@group01 = user_id <span class="keyword">and</span> @group02 = bank_account, @row_num + <span class="number">1</span>, <span class="number">1</span>) <span class="keyword">as</span> rn,</span><br><span class="line">		@group01 := user_id,</span><br><span class="line">		@group02 := bank_account	</span><br><span class="line">	<span class="keyword">from</span> (</span><br><span class="line">		<span class="keyword">select</span></span><br><span class="line">			user_id,</span><br><span class="line">			bank_account,</span><br><span class="line">			transaction_amount</span><br><span class="line">		<span class="keyword">from</span> bank_transaction </span><br><span class="line">		<span class="keyword">order</span> <span class="keyword">by</span> user_id, bank_account, transaction_amount <span class="keyword">desc</span></span><br><span class="line">	) t1</span><br><span class="line">) t</span><br><span class="line"><span class="keyword">where</span> t.rn &lt;= <span class="number">6</span></span><br></pre></td></tr></table></figure>

<p>Hive中如何实现组内排序求Top N，可以参考<a href="https://editor.csdn.net/md/?articleId=107349381">这里</a>。</p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL表同步到Hive</title>
    <url>/2020/10/11/MySQL%E8%A1%A8%E5%90%8C%E6%AD%A5%E5%88%B0Hive/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Python for循环中添加字典到列表，结果列表中全是循环中的最后一个值</title>
    <url>/2020/10/11/Python-for%E5%BE%AA%E7%8E%AF%E4%B8%AD%E6%B7%BB%E5%8A%A0%E5%AD%97%E5%85%B8%E5%88%B0%E5%88%97%E8%A1%A8%EF%BC%8C%E7%BB%93%E6%9E%9C%E5%88%97%E8%A1%A8%E4%B8%AD%E5%85%A8%E6%98%AF%E5%BE%AA%E7%8E%AF%E4%B8%AD%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AA%E5%80%BC/</url>
    <content><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>问题的是在Python解析Excel文件出现的。</p>
<a id="more"></a>

<p>下面是出现问题的一段代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 此函数实现了解析Excel文件，并将文件中的每行数据以字典的形式返回(key为列名，value为对应的值)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span>():</span></span><br><span class="line">	...</span><br><span class="line">	<span class="keyword">yield</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将解析返回的每行数据追加到一个列表中</span></span><br><span class="line">list = []</span><br><span class="line"><span class="keyword">for</span> dic <span class="keyword">in</span> func():</span><br><span class="line">	list.append(dic)</span><br><span class="line"></span><br><span class="line">print(list)</span><br></pre></td></tr></table></figure>
<p>上面代码执行之后，输出的list中，发现所有元素的内容都是相同的，再去对比源文件，发现列表的元素都为Excel文件中最后一行内容。很明显，这肯定是不对的！</p>
<h1 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h1><p>出现这个问题之后，首先想到的就是逐步调试这段循环代码，同时观察dic的值和list中元素的值。</p>
<ul>
<li>第一次循环时，list中对应的确实是Excel中第一行的内容，</li>
<li>第二次循环时，在dic已改变，但还没往list中追加第二个元素之前，发现第一次追加到list中的的元素的值已经发生了改变，而且改变之后的内容就和第二次循环时dic的内容相同。</li>
</ul>
<p>到这里，就恍然大悟了，原来每次往列表中追加的都是这个字典的引用，而不是字典的内容，所以才会在每次循环时，出现只要dic的内容发生了变化，list中之前追加进去的dic也随着发生改变的情况。</p>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>知道了这个原因，我们就知道在追加列表时，针对Excel的每行数据，都应该追加不同的引用。</p>
<p>那么具体的实现就是用到字典的copy()方法，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list = []</span><br><span class="line"><span class="keyword">for</span> dic <span class="keyword">in</span> func():</span><br><span class="line">	list.append(dic.copy())</span><br></pre></td></tr></table></figure>
<p>copy()方法对字典dic进行了一个浅拷贝，也就是说对字典创建了一个新的引用。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://docs.python.org/3.7/library/copy.html">https://docs.python.org/3.7/library/copy.html</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title>Python解析Excel、PDF、HTML、CSV...文件</title>
    <url>/2020/10/11/Python%E8%A7%A3%E6%9E%90Excel%E3%80%81PDF%E3%80%81HTML%E3%80%81CSV-%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<p>Python在文件解析方面有很多丰富的三方模块，接下来我们就看如何使用Python解析Excel、PDF、HTML、CSV这几种常见格式的文件。</p>
<a id="more"></a>

<h1 id="1-Excel文件解析"><a href="#1-Excel文件解析" class="headerlink" title="1. Excel文件解析"></a>1. Excel文件解析</h1><p>标准的Excel文件其实就相当于一张表，有表头和对应的数据。一般常见的Excel文件都以.xls、.xlsx和.et结尾。</p>
<p>以下是使用xlrd模块实现解析Excel文件的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> xlrd</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xls_parse</span>(<span class="params">file_data</span>):</span></span><br><span class="line">    <span class="comment"># 打开一个电子表格文件进行数据提取</span></span><br><span class="line">    data = xlrd.open_workbook(file_contents=a bytes object, encoding_override=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="comment"># 一个Excel文件中所有sheet的名称</span></span><br><span class="line">    sheets = data.sheet_names()</span><br><span class="line">    <span class="comment"># 通过sheet名获取对应的sheet表内容</span></span><br><span class="line">    table = data.sheet_by_name(sheets[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    start_row = <span class="number">0</span>  <span class="comment"># 起始行</span></span><br><span class="line">    rows = table.nrows  <span class="comment"># 表格总行数</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 假如第一行是表头</span></span><br><span class="line">	header = table.row_values(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历表记录</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, rows):</span><br><span class="line">        <span class="comment"># 表记录值与对应的表头组成字典</span></span><br><span class="line">        col_dic = dict(zip(header, table.row_values(i)))</span><br><span class="line">        </span><br><span class="line">        raw_item = json.dumps(col_dic, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">yield</span> raw_item  <span class="comment"># 返回json格式</span></span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h1 id="2-PDF文件解析"><a href="#2-PDF文件解析" class="headerlink" title="2. PDF文件解析"></a>2. PDF文件解析</h1><p>PDF相对来说是比较难的解析，除非PDF中有类似Excel格式的数据，否则解析出来的数据很难进行规范化。Python中也有很多解析pdf文件的第三方库。比如tabula等</p>
<p>tabula的解析代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> tabula</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pdf_parse</span>(<span class="params">file_data</span>):</span></span><br><span class="line">    <span class="comment"># 提取PDF文件中的表格</span></span><br><span class="line">    tables = tabula.read_pdf(file_data, pages=<span class="string">&quot;all&quot;</span>)  <span class="comment"># file_data为BytesIO对象</span></span><br><span class="line">    <span class="comment"># 遍历提取出的所有表格</span></span><br><span class="line">    <span class="keyword">for</span> df <span class="keyword">in</span> tables:</span><br><span class="line">        <span class="comment"># 提取表头</span></span><br><span class="line">        columns = list(df.columns)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 按列切分表格记录</span></span><br><span class="line">        df_new = pd.DataFrame(columns=columns)</span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> columns:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment"># DataFrame转JSON</span></span><br><span class="line">        json_array = json.loads(df_new.to_json(orient=<span class="string">&quot;records&quot;</span>, force_ascii=<span class="literal">False</span>))</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> json_array:</span><br><span class="line">            <span class="keyword">yield</span> json.dumps(item, ensure_ascii=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>除了tabula之外，本人还是测试了很多其他的第三方模块，比如pdfminer.six、pdfminer3k、pdfplumber、PyPDF2、slate3k、camelot。要使用哪个模块，主要依赖于pdf文件内容的规范程度，总的来说推荐使用pdfplumber或者tabula。</p>
<p>&nbsp;</p>
<h1 id="3-HTML文件解析"><a href="#3-HTML文件解析" class="headerlink" title="3. HTML文件解析"></a>3. HTML文件解析</h1><p>HTML格式的文件相当于是半结构的数据，解析相对来说比较简单。</p>
<p>html文件的解析代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> lxml.etree <span class="keyword">import</span> HTML</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">html_parse</span>(<span class="params">file_data</span>):</span></span><br><span class="line">    <span class="comment"># 从一个字符串常量中解析HTML文档</span></span><br><span class="line">    html = HTML(file_data.read())</span><br><span class="line">    html_tables = html.findall(<span class="string">&quot;body/table&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将HTML中的所有&lt;table&gt;都放到一个列表中, 以便后续统一处理</span></span><br><span class="line">    table_rows = []</span><br><span class="line">    <span class="keyword">for</span> html_table <span class="keyword">in</span> html_tables:</span><br><span class="line">        table_rows.extend(list(html_table))</span><br><span class="line"></span><br><span class="line">    rows = len(table_rows)  <span class="comment"># 表格总行数</span></span><br><span class="line">    start_row = <span class="number">0</span>  <span class="comment"># 起始行</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 假设第一行是表头</span></span><br><span class="line">    header = [col.text <span class="keyword">for</span> col <span class="keyword">in</span> table_rows[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历表记录</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(start_row, rows):</span><br><span class="line">        values = [col.text <span class="keyword">for</span> col <span class="keyword">in</span> table_rows[i]]</span><br><span class="line">        <span class="comment"># 表记录值与对应的列名组成字典</span></span><br><span class="line">        col_dic = dict(zip(header, values))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 字典转json</span></span><br><span class="line">        raw_item = json.dumps(col_dic, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">yield</span> raw_item</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>&nbsp;</p>
<h1 id="4-CSV文件解析"><a href="#4-CSV文件解析" class="headerlink" title="4. CSV文件解析"></a>4. CSV文件解析</h1><p>csv是默认以逗号(,)分割的一行行数据组成的文件。通常情况，csv文件都可以通过Excel软件打开，正常显示为表格。</p>
<p>csv文件解析代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">csv_parse</span>(<span class="params">file_data</span>):</span></span><br><span class="line">    file_bytes_obj = file_data.read()  <span class="comment"># file_data为BytesIO对象</span></span><br><span class="line">	<span class="comment"># 根据文件内容检测编码格式有时候会不准确, 这时可以尝试更范围的编码格式</span></span><br><span class="line">	<span class="comment"># 范围: gb18030 &gt; gb2312 &gt; gbk</span></span><br><span class="line">    encode = chardet.detect(file_bytes_obj)[<span class="string">&quot;encoding&quot;</span>]</span><br><span class="line"></span><br><span class="line">    file_string_io = StringIO(file_bytes_obj.decode(encode).replace(<span class="string">&#x27;\r&#x27;</span>, <span class="string">&#x27;\r\n&#x27;</span>))</span><br><span class="line">    csv_data = list(csv.reader(file_string_io))</span><br><span class="line"></span><br><span class="line">    start_row = <span class="number">-1</span>  <span class="comment"># 起始行</span></span><br><span class="line">    rows = len(csv_data)  <span class="comment"># 表格总行数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 表头</span></span><br><span class="line">    header = csv_data[i]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历表记录</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(start_row, rows):</span><br><span class="line">        col_dic = dict(zip(header, csv_data[i]))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 字典转json</span></span><br><span class="line">        raw_item = json.dumps(col_dic, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">yield</span> raw_item</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>如有遇到其他格式的文件，之后会持续更新到这篇文章中！</p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python使用POP3协议客户端poplib登录邮箱并解压缩zip、rar压缩包</title>
    <url>/2020/10/11/Python%E4%BD%BF%E7%94%A8POP3%E5%8D%8F%E8%AE%AE%E5%AE%A2%E6%88%B7%E7%AB%AFpoplib%E7%99%BB%E5%BD%95%E9%82%AE%E7%AE%B1%E5%B9%B6%E8%A7%A3%E5%8E%8B%E7%BC%A9zip%E3%80%81rar%E5%8E%8B%E7%BC%A9%E5%8C%85/</url>
    <content><![CDATA[<h1 id="1-项目背景"><a href="#1-项目背景" class="headerlink" title="1. 项目背景"></a>1. 项目背景</h1><p>目前在做的一个小项目，需要到登录到邮箱获取压缩包，解压压缩包获取文件，并从文件中抽取出有效数据入库，后续会做一些二次加工，最后用到业务风控中。</p>
<a id="more"></a>

<h1 id="2-poplib模块"><a href="#2-poplib模块" class="headerlink" title="2. poplib模块"></a>2. poplib模块</h1><p>Python内建了poplib模块来实现登录邮箱，这个模块提供了一个类POP3_SSL，它支持使用SSL(Secure Socket Layer，安全套阶层，在OSI模型处于会话层)作为底层协议连接POP3服务器。</p>
<h1 id="3-邮箱登录"><a href="#3-邮箱登录" class="headerlink" title="3. 邮箱登录"></a>3. 邮箱登录</h1><p>邮箱登录代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> poplib</span><br><span class="line"></span><br><span class="line">\<span class="comment"># 通过主机名、端口生成POP3对象</span></span><br><span class="line">pop = poplib.POP3_SSL(host=<span class="string">&quot;&quot;</span>, port=<span class="string">&quot;&quot;</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    \<span class="comment"># 通过用户名、密码登录邮箱</span></span><br><span class="line">    pop.user(<span class="string">&quot;&quot;</span>)</span><br><span class="line">    pop.pass_(<span class="string">&quot;&quot;</span>)</span><br><span class="line"><span class="keyword">except</span> poplib.error_proto <span class="keyword">as</span> e:</span><br><span class="line">    logger.error(<span class="string">&quot;Login failed: &quot;</span> + e)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    parse(pop, sys.argv)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    \<span class="comment"># 退出邮箱</span></span><br><span class="line">    pop.quit()</span><br></pre></td></tr></table></figure>
<p>注意，最后程序不管是正常结束还是非正常结束都一定要退出邮箱。</p>
<h1 id="4-获取邮件内容"><a href="#4-获取邮件内容" class="headerlink" title="4. 获取邮件内容"></a>4. 获取邮件内容</h1><p>获取指定邮件有多种方式，可以通过邮件发件人、邮件发件邮箱地址、邮件主题，还可以用邮件索引，使用邮件索引速度最快，这种获取方式就跟在散列表中根据key获取对应value很相似。</p>
<h2 id="1-获取邮件基本信息"><a href="#1-获取邮件基本信息" class="headerlink" title="1.  获取邮件基本信息"></a>1.  获取邮件基本信息</h2><p>登录邮箱之后，可以获取很多与邮件相关的基本信息，比如邮箱邮件列表、邮件主题、邮件内容、邮件发件人及发件地址、邮件接收时间等等</p>
<p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> email.parser <span class="keyword">import</span> Parser</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_mail_list</span>(<span class="params">pop</span>):</span></span><br><span class="line">    <span class="comment"># 查找附件, 并返回下载文件路径</span></span><br><span class="line">    emails = pop.list()[<span class="number">1</span>]  <span class="comment"># 邮箱邮件列表</span></span><br><span class="line">    email_num = len(emails)  <span class="comment"># 邮件数量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>]:</span><br><span class="line">        lines = pop.retr(i)[<span class="number">1</span>]  <span class="comment"># 获取指定的邮件</span></span><br><span class="line">        line_bytes = <span class="string">b&#x27;\r\n&#x27;</span>.join(lines)  <span class="comment"># 换行符分割邮件信息</span></span><br><span class="line">        msg_content = line_bytes.decode(<span class="string">&#x27;utf-8&#x27;</span>)  <span class="comment"># 解码</span></span><br><span class="line">        msg = Parser().parsestr(msg_content)  <span class="comment"># 每封邮件信息</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 发件人和邮箱地址</span></span><br><span class="line">        header, address = parseaddr(msg.get(<span class="string">&quot;From&quot;</span>, <span class="string">&quot;&quot;</span>)) </span><br><span class="line">        name, address = decode_str(header), decode_str(address)</span><br><span class="line">        <span class="comment"># 邮件主题</span></span><br><span class="line">        email_subject = decode_str(msg.get(<span class="string">&#x27;Subject&#x27;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 邮件接收时间</span></span><br><span class="line">        date_tuple = parsedate_tz(msg.get(<span class="string">&quot;Date&quot;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line">        date_formatted = datetime.fromtimestamp(mktime_tz(date_tuple)).strftime(<span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        compressed_files = list()</span><br><span class="line">        <span class="keyword">for</span> compressed_file <span class="keyword">in</span> get_attachment(msg, compressed_files):</span><br><span class="line">            <span class="keyword">yield</span> compressed_file</span><br></pre></td></tr></table></figure>

<h2 id="2-获取邮件中的附件"><a href="#2-获取邮件中的附件" class="headerlink" title="2. 获取邮件中的附件"></a>2. 获取邮件中的附件</h2><p>获取邮件附件中的zip或rar压缩包，<font color=red>注意：这里要考虑到一封邮件中有多个压缩包的情况。</font></p>
<p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_attachment</span>(<span class="params">msg, ompressed_files: list</span>):</span></span><br><span class="line">    <span class="keyword">if</span> msg.is_multipart() <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">        \<span class="comment"># 分层信息</span></span><br><span class="line">        parts = msg.get_payload()</span><br><span class="line">        <span class="keyword">for</span> n, part <span class="keyword">in</span> enumerate(parts):</span><br><span class="line">            res = get_attachment(part, compressed_files)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        content_type = msg.get_content_type()</span><br><span class="line">        <span class="keyword">if</span> content_type == <span class="string">&#x27;application/octet-stream&#x27;</span>:</span><br><span class="line">            <span class="keyword">for</span> subpart <span class="keyword">in</span> msg.walk():  \<span class="comment"># 遍历消息树(深度优先搜索算法), 获取每个子节点</span></span><br><span class="line">                file_name_encoder = subpart.get_filename()</span><br><span class="line">                file_name = decode_str(file_name_encoder)  \<span class="comment"># 解码获取文件名</span></span><br><span class="line">                \<span class="comment"># 判断是否是zip或rar格式文件</span></span><br><span class="line">                <span class="keyword">if</span> file_name.split(<span class="string">&quot;.&quot;</span>)[<span class="number">-1</span>] <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;zip&#x27;</span>, <span class="string">&quot;rar&quot;</span>]:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                data = msg.get_payload(decode=<span class="literal">True</span>)  \<span class="comment"># 附件二进制</span></span><br><span class="line">                file_data = base64.b64encode(data).decode()  \<span class="comment"># base64编码</span></span><br><span class="line">                compressed_files.append(&#123;<span class="string">&quot;file_data&quot;</span>: file_data, <span class="string">&quot;name&quot;</span>: file_name&#125;)  \<span class="comment"># 保存到列表</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> compressed_files</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode_str</span>(<span class="params">s</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    解码</span></span><br><span class="line"><span class="string">    :param s:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    value, charset = decode_header(s)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> charset:</span><br><span class="line">        value = value.decode(charset)</span><br><span class="line">    <span class="keyword">return</span> value</span><br></pre></td></tr></table></figure>


<h1 id="5-解压zip-rar压缩包"><a href="#5-解压zip-rar压缩包" class="headerlink" title="5. 解压zip/rar压缩包"></a>5. 解压zip/rar压缩包</h1><p>在获取到邮件附件的二进制内容之后，就可以使用zipfile模块和rarfile模块解解压zip和rar压缩包了。</p>
<h2 id="1-打开zip-rar压缩包"><a href="#1-打开zip-rar压缩包" class="headerlink" title="1. 打开zip/rar压缩包"></a>1. 打开zip/rar压缩包</h2><p>处理步骤4中获取的邮件附件，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> rarfile</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">pop</span>):</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> get_mail_list(pop):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> data[<span class="string">&#x27;name&#x27;</span>].endswith(<span class="string">&quot;.zip&quot;</span>):  <span class="comment"># zip格式的压缩文件</span></span><br><span class="line">                zip_obj = zipfile.ZipFile(BytesIO(base64.b64decode(data[<span class="string">&#x27;file_data&#x27;</span>].encode())))</span><br><span class="line">                <span class="keyword">for</span> record <span class="keyword">in</span> get_files_in_zip(zip_obj, item):</span><br><span class="line">                    <span class="keyword">pass</span>  <span class="comment"># 数据处理</span></span><br><span class="line">            <span class="keyword">elif</span> data[<span class="string">&#x27;name&#x27;</span>].endswith(<span class="string">&quot;.rar&quot;</span>):  <span class="comment"># rar格式的压缩文件</span></span><br><span class="line">                rar_obj = rarfile.RarFile(BytesIO(base64.b64decode(data[<span class="string">&#x27;file_data&#x27;</span>].encode())))</span><br><span class="line">                <span class="keyword">for</span> record <span class="keyword">in</span> get_files_in_rar(rar_obj, item):</span><br><span class="line">					<span class="keyword">pass</span>  <span class="comment"># 数据处理</span></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            logger.error(traceback.format_exc())</span><br></pre></td></tr></table></figure>

<h2 id="2-获取压缩包中文件"><a href="#2-获取压缩包中文件" class="headerlink" title="2. 获取压缩包中文件"></a>2. 获取压缩包中文件</h2><p>打开压缩包之后，便可迭代获取压缩包中文件。不过这里要注意的是，zip/rar压缩中可能会嵌套压缩包，比如zip压缩包中嵌套zip压缩包或rar压缩包，那么就需要使用递归进行解压缩。</p>
<p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> rarfile</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_files_in_zip</span>(<span class="params">zip_obj</span>):</span></span><br><span class="line">    <span class="comment"># 获取压缩包中文件列表, 同时过滤一些空目录</span></span><br><span class="line">    files = [file <span class="keyword">for</span> file <span class="keyword">in</span> zip_obj.namelist() <span class="keyword">if</span> re.search(<span class="string">&quot;\\..*$&quot;</span>, file)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历每个压缩包中的所有文件</span></span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">        <span class="comment"># 压缩包中嵌套压缩包</span></span><br><span class="line">        <span class="keyword">if</span> file.endswith(<span class="string">&quot;zip&quot;</span>):</span><br><span class="line">            inner_zip_obj = zipfile.ZipFile(BytesIO(base64.b64decode(base64.b64encode(zip_obj.read(file)).decode().encode())))</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> get_files_in_zip(inner_zip_obj)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">elif</span> file.endswith(<span class="string">&quot;rar&quot;</span>):</span><br><span class="line">            inner_rar_obj = rarfile.RarFile(BytesIO(base64.b64decode(base64.b64encode(zip_obj.read(file)).decode().encode())))</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> get_files_in_rar(inner_rar_obj)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">		<span class="keyword">pass</span>  <span class="comment"># 压缩中文件处理</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_files_in_rar</span>(<span class="params">rar_obj</span>):</span></span><br><span class="line">    <span class="comment"># 获取压缩包中文件列表, 同时过滤一些空目录</span></span><br><span class="line">    files = [file <span class="keyword">for</span> file <span class="keyword">in</span> rar_obj.namelist() <span class="keyword">if</span> re.search(<span class="string">&quot;\\..*$&quot;</span>, file)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析每个文件内容</span></span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">        <span class="comment"># 压缩包中嵌套压缩包</span></span><br><span class="line">        <span class="keyword">if</span> file.endswith(<span class="string">&quot;rar&quot;</span>):</span><br><span class="line">            inner_rar_obj = rarfile.RarFile(BytesIO(base64.b64decode(base64.b64encode(rar_obj.read(file)).decode().encode())))</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> get_files_in_rar(inner_rar_obj)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">elif</span> file.endswith(<span class="string">&quot;zip&quot;</span>):</span><br><span class="line">            inner_zip_obj = zipfile.ZipFile(BytesIO(base64.b64decode(base64.b64encode(rar_obj.read(file)).decode().encode())))</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> get_files_in_zip(inner_zip_obj)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">		<span class="keyword">pass</span>  <span class="comment"># 压缩中文件处理</span></span><br></pre></td></tr></table></figure>

<p>解压获取压缩包中文件之后，就可以从文件(这里主要是pdf、html、excel、csv等格式的文件)中提取数据了。</p>
<p>不同格式的文件如何解析，请参考我的<a href="https://blog.csdn.net/lovetechlovelife/article/details/107499638">另一篇文章</a>。</p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>邮箱</tag>
        <tag>POP</tag>
        <tag>ZIP</tag>
        <tag>RAR</tag>
        <tag>压缩包</tag>
      </tags>
  </entry>
  <entry>
    <title>Python读写Phoenix</title>
    <url>/2020/10/11/Python%E8%AF%BB%E5%86%99Phoenix/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Python读写Elasticsearch、HBase、Hive</title>
    <url>/2020/10/11/Python%E8%AF%BB%E5%86%99Elasticsearch%E3%80%81HBase%E3%80%81Hive/</url>
    <content><![CDATA[<h2 id="1-Elasticsearch"><a href="#1-Elasticsearch" class="headerlink" title="1. Elasticsearch"></a>1. Elasticsearch</h2><p>安装Elasticsearch模块：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip3 install elasticsearch==7.1.0</span><br></pre></td></tr></table></figure>

<p>读取ES索引的代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> elasticsearch <span class="keyword">import</span> Elasticsearch</span><br><span class="line"></span><br><span class="line">es = Elasticsearch(hosts=[&#123;<span class="string">&quot;host&quot;</span>: <span class="string">&quot;host&quot;</span>, <span class="string">&quot;port&quot;</span>: <span class="number">9200</span>&#125;])</span><br><span class="line">result = es.search(index=<span class="string">&quot;index_name&quot;</span>, body=&#123;<span class="string">&quot;query&quot;</span>: &#123;<span class="string">&quot;match_all&quot;</span>: &#123;&#125;&#125;&#125;)</span><br><span class="line"><span class="keyword">for</span> hit <span class="keyword">in</span> result[<span class="string">&#x27;hits&#x27;</span>][<span class="string">&#x27;hits&#x27;</span>]:</span><br><span class="line">    print(hit[<span class="string">&quot;_source&quot;</span>])</span><br></pre></td></tr></table></figure>

<h2 id="2-HBase"><a href="#2-HBase" class="headerlink" title="2. HBase"></a>2. HBase</h2><p>安装Python连接HBase的模块：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip3 install happybase</span><br></pre></td></tr></table></figure>

<p>读取HBase表的代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> happybase</span><br><span class="line"></span><br><span class="line">connection = happybase.Connection(<span class="string">&#x27;host&#x27;</span>)</span><br><span class="line">connection.open()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印所有的表</span></span><br><span class="line">print(connection.tables())</span><br><span class="line"></span><br><span class="line">table = connection.table(<span class="string">&#x27;table_name&#x27;</span>)</span><br><span class="line">print(table.families())</span><br><span class="line"></span><br><span class="line">row = table.row(<span class="string">&#x27;row_key&#x27;</span>, columns=[<span class="string">b&#x27;cf:col1&#x27;</span>, <span class="string">b&#x27;cf:col2&#x27;</span>, <span class="string">b&#x27;cf:col3&#x27;</span>])</span><br><span class="line">print(bytes(row[<span class="string">b&#x27;cf:col1&#x27;</span>]).decode(<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line"></span><br><span class="line">connection.close()</span><br></pre></td></tr></table></figure>

<h2 id="3-Hive"><a href="#3-Hive" class="headerlink" title="3. Hive"></a>3. Hive</h2><p>安装Python连接Hive的模块：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip3 install pyhive</span><br></pre></td></tr></table></figure>
<p>如果安装pyhive过程出错，请参考<a href="https://blog.csdn.net/lovetechlovelife/article/details/97128463">pyhive依赖模块sasl安装(pip install sasl)报错Microsoft Visual C++ 14.0 is required</a>。</p>
<p>读取Hive表的代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyhive <span class="keyword">import</span> hive</span><br><span class="line"></span><br><span class="line">conn = hive.Connection(host=<span class="string">&quot;master-1&quot;</span>, port=<span class="number">10000</span>, username=<span class="string">&quot;root&quot;</span>, auth=<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">cursor.execute(<span class="string">&quot;select * from db.table limit 10&quot;</span>)</span><br><span class="line">print(cursor.fetchall())</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Elasticsearch</tag>
        <tag>HBase</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Connector连接器之整合读写Elasticsearch</title>
    <url>/2020/10/11/Spark-Connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99Elasticsearch/</url>
    <content><![CDATA[<p>Elasticsearch提供了对Spark的支持，可以将ES中的索引（一行行的JSON数据）加载为RDD或DataFrame。</p>
<a id="more"></a>

<p>在使用elasticsearch-spark插件之前，需要在项目中添加依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.elasticsearch<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>elasticsearch-spark-20_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>7.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>


<h1 id="1-ES索引加载为RDD"><a href="#1-ES索引加载为RDD" class="headerlink" title="1. ES索引加载为RDD"></a>1. ES索引加载为RDD</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.elasticsearch.spark._</span><br><span class="line"></span><br><span class="line">val spark = SparkSession</span><br><span class="line">	.builder()</span><br><span class="line">	.config(<span class="string">&quot;hive.metastore.uris&quot;</span>, <span class="string">&quot;thrift://master-1:9083&quot;</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment">//es配置</span></span><br><span class="line">	.config(<span class="string">&quot;es.nodes&quot;</span>, <span class="string">&quot;node-1&quot;</span>) </span><br><span class="line">	.config(<span class="string">&quot;es.port&quot;</span>, <span class="string">&quot;9200&quot;</span>)</span><br><span class="line">	.config(<span class="string">&quot;pushdown&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line"></span><br><span class="line">	.appName(<span class="string">&quot;Spark ES&quot;</span>)</span><br><span class="line">	.enableHiveSupport()</span><br><span class="line">	.getOrCreate()</span><br><span class="line"></span><br><span class="line">val rdd = spark.sparkContext.esJsonRDD(<span class="string">&quot;index_name&quot;</span>)</span><br><span class="line"></span><br><span class="line">spark.close()</span><br></pre></td></tr></table></figure>
<p>配置项”es.nodes”的值只要给出ES集群中的一个数据节点即可，只要给出一个节点，ES就能找到集群中的其他节点。数据节点就是ES安装目录下配置文件(../elasticsearch-7.1.0/config/elasticsearch.yml)中”node.master”为true的节点。</p>
<p>Spark从ES加载出来的数据是JSON String类型的RDD，根据请求体的结构就可以取出来具体的数据。</p>
<h1 id="2-ES索引加载为DataFrame"><a href="#2-ES索引加载为DataFrame" class="headerlink" title="2. ES索引加载为DataFrame"></a>2. ES索引加载为DataFrame</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val spark = SparkSession</span><br><span class="line">	.builder()</span><br><span class="line">	.config(<span class="string">&quot;hive.metastore.uris&quot;</span>, <span class="string">&quot;thrift://master-1:9083&quot;</span>)</span><br><span class="line">	.appName(<span class="string">&quot;Spark ES&quot;</span>)</span><br><span class="line">	.enableHiveSupport()</span><br><span class="line">	.getOrCreate()</span><br><span class="line"></span><br><span class="line">val options = Map(</span><br><span class="line">	<span class="string">&quot;es.nodes&quot;</span> -&gt; <span class="string">&quot;node-1&quot;</span>,</span><br><span class="line">	<span class="string">&quot;es.port&quot;</span> -&gt; <span class="string">&quot;9200&quot;</span>,</span><br><span class="line">	<span class="string">&quot;pushdown&quot;</span> -&gt; <span class="string">&quot;true&quot;</span>)</span><br><span class="line"></span><br><span class="line">val df = spark.read.format(<span class="string">&quot;org.elasticsearch.spark.sql&quot;</span>)</span><br><span class="line">	.options(options)</span><br><span class="line">	.load(<span class="string">&quot;index_name&quot;</span>)</span><br><span class="line">	.where(<span class="string">&quot;col = 100&quot;</span>)</span><br><span class="line">	.select(<span class="string">&quot;col1&quot;</span>, <span class="string">&quot;col2&quot;</span>, <span class="string">&quot;col3&quot;</span>)</span><br><span class="line"></span><br><span class="line">spark.close()</span><br></pre></td></tr></table></figure>
<p>将ES索引加载成DataFrame后便可以方便地进行表操作了。</p>
<p>最后注意提交Spark任务时加上第三方依赖包elasticsearch-spark-20_2.11-7.1.0.jar。</p>
<p>参考</p>
<ol>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html">https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html</a></li>
</ol>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Connector连接器之整合读写HBase</title>
    <url>/2020/10/11/Spark-Connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99HBase/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Spark Connector连接器之整合读写Phoenix</title>
    <url>/2020/10/11/Spark-Connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99Phoenix/</url>
    <content><![CDATA[<p>Phoenix为NoSQL数据库HBase提供了标准SQL和JDBC API的强大功能，且具备完整的ACID事务处理能力。对于小数据量的查询，其性能可以达到毫秒级别；对于数千万行的数据，其性能也可以达到秒级。</p>
<a id="more"></a>

<p>要使用phoenix-spark插件，需要在项目中添加如下依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-spark<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.1-HBase-1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>Spark版本为2.3.1，HBase版本为1.2.0，Phoenix版本为4.14.1。</p>
<h2 id="Spark加载Phoenix表"><a href="#Spark加载Phoenix表" class="headerlink" title="Spark加载Phoenix表"></a>Spark加载Phoenix表</h2><p>方法一：使用数据源API加载Phoenix表为一个DataFrame</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">def <span class="title">readPhoenix</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val df = spark.read</span><br><span class="line">        .format(<span class="string">&quot;org.apache.phoenix.spark&quot;</span>)</span><br><span class="line">        .options(Map(<span class="string">&quot;table&quot;</span> -&gt; <span class="string">&quot;TEST&quot;</span>, <span class="string">&quot;zkUrl&quot;</span> -&gt; <span class="string">&quot;host:2181&quot;</span>))</span><br><span class="line">        .load()</span><br><span class="line"></span><br><span class="line">    df.show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>方法二：使用Configuration对象加载Phoenix表为一个DataFrame</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.phoenix.spark._</span><br><span class="line"></span><br><span class="line"><span class="function">def <span class="title">readPhoenix</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val conf = <span class="keyword">new</span> Configuration()</span><br><span class="line">    conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hostname:2181&quot;</span>)</span><br><span class="line">    val df = spark.sqlContext.phoenixTableAsDataFrame(</span><br><span class="line">        <span class="string">&quot;test&quot;</span>, <span class="comment">//表名</span></span><br><span class="line">        Seq(<span class="string">&quot;NAME&quot;</span>, <span class="string">&quot;AGE&quot;</span>), <span class="comment">//指定要加载的列名</span></span><br><span class="line">        predicate = Some(<span class="string">&quot;PHONE = 13012340000&quot;</span>), <span class="comment">//可设置where条件</span></span><br><span class="line">        conf = conf)</span><br><span class="line"></span><br><span class="line">    df.show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>方法三：使用Zookeeper URL加载Phoenix表为一个RDD</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.phoenix.spark._</span><br><span class="line"></span><br><span class="line"><span class="function">def <span class="title">readPhoenix</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val rdd =spark.sparkContext.phoenixTableAsRDD(</span><br><span class="line">        <span class="string">&quot;TEST&quot;</span>,</span><br><span class="line">        Seq(<span class="string">&quot;NAME&quot;</span>, <span class="string">&quot;AGE&quot;</span>),</span><br><span class="line">        predicate = Some(<span class="string">&quot;PHONE = 13012340000&quot;</span>),</span><br><span class="line">        zkUrl = Some(<span class="string">&quot;hostname:2181&quot;</span>) <span class="comment">//Zookeeper URL来连接Phoenix</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    rdd.map(_.get(<span class="string">&quot;&quot;</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="Spark持久化数据到Phoenix"><a href="#Spark持久化数据到Phoenix" class="headerlink" title="Spark持久化数据到Phoenix"></a>Spark持久化数据到Phoenix</h2><p>创建要导入的Phoenix表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">TEST</span>(<span class="keyword">id</span> <span class="built_in">BIGINT</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> PRIMARY <span class="keyword">KEY</span>, col1 <span class="built_in">VARCHAR</span>, col2 <span class="built_in">INTEGER</span>);</span><br></pre></td></tr></table></figure>
<h3 id="保存RDD到Phoenix"><a href="#保存RDD到Phoenix" class="headerlink" title="保存RDD到Phoenix"></a>保存RDD到Phoenix</h3><p>代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.phoenix.spark._</span><br><span class="line"></span><br><span class="line"><span class="function">def <span class="title">writePhoenix</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val dataSet = List((<span class="number">1L</span>, <span class="string">&quot;1&quot;</span>, <span class="number">1</span>), (<span class="number">2L</span>, <span class="string">&quot;2&quot;</span>, <span class="number">2</span>), (<span class="number">3L</span>, <span class="string">&quot;3&quot;</span>, <span class="number">3</span>))</span><br><span class="line">    spark.sparkContext.parallelize(dataSet)</span><br><span class="line">        .saveToPhoenix(</span><br><span class="line">            <span class="string">&quot;TEST&quot;</span>, <span class="comment">//表名</span></span><br><span class="line">            Seq(<span class="string">&quot;ID&quot;</span>,<span class="string">&quot;COL1&quot;</span>,<span class="string">&quot;COL2&quot;</span>), <span class="comment">//列命名</span></span><br><span class="line">            zkUrl = Some(<span class="string">&quot;host:2181&quot;</span>) <span class="comment">//Zookeeper URL</span></span><br><span class="line">        )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3 id="保存DataFrame到Phoenix"><a href="#保存DataFrame到Phoenix" class="headerlink" title="保存DataFrame到Phoenix"></a>保存DataFrame到Phoenix</h3><p>方法一：使用saveToPhoenix()方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val df: DataFrame = ...</span><br><span class="line"></span><br><span class="line">df.saveToPhoenix(Map(<span class="string">&quot;table&quot;</span> -&gt; <span class="string">&quot;TEST&quot;</span>, <span class="string">&quot;zkUrl&quot;</span> -&gt; <span class="string">&quot;host:2181&quot;</span>))</span><br></pre></td></tr></table></figure>


<p>方法二：使用format()方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val df: DataFrame = ...</span><br><span class="line"></span><br><span class="line">df.write</span><br><span class="line">    .format(<span class="string">&quot;org.apache.phoenix.spark&quot;</span>)</span><br><span class="line">    .mode(<span class="string">&quot;overwrite&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;table&quot;</span>, <span class="string">&quot;OUTPUT_TABLE&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;zkUrl&quot;</span>, <span class="string">&quot;host:2181&quot;</span>)</span><br><span class="line">    .save()</span><br></pre></td></tr></table></figure>



<p>参考</p>
<ol>
<li><a href="http://phoenix.apache.org/phoenix_spark.html">http://phoenix.apache.org/phoenix_spark.html</a></li>
</ol>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>HBase</tag>
        <tag>Spark</tag>
        <tag>Phoenix</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark connector连接器之整合读写MySQL及问题汇总</title>
    <url>/2020/10/11/Spark-connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99MySQL%E5%8F%8A%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Spark Dataframe转成HashMap</title>
    <url>/2020/10/11/Spark-Dataframe%E8%BD%AC%E6%88%90HashMap/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Spark DataFrame导出为Excel文件</title>
    <url>/2020/10/11/Spark-DataFrame%E5%AF%BC%E5%87%BA%E4%B8%BAExcel%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Spark中一行代码转换DataFrame所有列的类型</title>
    <url>/2020/10/10/Spark%E4%B8%AD%E4%B8%80%E8%A1%8C%E4%BB%A3%E7%A0%81%E8%BD%AC%E5%8C%96DataFrame%E6%89%80%E6%9C%89%E5%88%97%E7%9A%84%E7%B1%BB%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>当需要把DataFrame中所有列的类型转换成另外一种类型，并且这个DataFrame中字段很多，一个一个地转换要写很多冗余代码，那么就可以使用如下这两种转换方式。</p>
<a id="more"></a>

<h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><h2 id="1-foldLeft-函数"><a href="#1-foldLeft-函数" class="headerlink" title="1. foldLeft() 函数"></a>1. foldLeft() 函数</h2><p>代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.DataFrame</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.col</span><br><span class="line"></span><br><span class="line">val df: DataFrame = ...</span><br><span class="line">val columns: Array[String] = df.columns</span><br><span class="line">val df2: DataFrame = columns.foldLeft(df)&#123;(currentDF, column) =&gt; currentDF.withColumn(column, col(column).cast(<span class="string">&quot;string&quot;</span>))&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>变量columns</strong>：是一个String类型的数组，数组中的元素为df中的列名。</li>
<li><strong>foldLeft函数</strong>：此函数以df为初始值，从左向右遍历columns数组，并把df的每一行和columns的每个元素作为参数传入foldLeft后面的函数中(也就是foldLeft后面的大括号中)。</li>
<li>**withColumn()**：将每一列转换成String类型并赋值给当前列。如果存在同名的列，withColumn函数默认会进行覆盖。</li>
</ul>
<h2 id="2-map-函数"><a href="#2-map-函数" class="headerlink" title="2. map() 函数"></a>2. map() 函数</h2><p>代码如下</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;Column, DataFrame&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.col</span><br><span class="line"></span><br><span class="line">val df: DataFrame = ...</span><br><span class="line">val columns: Array[String] = df.columns</span><br><span class="line">val arrayColumn: Array[Column] = columns.map(column =&gt; col(column).cast(<span class="string">&quot;string&quot;</span>))</span><br><span class="line">val df2: DataFrame = df.select(arrayColumn :_*)</span><br></pre></td></tr></table></figure>
<p>通过map函数将columns中的每一列转换成String类型，并返回一个Column类型的数组，然后，将arrayColumn数组中的每个元素作为参数传入select函数中，就相当于df.select(col1, col2, col3, …)。</p>
<p>除此之外，这种写法还有一个很有用的场景：比如要在一个DataFrame中select出很多列(假如有几十个几百个)，如果要一个个显示写出来，既不方便又会让代码显得很冗余，那么就可以使用这种写法。</p>
<ol>
<li>先通过df.columns得到这个DataFrame中的所有列，返回一个包含所有列的数组；</li>
<li>再使用Scala中的这种语法进行查询df.select(arrayColumn :_*)，非常简洁明了。</li>
</ol>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark内存管理(静态内存管理和统一内存管理)</title>
    <url>/2020/10/11/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86-%E9%9D%99%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%92%8C%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Spark性能优化指南(官网文档)</title>
    <url>/2020/10/11/Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97-%E5%AE%98%E7%BD%91%E6%96%87%E6%A1%A3/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>YARN/Hadoop/HDFS常用命令</title>
    <url>/2020/10/11/YARN-Hadoop-HDFS%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Vim编辑器常用操作</title>
    <url>/2020/10/11/Vim%E7%BC%96%E8%BE%91%E5%99%A8%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>pyhive依赖模块sasl安装(pip install sasl)报错Microsoft Visual C++ 14.0 is required</title>
    <url>/2020/10/11/pyhive%E4%BE%9D%E8%B5%96%E6%A8%A1%E5%9D%97sasl%E5%AE%89%E8%A3%85-pip-install-sasl-%E6%8A%A5%E9%94%99Microsoft-Visual-C-14-0-is-required/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>pymysql + DBUtils实现数据库连接池及数据批量读写</title>
    <url>/2020/10/11/pymysql-DBUtils%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E6%B1%A0%E5%8F%8A%E6%95%B0%E6%8D%AE%E6%89%B9%E9%87%8F%E8%AF%BB%E5%86%99/</url>
    <content><![CDATA[<h2 id="1-安装pymysql、DBUtils"><a href="#1-安装pymysql、DBUtils" class="headerlink" title="1. 安装pymysql、DBUtils"></a>1. 安装pymysql、DBUtils</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple pymysql</span><br><span class="line">pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple DBUtils</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h2 id="2-创建连接池"><a href="#2-创建连接池" class="headerlink" title="2. 创建连接池"></a>2. 创建连接池</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> DBUtils.PooledDB <span class="keyword">import</span> PooledDB, PooledDBError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_pool</span>():</span></span><br><span class="line">	pool = PooledDB(</span><br><span class="line">	    creator=pymysql,  <span class="comment"># 数据库连接对象</span></span><br><span class="line">	    host=<span class="string">&quot;host&quot;</span>,</span><br><span class="line">	    user=<span class="string">&quot;user&quot;</span>,</span><br><span class="line">	    password=<span class="string">&quot;password&quot;</span>,</span><br><span class="line">	    database=<span class="string">&quot;database&quot;</span>,</span><br><span class="line">	    charset=<span class="string">&quot;charset&quot;</span>,</span><br><span class="line">	    blocking=<span class="literal">True</span>,  <span class="comment"># 超过最大连接数时该如何处理后来的任务。设置为true就表示阻塞等待直达有空闲连接</span></span><br><span class="line">	    maxconnections=<span class="number">2</span>,  <span class="comment"># 连接池所能允许的最大连接数</span></span><br><span class="line">	    mincached=<span class="number">1</span>,  <span class="comment"># 最小初始化空闲连接数</span></span><br><span class="line">	    autocommit=<span class="literal">True</span>  <span class="comment"># 是否要自动提交</span></span><br><span class="line">	)</span><br><span class="line">	<span class="keyword">return</span> pool.connection()</span><br></pre></td></tr></table></figure>

<h2 id="3-读数据"><a href="#3-读数据" class="headerlink" title="3. 读数据"></a>3. 读数据</h2><h3 id="1-只读一行"><a href="#1-只读一行" class="headerlink" title="1. 只读一行"></a>1. 只读一行</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_one</span>(<span class="params">self, conn, sql</span>):</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line">        cursor.execute(sql)</span><br><span class="line">        <span class="keyword">return</span> cursor.fetchone()[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        conn.close()</span><br><span class="line">        cursor.close()</span><br></pre></td></tr></table></figure>

<h3 id="2-批量读取"><a href="#2-批量读取" class="headerlink" title="2. 批量读取"></a>2. 批量读取</h3><p>批量读取的场景，如果要返回的数据量比较大，为了避免给数据库造成太大负载和占用过多带宽资源，可以选择分批读取数据。比如要读取10万条数据，每批只读2000条。</p>
<p>思路：</p>
<ol>
<li>先通过一条SQL获取要读取的数据行数</li>
<li>每读取一批之后，自增主键减去2000</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sql_count = <span class="string">&quot;select id from db.table where id between 25000 and 125000&quot;</span></span><br><span class="line">sql = <span class="string">&quot;select col1, col2, col3 from db.table&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_many</span>(<span class="params">self, conn, sql_count, sql, size</span>):</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 计算要读取的数据行数</span></span><br><span class="line">        cursor.execute(sql_count)</span><br><span class="line">        cursor.fetchall()</span><br><span class="line">        row_count = cursor.rowcount</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 分配读取数据</span></span><br><span class="line">        cursor.execute(sql)</span><br><span class="line">        <span class="keyword">while</span> row_count &gt; <span class="number">0</span>:</span><br><span class="line">            records = cursor.fetchmany(size)</span><br><span class="line">            row_count -= size</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> records</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        conn.close()</span><br><span class="line">        cursor.close()</span><br></pre></td></tr></table></figure>


<h2 id="4-写数据"><a href="#4-写数据" class="headerlink" title="4. 写数据"></a>4. 写数据</h2><p>如果可以，尽量进行批量写，因为executemany()方法在多行插入和更新的场景下提升写性能。否则，就等于是循环执行execute()方法一条条的写，性能较差。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">execute_many</span>(<span class="params">self, conn, sql, args</span>):</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 从连接池中获取一个连接</span></span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line">        cursor.executemany(sql, args)</span><br><span class="line">    <span class="keyword">except</span> PooledDBError <span class="keyword">as</span> e:</span><br><span class="line">        logger.error(<span class="string">&quot;Failed to insert records into MySQL table &#123;&#125;&quot;</span>.format(e))</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        conn.close()</span><br><span class="line">        cursor.close()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>Python</tag>
        <tag>连接池</tag>
      </tags>
  </entry>
  <entry>
    <title>pyspark中遇到的坑 (持续更新)</title>
    <url>/2020/10/11/pyspark%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>图解正则表达式中的贪婪模式和非贪婪模式</title>
    <url>/2020/10/11/%E5%9B%BE%E8%A7%A3%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%B8%AD%E7%9A%84%E8%B4%AA%E5%A9%AA%E6%A8%A1%E5%BC%8F%E5%92%8C%E9%9D%9E%E8%B4%AA%E5%A9%AA%E6%A8%A1%E5%BC%8F/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>单词统计WordCount (Scala/Python/Java)</title>
    <url>/2020/10/11/%E5%8D%95%E8%AF%8D%E7%BB%9F%E8%AE%A1WordCount-Scala-Python-Java/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>手把手教你维度数据建模</title>
    <url>/2020/10/11/%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E7%BB%B4%E5%BA%A6%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1/</url>
    <content><![CDATA[<p>在这篇文章中，你将会学到如何一步步地进行维度数据建模，你将看到如何在真实的场景中使用维度模型。</p>
<a id="more"></a>

<h1 id="什么是维度数据建模"><a href="#什么是维度数据建模" class="headerlink" title="什么是维度数据建模"></a>什么是维度数据建模</h1><p>维度数据建模是在进行数仓设计时的一种数据建模方法。这种建模方法的主要目标是为了提高数据检索效率，对select查询操作进行了优化。维度数据建模最适合数仓星型模型和雪花模型。</p>
<p>数仓中的维度数据建模不同于ER建模(Entity-Relationship Model，关系-实体模型)，ER建模的主要目标是通过减少数据的冗余来规范化数据， 而维度数据建模使得数据一旦存储在数仓中后，能被更容易地获取。维度模型是许多OLAP系统的底层数据模型。</p>
<p>维度模型是被传奇人物Ralph Kimball提出的，你可以读读他的这本书<a href="https://www.amazon.in/Data-Warehouse-Toolkit-Complete-Dimensional/dp/8126544279/ref=as_li_ss_tl?ie=UTF8&keywords=kimball&qid=1470065827&ref_=sr_1_1&s=books&sr=1-1&linkCode=ll1&tag=dwgeecom-21&linkId=fa8b801a6414c3020dad1a1d29a429f6">The Data Warehouse Toolkit</a></p>
<h1 id="维度数据建模的步骤"><a href="#维度数据建模的步骤" class="headerlink" title="维度数据建模的步骤"></a>维度数据建模的步骤</h1><p>接下来我们通过一个示例来了解维度数据建模的步骤。场景：您希望存储某个MedPlus商店每天销售多少片paracetamol 和 diclofenac 的信息。建模过程中，所有数据都归为两类：维度表和事实表。事实表中包含度量信息，维度表中包含限定度量的信息。</p>
<p>下面是数据仓库维度建模示例的步骤：</p>
<h2 id="第一步：选择业务目标"><a href="#第一步：选择业务目标" class="headerlink" title="第一步：选择业务目标"></a>第一步：选择业务目标</h2><p>在我们的例子中，业务目标就是存储单个商店每天paracetamol 和diclofenac 的销售数据。</p>
<h2 id="第二步：确定粒度"><a href="#第二步：确定粒度" class="headerlink" title="第二步：确定粒度"></a>第二步：确定粒度</h2><p>粒度是表中存储的最低级别的信息。例如，如果表包含每日销售数据，则粒度为“每日”。</p>
<p>在我们的例子中，假设一个特定的MedPlus商店在特定的一天销售1000片paracetamol ，那么粒度是每天，而在特定的月份销售10000片，那么粒度是每月。</p>
<p>设定粒度信息是非常重要的，我们的例子采用的是“每日”的粒度。</p>
<h2 id="第三步：确定维度和维度属性"><a href="#第三步：确定维度和维度属性" class="headerlink" title="第三步：确定维度和维度属性"></a>第三步：确定维度和维度属性</h2><p>在我们的例子中，可以确定三个维度：商店、药品(paracetamol 和diclofenac)和日期。下面是维度表的结构。<br><strong>Medicine</strong><br>| SK | NAME |<br>|–| –|<br>| 1 | Paracetamol |<br>| 2 | Diclofenac |</p>
<p><strong>Shop</strong><br>| SK | NAME |<br>|–| –|<br>| 1 | shop1|<br>| 2 | shop2|<br>| 3 | shop3|</p>
<p><strong>Day</strong><br>| SK | NAME |<br>|–| –|<br>| 1 | 2019-01-01|<br>| 2 | 2019-01-02|<br>| 3 | 2019-01-03|</p>
<h2 id="第四步：确定事实表"><a href="#第四步：确定事实表" class="headerlink" title="第四步：确定事实表"></a>第四步：确定事实表</h2><p>事实表包含的是一些可度量的东西。例子中，药片的销售量就是一个度量，我们可以创建单独的事实表来存储这些度量。</p>
<p>例子中，粒度是每天销售的药片，我将Medicine、Shop和Day这三张表的SK列添加到下面的事实表中。<br><img src="01.png" alt="alt"><br>在这个例子中，我们创建了3个维表和1个事实表，维度表通过外键连接到事实表上，这个模型看起来像个星型，也被称为<strong>星型模型</strong>。</p>
<h1 id="维度建模的优势"><a href="#维度建模的优势" class="headerlink" title="维度建模的优势"></a>维度建模的优势</h1><ol>
<li>提升了数据查询效率：通过冗余存储减少了关联查询</li>
<li>简化了业务报表逻辑</li>
<li>更容易理解：维度模型中的数据不是维度就是事实</li>
<li>可扩展性：维度模型可以更新新的维度。</li>
</ol>
<h1 id="维度数据建模工具"><a href="#维度数据建模工具" class="headerlink" title="维度数据建模工具"></a>维度数据建模工具</h1><p>建议可以尝试被广泛使用的erwin数据模型工具。</p>
]]></content>
      <categories>
        <category>数仓建模</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
        <tag>维度建模</tag>
        <tag>星型模型</tag>
      </tags>
  </entry>
  <entry>
    <title>数据仓库架构、维度数据建模、雪花模型和星型模型</title>
    <url>/2020/10/11/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E3%80%81%E7%BB%B4%E5%BA%A6%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E3%80%81%E9%9B%AA%E8%8A%B1%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%98%9F%E5%9E%8B%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>本地Spark连接远程集群Hive(Scala/Python)</title>
    <url>/2020/10/11/%E6%9C%AC%E5%9C%B0Spark%E8%BF%9E%E6%8E%A5%E8%BF%9C%E7%A8%8B%E9%9B%86%E7%BE%A4Hive-Scala-Python/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
</search>
