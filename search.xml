<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Flink和Spark Streaming背压</title>
      <link href="2020/10/12/Flink%E5%92%8CSpark-Streaming%E8%83%8C%E5%8E%8B/"/>
      <url>2020/10/12/Flink%E5%92%8CSpark-Streaming%E8%83%8C%E5%8E%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是背压"><a href="#什么是背压" class="headerlink" title="什么是背压"></a>什么是背压</h2><p>在流处理系统中，出现下游消费者消费数据的速率跟不上上游生产者生产数据的速率的情况，就叫做<strong>背压</strong>。</p><a id="more"></a><h1 id="Spark-Streaming中的背压"><a href="#Spark-Streaming中的背压" class="headerlink" title="Spark Streaming中的背压"></a>Spark Streaming中的背压</h1><p>在Spark 1.5之前，流应用程序如果因为资源不足导致处理数据的速度跟不上接收数据的速度的情况，可以通过设置每秒所接收数据的最大条数来缓解这种情况。对于使用Receiver的方式可以通过设置’spark.streaming.receiver.maxRate’参数；对于使用Direct的方式设置参数’spark.streaming.kafka.maxRatePerPartition’。</p><p>Spark在1.5版本中引入了背压功能，就不再需要设置上述的速率限制了，Spark Streaming会随着处理条件的变化，自动计算所需要的速率，并进行动态地调整，但前提条件是要通过设置参数’spark.streaming.backpressure.enabled’=true来启用这个功能，因为这个功能在默认情况下是未启用的。</p><h1 id="Flink中的背压"><a href="#Flink中的背压" class="headerlink" title="Flink中的背压"></a>Flink中的背压</h1><p>Flink的web界面上提供了一个标签来监控正在运行的job的背压行为。如果你看一个task的back pressure warning为High的话，那就表明产生数据的速度要比下游算子的消费速度快。拿一个简单的Source -&gt; Sink的例子来说，就是Sink算子对上游的Source算子产生了背压。</p><h2 id="Flink是如何监控背压的"><a href="#Flink是如何监控背压的" class="headerlink" title="Flink是如何监控背压的"></a>Flink是如何监控背压的</h2><p>Flink中的背压监控是通过反复获取运行任务的堆栈跟踪样本来实现的。JobManager会为job中的tasks重复调用Thread.getStackTrace()。</p><p>下面是官方提供的示意图：<br><img src="01.png" alt="alt"><br>如果堆栈跟踪样本显示某个task线程被卡在了某个内部方法的调用中(从网络堆栈请求缓冲)，则表明该task存在背压。</p><p>默认情况下，为了确认是否存在背压，Job Manager会每50ms为每个task触发100次堆栈跟踪。我们可以从Web界面上看到有多少堆栈跟踪被卡在了内部方法的调用中，例如，0.01表示100跟踪里面有一个被卡在该方法中。</p><p>下面是不同的比率对应的背压情况：</p><ul><li><p><strong>OK</strong>: 0 &lt;= Ratio &lt;= 0.10</p></li><li><p><strong>LOW</strong>: 0.10 &lt; Ratio &lt;= 0.5</p></li><li><p><strong>HIGH</strong>: 0.5 &lt; Ratio &lt;= 1</p><p>为了不让task managers因为频繁触发堆栈跟踪而负载过重，Web监控接口会每60秒刷新一次跟踪样本。</p></li></ul><h2 id="背压状态"><a href="#背压状态" class="headerlink" title="背压状态"></a>背压状态</h2><p>我们可以在Job overview旁边找到 Back Pressure标签。</p><p>如果tasks的背压状态是<strong>OK</strong>，则表明没有背压情况。如下图：<br><img src="02.png" alt="alt"></p><p>如果状态是<strong>HIGH</strong>，则表明tasks出现了背压。如下图：<br><img src="03.png" alt="alt"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html">https://spark.apache.org/docs/latest/streaming-programming-guide.html</a></li><li><a href="https://ci.apache.org/projects/flink/flink-docs-stable/monitoring/back_pressure.html">https://ci.apache.org/projects/flink/flink-docs-stable/monitoring/back_pressure.html</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> Spark Streaming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pyspark中遇到的坑</title>
      <link href="2020/10/12/pyspark%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/"/>
      <url>2020/10/12/pyspark%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/</url>
      
        <content type="html"><![CDATA[<h1 id="模块依赖问题"><a href="#模块依赖问题" class="headerlink" title="模块依赖问题"></a>模块依赖问题</h1><p>因为平时写Spark应用程序基本都用Scala，之前有写过一段pyspark，但是都是在一个类或模块中实现所有的功能，也就自然没有遇到过要在一个模块中导入(import)自己写的另一个模块。这次遇到了，也发现了关于import模块时要注意的问题。</p><a id="more"></a><h2 id="1-PyCharm执行"><a href="#1-PyCharm执行" class="headerlink" title="1. PyCharm执行"></a>1. PyCharm执行</h2><p>要注意的是，当在一个模块(假如是module1)中导入相同目录(假设是demo)下的另一个模块(module2)中的变量list时，想当然的以为应该是这样导入 ——&gt; from module2 import list，但其实是这样 ——&gt; from demo.module2 import list，就是你要加上要导入模块的上级目录才行，如果想要实现这样导入 —— from module2 import list，需要进行一些设置，如下：<br><img src="01.png" alt="alt"></p><p>还有另外一种可以不进行设置的方式 ——&gt; from .module2 import list，就是在要导入的模块的前面加一个.点表示导入的是同级目录下的模块。</p><h2 id="2-集群提交"><a href="#2-集群提交" class="headerlink" title="2. 集群提交"></a>2. 集群提交</h2><p>提交命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--driver-memory 4G \</span><br><span class="line">--executor-memory 10G \</span><br><span class="line">--executor-cores 5 \</span><br><span class="line">--num-executors 6 \</span><br><span class="line">--conf &quot;spark.yarn.maxAppAttempts=1&quot; \</span><br><span class="line">--py-files /home/module2.py \</span><br><span class="line">/home/module1.py</span><br></pre></td></tr></table></figure><p>因为spark任务在运行的时候一般都是分布式的，会先把程序代码module1.py发送到各个executor，而 module1.py 依赖模块 module2.py，那么也就需要把 module2.py 分发到各个executor节点，这就需要用到参数选项–py-files。</p><p>在提交到远程集群执行的时候，并不需要像在本地那样还需要注意被导入模块的路径问题，只需要from module2 import list即可。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>pyhive依赖模块sasl安装(pip install sasl)报错Microsoft Visual</title>
      <link href="2020/10/12/pyhive%E4%BE%9D%E8%B5%96%E6%A8%A1%E5%9D%97sasl%E5%AE%89%E8%A3%85-pip-install-sasl-%E6%8A%A5%E9%94%99Microsoft-Visual/"/>
      <url>2020/10/12/pyhive%E4%BE%9D%E8%B5%96%E6%A8%A1%E5%9D%97sasl%E5%AE%89%E8%A3%85-pip-install-sasl-%E6%8A%A5%E9%94%99Microsoft-Visual/</url>
      
        <content type="html"><![CDATA[<h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>这个问题是在Windows7上安装pyhive时遇到的。因为安装 pyhive 要依赖 sasl 模块，在pip install sasl 时报错 “缺少Microsoft Visual C++编译器” 。</p><a id="more"></a><p><strong><font color=red, size=4>以下是在执行 pip install sasl 时出现的一系列问题以及解决方法</font></strong></p><h2 id="1-问题一：error-Microsoft-Visual-C-14-0-is-required"><a href="#1-问题一：error-Microsoft-Visual-C-14-0-is-required" class="headerlink" title="1. 问题一：error: Microsoft Visual C++ 14.0 is required"></a>1. 问题一：error: Microsoft Visual C++ 14.0 is required</h2><p>报错信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">error: Microsoft Visual C++ <span class="number">14.0</span> <span class="keyword">is</span> required. Get it <span class="keyword">with</span> <span class="string">&quot;Microsoft Visual C++ Build Tools&quot;</span>: </span><br><span class="line">https://visualstudio.microsoft.com/downloads.</span><br></pre></td></tr></table></figure><p>原因：<br>虽然Python是一种解释性语言，但是在某些情况下可能需要安装Windows C++编译器。与Linux不同的是，Windows编译器在默认情况下是不包含在操作系统中的。这就需要我们自己安装了。微软提供了官方的C++编译器Visual C++，它是和Visual Studio捆绑在一起的。那么我们最终选择一个包含Visual C++ 14.0的Visual Studio来安装，这里选择的是Visual Studio 2017的版本。</p><p><font color=red>注意：在安装Visual Studio 2017之前，建议先更新 pip install –upgrade setuptools。官方说是它包含兼容性的改进和添加编译器的自动使用。</font></p><h3 id="解决方案：安装Microsoft-Visual-Studio"><a href="#解决方案：安装Microsoft-Visual-Studio" class="headerlink" title="解决方案：安装Microsoft Visual Studio"></a>解决方案：安装Microsoft Visual Studio</h3><h4 id="1-下载"><a href="#1-下载" class="headerlink" title="1. 下载"></a>1. 下载</h4><p>官方下载地址：<a href="https://docs.microsoft.com/zh-cn/visualstudio/releasenotes/vs2017-relnotes#15.9.14">https://docs.microsoft.com/zh-cn/visualstudio/releasenotes/vs2017-relnotes#15.9.14</a> 。 打开网址后，下载免费的社区版本即可。</p><p><img src="01.png" alt="alt"></p><h4 id="2-安装"><a href="#2-安装" class="headerlink" title="2. 安装"></a>2. 安装</h4><p>安装时，注意要勾选的选项，”工作负载” ——&gt; Windows一栏勾选 “使用 C++ 的桌面开发” 、Web和云一栏勾选Python开发，如下：</p><p><img src="02.png" alt="alt"></p><p>要下载的文件比较大，有7G左右，耗时较长。其他步骤默认即可。</p><h2 id="2-问题二：ImportError-DLL-load-failed"><a href="#2-问题二：ImportError-DLL-load-failed" class="headerlink" title="2. 问题二：ImportError: DLL load failed"></a>2. 问题二：ImportError: DLL load failed</h2><p>问题一处理之后，再次安装sasl时，你可能会继续遇到第二个问题：ImportError: DLL load failed: 找不到指定的程序。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>用Python安装根目录下的 python3.dll 文件替换PyCharm工程\venv\Scripts目录下的同名文件。具体原因不太明确，可能是要加载的DLL文件不不匹配？</p><h2 id="3-问题三：“sasl-sasl-h”-No-such-file-or-directory"><a href="#3-问题三：“sasl-sasl-h”-No-such-file-or-directory" class="headerlink" title="3. 问题三：“sasl/sasl.h”: No such file or directory"></a>3. 问题三：“sasl/sasl.h”: No such file or directory</h2><p> 第二个问题解决之后，继续执行pip install sasl，可能又会遇到第三个问题：<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">..\pip-install-wrsldjm1\sasl\sasl\saslwrapper.h(<span class="number">22</span>): fatal error C1083: 无法打开包括文件: “sasl/sasl.h”: No such file <span class="keyword">or</span> directory</span><br></pre></td></tr></table></figure></p><h3 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h3><p>点 <a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#sasl">这里</a>下载一个对应你所使用的Python版本和Windows版本的sasl文件。比如本人下载的是 sasl‑0.2.1‑cp36‑cp36m‑win_amd64.whl，则对应Python 版本为3.6，Windows系统为64位。</p><p>下载好之后，执行pip install sasl-0.2.1-cp37-cp37m-win_amd64.whl</p><p>最后，解决完这几个问题，应该就可以成功的安装sasl了。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://wiki.python.org/moin/WindowsCompilers#Microsoft_Visual_C.2B-.2B-_14.0_with_Visual_Studio_2017_.28x86.2C_x64.2C_ARM.2C_ARM64.29">WindowsCompilers</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库架构、维度数据建模、雪花模型和星型模型</title>
      <link href="2020/10/11/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E3%80%81%E7%BB%B4%E5%BA%A6%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E3%80%81%E9%9B%AA%E8%8A%B1%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%98%9F%E5%9E%8B%E6%A8%A1%E5%9E%8B/"/>
      <url>2020/10/11/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E3%80%81%E7%BB%B4%E5%BA%A6%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E3%80%81%E9%9B%AA%E8%8A%B1%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%98%9F%E5%9E%8B%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>在GeeksforGeeks上看到了几篇关于数据仓库架构、维度数据建模的文章，进行翻译整理并加入了一些自己的理解，输出了这篇文章。</p><a id="more"></a><h1 id="一、数据仓库架构"><a href="#一、数据仓库架构" class="headerlink" title="一、数据仓库架构"></a>一、数据仓库架构</h1><p>数据仓库是将不同来源的数据在统一的模式下组织起来的异构集合。构建数据仓库有两种方法：自顶向下和自底向上。</p><h2 id="1-自顶向下"><a href="#1-自顶向下" class="headerlink" title="1. 自顶向下"></a>1. 自顶向下</h2><p>自顶向下的架构如下图：<br><img src="01.png" alt="alt"><br>图中各主件的作用如下：</p><ol><li><p>External Sources<br>外部源是不管收集的数据是何种类型的数据源。数据可以是结构化的、半结构化的和非结构化的。结构化的数据，比如：关系型数据库中的表等；半结构化的数据，比如：Json串、XML文件等；非结构化的数据，比如：图片、音频、视频等。</p></li><li><p>Stage Area<br>由于从外部数据源抽取的数据没有特定的格式，因此需要对这些数据进行处理，以将其加载到数据仓库中。为此，建议使用ETL工具：</p><p> <strong>E(Extracted)</strong>: 从外部数据源抽取数据。<br> <strong>T(Transform)</strong>: 将抽取的数据转换成标准格式。<br> <strong>L(Load):</strong> 将标准格式的数据加载进数据仓库。</p><ol start="3"><li>Data-warehouse<br>在自顶向下的方法中，数据仓库存储最原始的数据。</li><li>Data Marts<br>数据集市也是存储组件中的一部分。它存储由不同主题组织起来的数据。</li><li>Data Mining<br>数据挖掘就是对数据仓库中的大量数据进行分析处理。利用数据挖掘算法找出数据仓库中隐藏的数据价值。</li></ol></li></ol><p>自顶向下的方法是被数据仓库之父Bill Inmon定义的——数据仓库作为公司的中央仓库，而数据集市从完整的数据仓库创建而来。</p><h3 id="自顶向下的优点"><a href="#自顶向下的优点" class="headerlink" title="自顶向下的优点"></a>自顶向下的优点</h3><ol><li>由于数据集市是从数据仓库创建而来，因此提供了数据集市的一致维度视图。</li><li>这种模型可以很好的应对业务数据的变化。大公司会更喜欢采用这种方法。</li><li>从数据仓库创建数据集市比较容易。</li></ol><h3 id="自顶向下的缺点"><a href="#自顶向下的缺点" class="headerlink" title="自顶向下的缺点"></a>自顶向下的缺点</h3><ol><li>设计和维护的成本比较高。</li></ol><h2 id="2-自底向上"><a href="#2-自底向上" class="headerlink" title="2. 自底向上"></a>2. 自底向上</h2><p>自底向上架构如下图：<br><img src="02.png" alt="alt"><br>整个数据的流转过程：</p><ol><li>首先，数据会从外部数据源被抽取过来。</li><li>数据经过ETL之后，不再进入数据仓库，而是被加载进数据集市。每个数据集市针对的是不同的业务主题，并且可以直接提供报表功能。</li><li>将数据集市整合进数据仓库。</li></ol><p>自下而上的方法是Ralph Kimball(数据仓库和BI领域的权威专家)提出的——首先创建数据集市，并为分析提供单个业务视图，在创建完整的数据集市之后再创建数据仓库。</p><h3 id="自底向上的优点"><a href="#自底向上的优点" class="headerlink" title="自底向上的优点"></a>自底向上的优点</h3><ol><li>由于是先创建数据集市，所以会快速生成报表。</li><li>数据仓库可以根据数据集市进行扩展。</li><li>设计成本比较低。</li></ol><h3 id="自底向上的缺点"><a href="#自底向上的缺点" class="headerlink" title="自底向上的缺点"></a>自底向上的缺点</h3><ol><li>由于维度视图的局限性，造成这种方法没有对数据的一个全局把控。</li></ol><h1 id="二、维度数据建模"><a href="#二、维度数据建模" class="headerlink" title="二、维度数据建模"></a>二、维度数据建模</h1><p>维度建模的概念是由Ralph Kimball提出的，是许多OLAP(Online Analytical Processing，联机分析处理)系统使用的数据模型。维度建模有两个比较常用的建模模型：<strong>星型模型</strong>和<strong>雪花模型</strong>，模型由<strong>事实表</strong>和<strong>维度表</strong>组成。</p><p>维度建模步骤如下图：</p><p><img src="03.png" alt="alt"></p><ol><li>Identifying the business objective<br>第一步就是确定业务对象，比如，销售(Sales)、人力资源(HR)、促销(Marketing)等。业务对象的选择直接影响着之后所用数据的质量，是数据建模中非常重要的一步。</li><li>Identifying Granularity<br>确定要存储到表中的数据的粒度。<ol start="3"><li>Indentify Dimensions and ites Attributes<br>维度用来对数据仓库中的事实数据进行分类。比如，数据维度可能会是时间中的年、月、日，也可能是地域中的省、市、区县等等。</li></ol></li><li>Indentifying the Fact<br>确认事实表，比如，商品价格、尺寸等。</li><li>Building of Schema<br>构建模型，比较常用的模型有：星型模型和雪花模型。</li></ol><h1 id="三、星型模型和雪花模型"><a href="#三、星型模型和雪花模型" class="headerlink" title="三、星型模型和雪花模型"></a>三、星型模型和雪花模型</h1><h2 id="1-星型模型"><a href="#1-星型模型" class="headerlink" title="1. 星型模型"></a>1. 星型模型</h2><p>星型模型是数仓建模中较为常用的模型，它包含一个或多个事实表，以及连接到事实表上的维度表。星型模型相较于雪花模型在查询处理方面是更为高效的。</p><p>星型模型之所以被称为星型，是因为它的物理模型就像是一个恒星的形状，中心是一个事实表，事实表上连着维度表。如下图：<br><img src="04.png" alt="alt"><br>在上图中，SALES就是一个事实表，其他的都是维度表，并各自都有属于自己的属性。事实表中存储业务流程中的定量数据；维度表中存储事实数据的描述性特征。事实数据，比如图中的：销售价格、数量、重量等。</p><h3 id="星型模型的优点："><a href="#星型模型的优点：" class="headerlink" title="星型模型的优点："></a>星型模型的优点：</h3><ol><li>关联查询比较简单，没有过于复杂的关联关系。</li><li>由于一些维度表已经预先进行了合并，因此不需要过多的join操作，那么关联查询效率就会更高。</li></ol><h3 id="星型模型的缺点："><a href="#星型模型的缺点：" class="headerlink" title="星型模型的缺点："></a>星型模型的缺点：</h3><ol><li>数据并不像3NF那样规范化。</li><li>由于一些维度表已经预先进行了合并，就会造成数据的冗余存储，占用了更多的空间。</li></ol><h2 id="2-雪花模型"><a href="#2-雪花模型" class="headerlink" title="2. 雪花模型"></a>2. 雪花模型</h2><p>雪花模型可以认为是星型模型的变体。雪花模型在星型模型的基础上，对维度表进行了更规范化的拆分，就会促使某些维度表拆分出更细分的维度表。看下图：<br><img src="05.png" alt="alt"><br>图中就把Employee表拆分成了Employee和Department两张维度表，Department维度表可以提供一个部门更详细的信息，比如名字和位置。还有也把Customer维度表拆分成了Customer和City两张维度表，City维度表有关于一个城市的详细信息，比如城市名、邮政编码、所属省和国家。</p><p>雪花模型和星型模型的主要区别在于，雪花模型的维度表是规范化存储的，减少了冗余。这样做的好处是易于维护和节省存储空间，缺点就是需要更多的连接来执行查询，性能较差。</p><p>通常情况下，不建议使用雪花模型，因为它会增加维度模型的复杂度，可理解性差，而且需要连接更多的表来满足查询，性能低。</p><p>最后总结一下雪花模型有哪些优缺点。</p><h3 id="雪花模型的优点"><a href="#雪花模型的优点" class="headerlink" title="雪花模型的优点"></a>雪花模型的优点</h3><ol><li>提供了规范化的数据，数据完整性高。</li><li>由于数据时高度规范化的，因此占用的存储空间较小。</li></ol><h3 id="雪花模型的缺点"><a href="#雪花模型的缺点" class="headerlink" title="雪花模型的缺点"></a>雪花模型的缺点</h3><ol><li>高度结构化的数据，在另一方面也增加了模型的复杂度。</li><li>规范化的数据，在查询的时候会有更多的join连接，就会导致较差的性能。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 数据仓库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据仓库 </tag>
            
            <tag> 维度模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Dataframe转成HashMap</title>
      <link href="2020/10/11/Spark-Dataframe%E8%BD%AC%E6%88%90HashMap/"/>
      <url>2020/10/11/Spark-Dataframe%E8%BD%AC%E6%88%90HashMap/</url>
      
        <content type="html"><![CDATA[<h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>现在有一个json串”{‘address’:[[‘33’,’3301’,’330108’],’xx路xx号’]}”, 需要把address解析出来之后, 将行政区划编码替换为对应的省市区县, 最后输出这样的格式(“浙江省杭州市滨江区xx路xx号”). </p><a id="more"></a><p>开发给到的行政区划表(area)的结构如下图:<br><img src="https://img-blog.csdnimg.cn/20200104143122881.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xvdmV0ZWNobG92ZWxpZmU=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>因为行政区划表在解析json的时候, 很多地方都会用到, 并且行政区划表本身记录数比较少, 占用的内存空间较小, 就考虑将行政区划编码和对应的省市区县放到一个HashMap中, 查询效率也会比较高. </p><p>有的同学可能会问, 为什么不用join? 如果用join关联, 在查省、市、区县时，要么是先过滤出三个Dataframe，分别代表省、市、区县，然后再分别join，要么就是不区分，关联整个行政区划表3次。这样一来，不仅比较麻烦，效率也不高。</p><h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><p>代码实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"></span><br><span class="line">val hashMap = spark.read.table(<span class="string">&quot;ods.area&quot;</span>) <span class="comment">//行政区划表</span></span><br><span class="line">.select(<span class="string">&quot;area_code&quot;</span>, <span class="string">&quot;area_name&quot;</span>) <span class="comment">//只查询需要的字段(行政区划编码、省市区县名称)</span></span><br><span class="line">.rdd <span class="comment">//Dataframe转化为RDD</span></span><br><span class="line"><span class="comment">//RDD中每一行转化成area_code和area_name映射的key-value</span></span><br><span class="line">.map(row =&gt; row.getAs(<span class="string">&quot;area_code&quot;</span>).toString -&gt; row.getAs(<span class="string">&quot;area_name&quot;</span>).toString) </span><br><span class="line">.collectAsMap() <span class="comment">//将key-value对类型的RDD转化成Map</span></span><br><span class="line">.asInstanceOf[mutable.HashMap[String, String]]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataFrame </tag>
            
            <tag> HashMap </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图解正则表达式中的贪婪模式和非贪婪模式</title>
      <link href="2020/10/11/%E5%9B%BE%E8%A7%A3%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%B8%AD%E7%9A%84%E8%B4%AA%E5%A9%AA%E6%A8%A1%E5%BC%8F%E5%92%8C%E9%9D%9E%E8%B4%AA%E5%A9%AA%E6%A8%A1%E5%BC%8F/"/>
      <url>2020/10/11/%E5%9B%BE%E8%A7%A3%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%B8%AD%E7%9A%84%E8%B4%AA%E5%A9%AA%E6%A8%A1%E5%BC%8F%E5%92%8C%E9%9D%9E%E8%B4%AA%E5%A9%AA%E6%A8%A1%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="正则表达式符号"><a href="#正则表达式符号" class="headerlink" title="正则表达式符号"></a>正则表达式符号</h1><p>正则表达式中部分符号含义：</p><ul><li>点号(.) —&gt; 匹配除换行符之外的任意单个字符</li><li>星号(*) —&gt; 匹配0个或多个之前的字符（比如：a*可以匹配0个或多个a）</li><li>问号(?) —&gt; 匹配0个或1个之前的字符（比如：a?可以匹配0个或1个a）</li><li>点号和星号组合(.*) —&gt; 匹配除换行符之外的所有字符（贪婪模式）</li><li>点号、星号和问号组合(.*?) —&gt; 匹配符合条件的最少字符（也即非贪婪模式）</li></ul><a id="more"></a><p>这里分享一个网站<a href="https://regex101.com/">regex101</a>，可以方便的查看和测试正则表达式。</p><h1 id="贪婪模式"><a href="#贪婪模式" class="headerlink" title="贪婪模式"></a>贪婪模式</h1><p><img src="01.png" alt="alt"><br>上图中，正则表达式为(.*)(\d+).*，测试文本为”Flink_123.java”。</p><p>可以在右下角看到匹配结果：</p><ul><li>分组1中，(.*)匹配了尽可能多的字符。</li><li>分组2中，虽然(\d+)可以匹配多个数字，但是因为分组1是贪婪模式，所以(\d+)只匹配了最后一个数字。</li></ul><h1 id="非贪婪模式"><a href="#非贪婪模式" class="headerlink" title="非贪婪模式"></a>非贪婪模式</h1><p><img src="02.png" alt="alt"><br>上图中，正则表达式为(.*?)(\d+).*，测试文本为”Flink_123.java”。</p><p>可以在右下角看到匹配结果：</p><ul><li>分组1中，(.*?)匹配了尽可能少的符合条件的字符。</li><li>分组2中，(\d+)就匹配了所有数字</li></ul>]]></content>
      
      
      <categories>
          
          <category> 正则表达式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 正则表达式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive/Spark SQL常用函数-窗口分析函数、行列转换、JSON处理</title>
      <link href="2020/10/11/Hive-Spark-SQL%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3%E5%88%86%E6%9E%90%E5%87%BD%E6%95%B0%E3%80%81%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2%E3%80%81JSON%E5%A4%84%E7%90%86/"/>
      <url>2020/10/11/Hive-Spark-SQL%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3%E5%88%86%E6%9E%90%E5%87%BD%E6%95%B0%E3%80%81%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2%E3%80%81JSON%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>Hive SQL和Spark SQL常用函数。</p><a id="more"></a><h1 id="窗口分析函数"><a href="#窗口分析函数" class="headerlink" title="窗口分析函数"></a>窗口分析函数</h1><p>函数中用到的表数据如下图：<br><img src="01.png" alt="alt"></p><h3 id="1-分析函数"><a href="#1-分析函数" class="headerlink" title="1. 分析函数"></a>1. 分析函数</h3><ul><li>row_number()</li><li>rank()</li><li>dense_rank()</li></ul><p>这3个函数通常用在组内排序中，但实现的效果却不相同，用法如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">    <span class="keyword">name</span>,subject,score,</span><br><span class="line">    row_number() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> score) rn, </span><br><span class="line">    <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> score) rk,</span><br><span class="line">    <span class="keyword">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> score) dr</span><br><span class="line"><span class="keyword">from</span> grade;</span><br></pre></td></tr></table></figure><p>排序之后编号对比， 如下图：<br><img src="02.png" alt="alt"></p><h3 id="2-聚合函数"><a href="#2-聚合函数" class="headerlink" title="2. 聚合函数"></a>2. 聚合函数</h3><ul><li>count()组内计数</li><li>sum()组内求和</li><li>avg()组内求平均值</li><li>max()&amp;min()组内求最大最小值</li></ul><p>下面SQL以sum函数为例展示聚合函数的用法，其他函数的用法类似。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line"><span class="keyword">name</span>,subject,score,</span><br><span class="line"><span class="keyword">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> subject) <span class="keyword">as</span> sum1, <span class="comment">-- 分组内起始行到当前行的和</span></span><br><span class="line"><span class="keyword">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> subject <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> sum2, <span class="comment">--分组内当前行与前一行的和</span></span><br><span class="line"><span class="keyword">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> subject <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="number">2</span> <span class="keyword">following</span>) <span class="keyword">as</span> sum3, <span class="comment">--分组内当前行与后两行的和</span></span><br><span class="line"><span class="keyword">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> subject <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">unbounded</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> sum4, <span class="comment">--分组内起始行到当前行的和</span></span><br><span class="line"><span class="keyword">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> subject <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">unbounded</span> <span class="keyword">following</span>) <span class="keyword">as</span> sum5 <span class="comment">---分组内当前行到终止行的和</span></span><br><span class="line"><span class="keyword">from</span> grade;</span><br></pre></td></tr></table></figure><p>计算结果如下图：<br><img src="03.png" alt="alt"></p><h3 id="3-窗口函数"><a href="#3-窗口函数" class="headerlink" title="3. 窗口函数"></a>3. 窗口函数</h3><ul><li>lag(col, n, default)  表示分组内列(col)的当前行之前的第n行, default为默认值</li><li>lead(col, n, default)  表示分组内列(col)的当前行之后的第n行, default为默认值</li></ul><p>用法如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">    <span class="keyword">name</span>,subject,score,</span><br><span class="line">    lag(score, <span class="number">1</span>, <span class="number">0</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> score) lag, <span class="comment">-- lag(score, 1, 0)表示分组内列(score)的当前行之前的第1行, 0为默认值</span></span><br><span class="line">    <span class="keyword">lead</span>(score, <span class="number">1</span>, <span class="number">0</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> score) <span class="keyword">lead</span> <span class="comment">-- lead(score, 1, 0)表示分组内列(score)的当前行之后的第1行, 0为默认值</span></span><br><span class="line"><span class="keyword">from</span> grade;</span><br></pre></td></tr></table></figure><p>结果如下图：<br><img src="04.png" alt="alt"></p><ul><li>first_value(col) 组内排序第一个值</li><li>last_value(col) 组内排序最后一个值</li></ul><p>用法如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">    <span class="keyword">name</span>,subject,score,</span><br><span class="line">    <span class="keyword">first_value</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> score) f</span><br><span class="line"><span class="keyword">from</span> grade;</span><br></pre></td></tr></table></figure><p><img src="05.png" alt="alt"></p><h1 id="行列转换"><a href="#行列转换" class="headerlink" title="行列转换"></a>行列转换</h1><h3 id="1-行转列"><a href="#1-行转列" class="headerlink" title="1. 行转列"></a>1. 行转列</h3><p>原始数据如下图：<br><img src="06.png" alt="alt"><br>转换SQL：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>, collect_list(score) <span class="keyword">from</span> grade <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span>;</span><br></pre></td></tr></table></figure><p>结果：<br><img src="07.png" alt="alt"></p><h3 id="2-列转行"><a href="#2-列转行" class="headerlink" title="2. 列转行"></a>2. 列转行</h3><p>原始数据：<br><img src="08.png" alt="alt"><br>转换SQL：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>, score <span class="keyword">from</span> grade <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span> (scores) tmp <span class="keyword">as</span> score</span><br></pre></td></tr></table></figure><p>结果：<br><img src="09.png" alt="alt"></p><h1 id="JSON处理"><a href="#JSON处理" class="headerlink" title="JSON处理"></a>JSON处理</h1><h2 id="1-JSON对象"><a href="#1-JSON对象" class="headerlink" title="1. JSON对象"></a>1. JSON对象</h2><p>JSON对象的处理可以用get_json_object()函数或json_tuple()函数。</p><p>字段field的值是一个JSONObject：{“status”:0,”version”:”v1.0”}</p><ul><li>get_json_object()  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">    get_json_object(<span class="keyword">field</span>, <span class="string">&quot;$.status&quot;</span>),</span><br><span class="line">    get_json_object(<span class="keyword">field</span>, <span class="string">&quot;$.version&quot;</span>)</span><br><span class="line"><span class="keyword">from</span> db.table;</span><br></pre></td></tr></table></figure></li><li>json_tuple()<br>如果需要获取多个key的值，建议用json_tuple函数，性能优于get_json_object()。  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">    json.status, json.version</span><br><span class="line"><span class="keyword">from</span> qjdods.cif_credit_report t</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> json_tuple(report_value, <span class="string">&#x27;status&#x27;</span>, <span class="string">&#x27;version&#x27;</span>) <span class="keyword">json</span> <span class="keyword">as</span> <span class="keyword">status</span>, <span class="keyword">version</span> <span class="keyword">limit</span> <span class="number">1</span>;</span><br></pre></td></tr></table></figure><h2 id="2-JSON数组"><a href="#2-JSON数组" class="headerlink" title="2. JSON数组"></a>2. JSON数组</h2><h3 id="1-Hive-SQL实现"><a href="#1-Hive-SQL实现" class="headerlink" title="1. Hive SQL实现"></a>1. Hive SQL实现</h3>Hive中的处理思路：</li></ul><ol><li>把JSON对象之间的逗号(，)替换成特殊字符，比如^*，因为之后要以这个特殊字符串来切分</li><li>替换掉中括号([])，为空</li><li>以步骤1中的特殊字符串切分处理后的JSON数组</li><li>结合 lateral view explode()函数，使得JSON数组转成多行JSON对象</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">    <span class="keyword">id</span>, <span class="keyword">json</span></span><br><span class="line"><span class="keyword">from</span> db.table</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(</span><br><span class="line">    <span class="keyword">split</span>(</span><br><span class="line">        regexp_replace(regexp_replace(json_array, <span class="string">&quot;&#125;,&quot;</span>, <span class="string">&quot;&#125;^*^*&quot;</span>), <span class="string">&quot;\\[|\\]&quot;</span>, <span class="string">&quot;&quot;</span>), <span class="string">&quot;\\^\\*\\^\\*&quot;</span></span><br><span class="line">    )</span><br><span class="line">) t <span class="keyword">as</span> <span class="keyword">json</span></span><br></pre></td></tr></table></figure><h3 id="3-Spark-SQL实现"><a href="#3-Spark-SQL实现" class="headerlink" title="3. Spark SQL实现"></a>3. Spark SQL实现</h3><p>使用Hive SQL处理JSON数组有一个弊端，如果JSON数组里面有嵌套数组的时候，单纯的替换掉中括号得出的结果就是错误的。而Spark SQL提供了一个内建函数substring_index(str: Column, delim: String, count: Int)，这个函数可以从指定的索引位置，并指定指定分隔符来切分字符串，这样就可以实现只替换JSON数组中的首尾中括号。当然，在Hive SQL也可以自己写一个UDF来实现这个功能。</p><p>实现代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Json数组行转列</span></span><br><span class="line"><span class="function">def <span class="title">explodeFunc</span><span class="params">(spark: SparkSession, df: Dataset[Row])</span>: Dataset[Row] </span>= &#123;</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    df.select($<span class="string">&quot;user_id&quot;</span>,</span><br><span class="line">        explode(</span><br><span class="line">            split(</span><br><span class="line">                substring_index(</span><br><span class="line">                    substring_index(</span><br><span class="line">                        regexp_replace($<span class="string">&quot;json_array&quot;</span>, <span class="string">&quot;&#125;,&quot;</span>, <span class="string">&quot;&#125;^*^*&quot;</span>),</span><br><span class="line">                        <span class="string">&quot;[&quot;</span>, -<span class="number">1</span>),</span><br><span class="line">                    <span class="string">&quot;]&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="string">&quot;\\^\\*\\^\\*&quot;</span></span><br><span class="line">            )</span><br><span class="line">        ).as(<span class="string">&quot;json&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="时间处理"><a href="#时间处理" class="headerlink" title="时间处理"></a>时间处理</h1><ul><li>获取当前时间，并格式化(yyyy-MM-dd HH:mm:ss)  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from_unixtime(unix_timestamp(), &#x27;yyyy-MM-dd HH:mm:ss&#x27;)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
            <tag> Spark SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CDH集群安装</title>
      <link href="2020/10/11/CDH%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"/>
      <url>2020/10/11/CDH%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<p>在云计算时代，可能很多公司为了成本的考虑，会采用第三方厂商提供的大数据集群，比如阿里的Maxcompute、华为的FusionInsight等。但选择安装开源的软件，能让你更加清楚其内部的原理，也能更好的针对自己公司的业务需求来定制软件进行二次开发。</p><a id="more"></a><p>下面针对自己在公司安装CDH集群的经历，整理了以下安装步骤，希望能对有需要的同学有所帮助！</p><p><font color=red>注：以下步骤中，从第1步到第8步，除了元数据库的安装之外，其他步骤都是要在集群所有节点上执行的！</font></p><h1 id="1-配置主机名"><a href="#1-配置主机名" class="headerlink" title="1. 配置主机名"></a>1. 配置主机名</h1><p>在文件/etc/hosts的最后加入以下配置(集群所有节点都需要配置)<br>10.1.12.61bigdatadev01<br>10.1.12.62bigdatadev02<br>10.1.12.63bigdatadev03<br>10.1.12.64bigdatadev04<br>10.1.12.65bigdatadev05</p><p>依次修改每个节点文件/etc/hostname中的主机名为bigdata01，bigdata02…修改完之后需要重启主机reboot，才能生效。</p><h1 id="2-时间同步"><a href="#2-时间同步" class="headerlink" title="2. 时间同步"></a>2. 时间同步</h1><p>使用chrony与外网进行时间同步，不需要配置从节点向主节点同步。</p><ol><li>安装chrony服务：yum -y install chrony</li><li>启动服务：systemctl start chronyd</li><li>查看状态：systemctl status chronyd</li><li>设置开机启动：systemctl enable chronyd</li></ol><h1 id="3-ssh免密登录"><a href="#3-ssh免密登录" class="headerlink" title="3. ssh免密登录"></a>3. ssh免密登录</h1><ol><li><p>在集群各节点上产生公钥和私钥<br>ssh-keygen -t rsa<br>注：ssh-keygen为ssh生成、管理和转换认证密钥, 它支持RSA和DSA两种认证密钥。-t选项: 指定要创建的密钥类型。</p></li><li><p>拷贝被访问节点的公钥到访问节点<br>拷贝主节点的公钥到所有节点，需要主节点能访问所有节点包括主节点本身。<br>ssh-copy-id master</p><p>命令格式：ssh-copy-id [ -i [identity_file] ] [user@]machine<br>ssh-copy-id是把本地主机的公钥复制到远程主机的authorized_keys文件上, 也会给远程主机的用户主目录(home)和~/.ssh和~/.ssh/authorized_keys设置合适的权限。-i选项用来把本地的ssh公钥文件安装到远程主机对应账户下。<br>例如：ssh-copy-id user@server 或 ssh-copy-id -i ~/.ssh/id_rsa.pub user@server</p></li></ol><h1 id="4-关闭防火墙"><a href="#4-关闭防火墙" class="headerlink" title="4. 关闭防火墙"></a>4. 关闭防火墙</h1><ol><li>查看防火墙服务状态: systemctl status firewalld</li><li>关闭防火墙: systemctl stop firewalld</li><li>禁止开机启动：systemctl disable firewalld</li></ol><h1 id="5-禁用SELinux"><a href="#5-禁用SELinux" class="headerlink" title="5. 禁用SELinux"></a>5. 禁用SELinux</h1><ol><li>查看SElLinux状态: sestatus -v 或 getenforce</li><li>永久关闭SELinux：<ol><li>编辑vi /etc/selinux/config</li><li>修改SELINUX=disabled</li><li>重启主机</li></ol></li></ol><h1 id="6-安装Java环境"><a href="#6-安装Java环境" class="headerlink" title="6. 安装Java环境"></a>6. 安装Java环境</h1><p>查看是否安装了jdk：rpm -qa | grep jdk<br>如果没安装，则通过rpm安装JDK：rpm -ivh jdk-8u172-linux-x64.rpm </p><h1 id="7-元数据库安装"><a href="#7-元数据库安装" class="headerlink" title="7. 元数据库安装"></a>7. 元数据库安装</h1><h2 id="卸载mariadb"><a href="#卸载mariadb" class="headerlink" title="卸载mariadb"></a>卸载mariadb</h2><p>centos默认安装mariadb，需要先卸载以避免冲突。</p><ol><li>查看已安装MariaDB相关包<br>rpm -qa | grep -i mariadb</li><li>查看已安装的MariaDB相关yum包，包需要根据rpm命令的结果判断<br>yum list mariadb-libs</li><li>移除已安装的MariaDB相关的yum包，包名需根据yum list命令结果判断<br>yum remove mariadb-libs</li></ol><h2 id="安装MySQL"><a href="#安装MySQL" class="headerlink" title="安装MySQL"></a>安装MySQL</h2><ol><li><p>下载MySQL rpm包<br>官网下载地址：<a href="https://dev.mysql.com/downloads/mysql/5.7.html#downloads">https://dev.mysql.com/downloads/mysql/5.7.html#downloads</a></p></li><li><p>依次执行以下命令(包之间有前后依赖关系，务必按以下顺序安装)<br> rpm -ivh mysql-community-common-5.7.17-1.el7.x86_64.rpm<br> rpm -ivh mysql-community-libs-5.7.17-1.el7.x86_64.rpm<br> rpm -ivh mysql-community-client-5.7.17-1.el7.x86_64.rpm<br> rpm -ivh mysql-community-server-5.7.17-1.el7.x86_64.rpm</p><p> 如果执行上面第四个命令报错，执行如下命令：</p><ol><li>yum -y install perl</li><li>下载libaio rpm包：<a href="http://mirror.centos.org/centos/6/os/x86_64/Packages/libaio-0.3.107-10.el6.x86_64.rpm">http://mirror.centos.org/centos/6/os/x86_64/Packages/libaio-0.3.107-10.el6.x86_64.rpm</a></li><li>rpm -ivh libaio-0.3.107-10.el6.x86_64.rpm</li><li>yum -y install net-tools</li></ol></li><li><p>数据库初始化</p><ol><li><p>执行mysqld –initialize –user=mysql初始化，之后会在/var/log/mysqld.log中生成一个root账号密码</p></li><li><p>启动数据库 systemctl start mysqld，并设置mysql开机自启动 systemctl enable mysqld</p></li><li><p>登录 mysql -uroot -p</p></li><li><p>设置密码 alter user ‘root’@’localhost’ identified by ‘yourpasswd’;</p></li><li><p>创建集群组件所必须的元数据库，并给每个数据库设置用户名和密码<br> create database hive    default charset utf8 collate utf8_general_ci;<br> create database oozie  default charset utf8 collate utf8_general_ci;<br> create database hue    default charset utf8 collate utf8_general_ci;<br> create database amon default charset utf8 collate utf8_general_ci;</p><p> grant all on hive.* ‘hive’@’%’ identified by ‘123456’;<br> grant all on oozie .* ‘oozie ‘@’%’ identified by ‘123456’;<br> grant all on hue .* ‘hue ‘@’%’ identified by ‘123456’;<br> grant all on amon .* ‘amon ‘@’%’ identified by ‘123456’;</p></li><li><p>修改 /etc/my.cnf<br> my.cnf中配置参考<a href="https://www.cloudera.com/documentation/enterprise/5-15-x/topics/cm_ig_mysql.html#cmig_topic_5_5">https://www.cloudera.com/documentation/enterprise/5-15-x/topics/cm_ig_mysql.html#cmig_topic_5_5</a>中步骤4。</p></li></ol></li></ol><h1 id="8-安装-Cloudera-Manager"><a href="#8-安装-Cloudera-Manager" class="headerlink" title="8. 安装 Cloudera Manager"></a>8. 安装 Cloudera Manager</h1><ol><li><p>下载cloudera manger和CDH<br> 下载<a href="https://archive.cloudera.com/cm5/cm/5/">https://archive.cloudera.com/cm5/cm/5/</a>，版本选择 cloudera-manager-centos7-cm5.15.0_x86_64.tar.gz。</p><p> 下载<a href="https://archive.cloudera.com/cdh5/parcels/5.15.0/">https://archive.cloudera.com/cdh5/parcels/5.15.0/</a>，版本选择CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel 和 CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel.sha1</p></li><li><p>解压压缩包，并进行必要的配置</p><ol><li>将步骤1中下载的CM压缩包copy到/opt目录下；</li><li>解压 tar -zxvf cloudera-manager-centos7-cm5.15.0_x86_64.tar.gz；</li><li>配置CM server的主机名：vi cm-5.15.0/etc/cloudera-scm-agent/config.ini ；</li><li>在所有节点上创建用户，执行 useradd –system –home-dir /opt/cm-5.15.0/run/cloudera-scm-server/ </li></ol></li></ol><p>–no-create-home –shell=/bin/false –comment “Cloudera SCM User” cloudera-scm；<br>    5. 将mysql数据库驱动 mysql-connector-java-5.1.42-bin.jar 上传到所有节点的/opt/cm-5.15.0/share/cmf/lib/ 目录下；<br>    6. 为CM创建数据库 /opt/cm-5.15.0/share/cmf/schema/scm_prepare_database.sh  mysql -uroot<br>-pyourpasswd  scm yourscm（yourpasswd是新用户yourscm的密码）</p><h1 id="9-启动CM相关服务"><a href="#9-启动CM相关服务" class="headerlink" title="9. 启动CM相关服务"></a>9. 启动CM相关服务</h1><ol><li>集群主节点上启动Cloudera server： /opt/cm-5.15.0/etc/init.d/cloudera-scm-server start；</li><li>所有节点启动Cloudera agent：/opt/cm-5.15.0/etc/init.d/cloudera-scm-agent start；</li></ol><h1 id="10-Web端操作"><a href="#10-Web端操作" class="headerlink" title="10. Web端操作"></a>10. Web端操作</h1><ol><li><p>如果之前操作没出现异常，那么在浏览器中访问 hostname:7180（默认用户名密码都为admin）就会出现以下界面：<br><img src="01.png" alt="alt"></p></li><li><p>使用默认账户密码登录，之后出现如下界面：<br><img src="02.png" alt="alt"><br>选择免费的那个版本即可，当然也可以购买使用Cloudera提供的高级功能。</p></li><li><p>选好版本之后，点继续，出现以下界面：<br><img src="03.png" alt="alt"><br>将你配置的所有节点都勾选上，然后继续。</p></li><li><p>选择CDH版本<br><img src="04.png" alt="alt"></p></li><li><p>集群组件配置<br><img src="05.png" alt="alt"><br><img src="06.png" alt="alt"><br>因为有些组件是没必要在所有节点上都安装的，建议将组件均衡的安装到集群节点上，而不是集中在某几个节点上，否则可能会造成某些节点资源占用过多。</p></li><li><p>安装组件<br>选择好要安装的组件，点下一步，就会依次安装这些组件。如果安装过程中没出什么问题，那就大功告成了！<br>这里贴出本人在安装过程中遇到的问题：</p><ol><li>在测试Hive、Ooozie、Hue等的数据库连接时，Hue报错 Unexpected error. Unable to verify database connection<br>解决：选择把Hue安装在MySQL所安装的主机上，并安装以下包：<br>rpm -ivh mysql-community-libs-compat-5.7.20-1.el7.x86_64.rpm<br>yum install python-lxml</li><li>安装在过程中出现 java.lang.ClassNotFoundException: com.mysql.jdbc.Driver<br>将mysql驱动包<a href="https://repo1.maven.org/maven2/mysql/mysql-connector-java/5.1.42/mysql-connector-java-5.1.42.jar">mysql-connector-java-5.1.42.jar</a>拷贝到以下目录：<br>/opt/cloudera/parcels/CDH-5.15.0-1.cdh5.15.0.p0.21/lib/hive/lib<br>/opt/cloudera/parcels/CDH-5.15.0-1.cdh5.15.0.p0.21/lib/oozie/lib<br>/opt/cloudera/parcels/CDH-5.15.0-1.cdh5.15.0.p0.21/lib/oozie/libext </li><li>Cloudera Manager Web界面出现 Hue Load Balancer 运行状况不良<br>yum -y install httpd<br>yum -y install mod_ssl</li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Cloudera </tag>
            
            <tag> CDH </tag>
            
            <tag> 集群安装部署 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>手把手教你维度数据建模</title>
      <link href="2020/10/11/%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E7%BB%B4%E5%BA%A6%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1/"/>
      <url>2020/10/11/%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E7%BB%B4%E5%BA%A6%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1/</url>
      
        <content type="html"><![CDATA[<p>在这篇文章中，你将会学到如何一步步地进行维度数据建模，你将看到如何在真实的场景中使用维度模型。</p><a id="more"></a><h1 id="什么是维度数据建模"><a href="#什么是维度数据建模" class="headerlink" title="什么是维度数据建模"></a>什么是维度数据建模</h1><p>维度数据建模是在进行数仓设计时的一种数据建模方法。这种建模方法的主要目标是为了提高数据检索效率，对select查询操作进行了优化。维度数据建模最适合数仓星型模型和雪花模型。</p><p>数仓中的维度数据建模不同于ER建模(Entity-Relationship Model，关系-实体模型)，ER建模的主要目标是通过减少数据的冗余来规范化数据， 而维度数据建模使得数据一旦存储在数仓中后，能被更容易地获取。维度模型是许多OLAP系统的底层数据模型。</p><p>维度模型是被传奇人物Ralph Kimball提出的，你可以读读他的这本书<a href="https://www.amazon.in/Data-Warehouse-Toolkit-Complete-Dimensional/dp/8126544279/ref=as_li_ss_tl?ie=UTF8&keywords=kimball&qid=1470065827&ref_=sr_1_1&s=books&sr=1-1&linkCode=ll1&tag=dwgeecom-21&linkId=fa8b801a6414c3020dad1a1d29a429f6">The Data Warehouse Toolkit</a></p><h1 id="维度数据建模的步骤"><a href="#维度数据建模的步骤" class="headerlink" title="维度数据建模的步骤"></a>维度数据建模的步骤</h1><p>接下来我们通过一个示例来了解维度数据建模的步骤。场景：您希望存储某个MedPlus商店每天销售多少片paracetamol 和 diclofenac 的信息。建模过程中，所有数据都归为两类：维度表和事实表。事实表中包含度量信息，维度表中包含限定度量的信息。</p><p>下面是数据仓库维度建模示例的步骤：</p><h2 id="第一步：选择业务目标"><a href="#第一步：选择业务目标" class="headerlink" title="第一步：选择业务目标"></a>第一步：选择业务目标</h2><p>在我们的例子中，业务目标就是存储单个商店每天paracetamol 和diclofenac 的销售数据。</p><h2 id="第二步：确定粒度"><a href="#第二步：确定粒度" class="headerlink" title="第二步：确定粒度"></a>第二步：确定粒度</h2><p>粒度是表中存储的最低级别的信息。例如，如果表包含每日销售数据，则粒度为“每日”。</p><p>在我们的例子中，假设一个特定的MedPlus商店在特定的一天销售1000片paracetamol ，那么粒度是每天，而在特定的月份销售10000片，那么粒度是每月。</p><p>设定粒度信息是非常重要的，我们的例子采用的是“每日”的粒度。</p><h2 id="第三步：确定维度和维度属性"><a href="#第三步：确定维度和维度属性" class="headerlink" title="第三步：确定维度和维度属性"></a>第三步：确定维度和维度属性</h2><p>在我们的例子中，可以确定三个维度：商店、药品(paracetamol 和diclofenac)和日期。下面是维度表的结构。<br><strong>Medicine</strong><br>| SK | NAME |<br>|–| –|<br>| 1 | Paracetamol |<br>| 2 | Diclofenac |</p><p><strong>Shop</strong><br>| SK | NAME |<br>|–| –|<br>| 1 | shop1|<br>| 2 | shop2|<br>| 3 | shop3|</p><p><strong>Day</strong><br>| SK | NAME |<br>|–| –|<br>| 1 | 2019-01-01|<br>| 2 | 2019-01-02|<br>| 3 | 2019-01-03|</p><h2 id="第四步：确定事实表"><a href="#第四步：确定事实表" class="headerlink" title="第四步：确定事实表"></a>第四步：确定事实表</h2><p>事实表包含的是一些可度量的东西。例子中，药片的销售量就是一个度量，我们可以创建单独的事实表来存储这些度量。</p><p>例子中，粒度是每天销售的药片，我将Medicine、Shop和Day这三张表的SK列添加到下面的事实表中。<br><img src="01.png" alt="alt"><br>在这个例子中，我们创建了3个维表和1个事实表，维度表通过外键连接到事实表上，这个模型看起来像个星型，也被称为<strong>星型模型</strong>。</p><h1 id="维度建模的优势"><a href="#维度建模的优势" class="headerlink" title="维度建模的优势"></a>维度建模的优势</h1><ol><li>提升了数据查询效率：通过冗余存储减少了关联查询</li><li>简化了业务报表逻辑</li><li>更容易理解：维度模型中的数据不是维度就是事实</li><li>可扩展性：维度模型可以更新新的维度。</li></ol><h1 id="维度数据建模工具"><a href="#维度数据建模工具" class="headerlink" title="维度数据建模工具"></a>维度数据建模工具</h1><p>建议可以尝试被广泛使用的erwin数据模型工具。</p>]]></content>
      
      
      <categories>
          
          <category> 数仓建模 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据仓库 </tag>
            
            <tag> 维度建模 </tag>
            
            <tag> 星型模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop集群数据迁移</title>
      <link href="2020/10/11/Hadoop%E9%9B%86%E7%BE%A4%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB/"/>
      <url>2020/10/11/Hadoop%E9%9B%86%E7%BE%A4%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="集群版本"><a href="#集群版本" class="headerlink" title="集群版本"></a>集群版本</h1><pre><code>源集群: Hadoop-2.7.3、Hive-1.2.1(均为Apache开源版本)目标集群: Hadoop-2.6.0、Hive-1.1.0(均为CDH-5.15.0版本)</code></pre><a id="more"></a><h1 id="迁移步骤"><a href="#迁移步骤" class="headerlink" title="迁移步骤"></a>迁移步骤</h1><h2 id="第一步：Hive-export命令导出表到HDFS指定目录"><a href="#第一步：Hive-export命令导出表到HDFS指定目录" class="headerlink" title="第一步：Hive export命令导出表到HDFS指定目录"></a>第一步：Hive export命令导出表到HDFS指定目录</h2><pre><code>hive -e &quot;export table test.user_info to &#39;/hive_export/test.user_info&#39;;</code></pre><blockquote><p>import和export命令的基本用法可以参考:<br>    <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ImportExport">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ImportExport</a></p></blockquote><p>默认配置参数情况下，如果表中单个文件大于32M，导出时会报错：</p><pre><code>Failed with exception Cannot get DistCp constructor: org.apache.hadoop.tools.DistCp.&lt;init&gt;()FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.CopyTask</code></pre><blockquote><p>原因：由于export命令相当于拷贝hive表数据到指定目录，拷贝文件的默认最大值为hive.exec.<br>copyfile.maxsize=33554432(32M)，超过了这个阈值，会报上述错误。解决办法就是给参数hive.<br>exec.copyfile.maxsize设置一个适合你表文件大小的值，例如set hive.exec.copyfile.maxsize = 335544320(320M)。</p></blockquote><h2 id="第二步：传输数据到目标集群"><a href="#第二步：传输数据到目标集群" class="headerlink" title="第二步：传输数据到目标集群"></a>第二步：传输数据到目标集群</h2><pre><code>hadoop distcp hftp://host:50070/hive_export/test.user_info/* hdfs://nameservice1/hive_import/test.user_info</code></pre><p>distcp基本用法可参考：<a href="http://hadoop.apache.org/docs/current/hadoop-distcp/DistCp.html">http://hadoop.apache.org/docs/current/hadoop-distcp/DistCp.html</a></p><blockquote><p>注意： 此命令要在目标集群的Namenode节点执行</p></blockquote><p>如果你的源集群和目标集群版本不一致会报以下错误信息：</p><pre><code>Caused by: java.io.IOException: Couldn&#39;t run retriable-commandCaused by: java.io.IOException: Check-sum mismatch between</code></pre><blockquote><p>解决方法：可以在distcp后面跟上 -Ddfs.checksum.type=CRC32 或 -pb</p></blockquote><h2 id="第三步：Hive-import命令将数据导入到Hive表"><a href="#第三步：Hive-import命令将数据导入到Hive表" class="headerlink" title="第三步：Hive import命令将数据导入到Hive表"></a>第三步：Hive import命令将数据导入到Hive表</h2><pre><code>hive -e &quot;import table test.user_info from &#39;/hive_import/test.user_info&#39;;&quot;</code></pre><p>可以给表重命名，也可以导出为外部表。可参考：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ImportExport">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ImportExport</a></p><h2 id="第四步：修改hive表存储路径"><a href="#第四步：修改hive表存储路径" class="headerlink" title="第四步：修改hive表存储路径"></a>第四步：修改hive表存储路径</h2><blockquote><p>只有在源集群和目标集群的NameNode别名不一致时才执行，否则Spark SQL无法访问Hive表。</p></blockquote><pre><code>hive -e &quot;alter table test.user_info set serdeproperties (&#39;path&#39; = &#39;hdfs://nameservice1/user/hive/warehouse/test.db/user_info&#39;);“</code></pre>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python读写Phoenix</title>
      <link href="2020/10/11/Python%E8%AF%BB%E5%86%99Phoenix/"/>
      <url>2020/10/11/Python%E8%AF%BB%E5%86%99Phoenix/</url>
      
        <content type="html"><![CDATA[<p>Apache Phoenix的Python驱动程序实现了Python DB 2.0 API，来通过Phoenix Query Server访问Phoenix。</p><a id="more"></a><h1 id="模块安装"><a href="#模块安装" class="headerlink" title="模块安装"></a>模块安装</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install phoenixdb</span><br></pre></td></tr></table></figure><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>Python版本：3.6.5<br>Phoenix版本：4.14</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> phoenixdb</span><br><span class="line"><span class="keyword">import</span> phoenixdb.cursor</span><br><span class="line"></span><br><span class="line">database_url = <span class="string">&#x27;http://localhost:8765/&#x27;</span> <span class="comment"># 8765为连接Phoenix的默认端口号</span></span><br><span class="line">conn = phoenixdb.connect(database_url, autocommit=<span class="literal">True</span>) <span class="comment"># 注意要设置自动提交</span></span><br><span class="line"></span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">cursor.execute(<span class="string">&quot;CREATE TABLE users (id INTEGER PRIMARY KEY, username VARCHAR)&quot;</span>)</span><br><span class="line">cursor.execute(<span class="string">&quot;UPSERT INTO users VALUES (?, ?)&quot;</span>, (<span class="number">1</span>, <span class="string">&#x27;admin&#x27;</span>))</span><br><span class="line">cursor.execute(<span class="string">&quot;SELECT * FROM users&quot;</span>)</span><br><span class="line">print(cursor.fetchall())</span><br><span class="line"></span><br><span class="line">cursor = conn.cursor(cursor_factory=phoenixdb.cursor.DictCursor)</span><br><span class="line">cursor.execute(<span class="string">&quot;SELECT * FROM users WHERE id=1&quot;</span>)</span><br><span class="line">print(cursor.fetchone()[<span class="string">&#x27;USERNAME&#x27;</span>])</span><br></pre></td></tr></table></figure><h1 id="程序执行"><a href="#程序执行" class="headerlink" title="程序执行"></a>程序执行</h1><p>执行Python程序之前必须先启动Phoenix Query Server，具体就是在Phoenix安装路径下执行如下脚本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/queryserver.py start</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Phoenix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Phoenix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL表同步到Hive</title>
      <link href="2020/10/11/MySQL%E8%A1%A8%E5%90%8C%E6%AD%A5%E5%88%B0Hive/"/>
      <url>2020/10/11/MySQL%E8%A1%A8%E5%90%8C%E6%AD%A5%E5%88%B0Hive/</url>
      
        <content type="html"><![CDATA[<p>我们在往数据仓库中填充数据的第一步，就是要把RDBMS中的表导入到HDFS中。基于MapReduce的Sqoop工具可以很方便的将关系型数据库中的表导入到Hive、HBase中，并且记录可以以文本文件(每行一条记录)存储，也可以以Avro或SequenceFiles的二进制格式存储。</p><a id="more"></a><h1 id="Sqoop命令"><a href="#Sqoop命令" class="headerlink" title="Sqoop命令"></a>Sqoop命令</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://host:3306/mysqldb?autoReconnect=true&amp;tinyInt1isBit=false&amp;zeroDateTimeBehavior=round \</span><br><span class="line">--username user_name \</span><br><span class="line">--password-file /home/sqoop/password \</span><br><span class="line">--hive-import \</span><br><span class="line">--hive-database hivedb \</span><br><span class="line">--hive-table hivetable \</span><br><span class="line">--query &quot;select id,user_id,user_name from mysql_table where (updated_at &gt;= &#x27;2019-04-19 00:00:00&#x27;) and \$CONDITIONS&quot; \</span><br><span class="line">--null-string &#x27;\\N&#x27; \</span><br><span class="line">--null-non-string &#x27;\\N&#x27; \</span><br><span class="line">--hive-drop-import-delims \</span><br><span class="line">--map-column-java id=String,user_id=String,user_name=String</span><br><span class="line">--target-dir /tmp \</span><br><span class="line">--as-parquetfile \</span><br><span class="line">--fields-terminated-by &#x27;^&#x27; \</span><br><span class="line">-m 1</span><br></pre></td></tr></table></figure><h1 id="参数详解"><a href="#参数详解" class="headerlink" title="参数详解"></a>参数详解</h1><ul><li><strong>–connect</strong>： 要连接的服务器和数据库  </li><li><strong>–username</strong>： 数据库连接用户名  </li><li><strong>–password-file</strong>：  数据库连接密码。也可以使用参数–password(直接将密码写在参数后面)或-P(控制台输入)，但是向数据库提供密码的安全方式是将密码保存在用户目录下的一个文件中，并赋予400权限。 </li><li><strong>–hive-import</strong>：  导入表到Hive </li><li><strong>–hive-database</strong>：  指定要导入的Hive数据仓库名 </li><li><strong>–hive-table</strong>：  指定要导入的Hive表名 </li><li><strong>–query</strong>：  导入指定的statement查询结果 </li><li><strong>–null-string</strong>：  对于string类型的列，要转换成null的字符串。具体含义：因为在默认情况下，Sqoop会将NULL值导入为null字符串(“null”)，但是Hive使用字符串\N(“\N”)来表示NULL值，因此谓词处理将不能正确工作(像is null)。如果希望将MySQL中的NULL导入到Hive之后还同样是NULL，可添加参数–null-string和–null-non-string。 </li><li><strong>–null-non-string</strong>：  对于非string类型的列，要转换为null的字符串，含义同上。 </li><li><strong>–hive-drop-import-delims</strong>：  如果你的数据库的行包含具有Hive默认行分隔符(\n和\r字符)或列分隔符(\01字符)的字符串字段，则在Sqoop-import导入时会出现问题。可以使用–hive-drop-import-delims选项在导入时删除这些字符，以提供与Hive兼容的文本数据。或者也可以使用–hive-delims-replacement选项，使用用户定义的导入字符替换这些字符。 </li><li><strong>–map-column-java</strong>：  Sqoop预先配置为将大多数SQL类型映射到合适的Java或Hive类型，然而，默认的映射并不能适用于所有情况，可以使用–map-column-java选项覆盖用于将映射更改为Java或使用–map-column-hive选项覆盖用于更改Hive映射。 </li><li><strong>–target-dir</strong>：  MapReduce job指定在HDFS上的输出目录 </li><li><strong>–as-parquetfile</strong>：  导入数据为parquet格式的文件 </li><li><strong>–fields-terminated-by</strong>：  字段分隔符 </li><li><strong>-m</strong>：  控制map任务数量，也就是并行度 </li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html#_connecting_to_a_database_server">http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html#_connecting_to_a_database_server</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark实现单词统计WordCount (Scala/Python/Java)</title>
      <link href="2020/10/11/%E5%8D%95%E8%AF%8D%E7%BB%9F%E8%AE%A1WordCount-Scala-Python-Java/"/>
      <url>2020/10/11/%E5%8D%95%E8%AF%8D%E7%BB%9F%E8%AE%A1WordCount-Scala-Python-Java/</url>
      
        <content type="html"><![CDATA[<p>入门Spark的第一个小例子就是编写一个简单的WordCount程序，来统计一个文件中每个单词的个数。接下来分别是用Scala、Python和Java三种语言来实现WordCount的代码，都是基于<strong>Spark2.0+版本</strong>和<strong>本地local模式</strong>。</p><a id="more"></a><h1 id="编程语言选择"><a href="#编程语言选择" class="headerlink" title="编程语言选择"></a>编程语言选择</h1><p><strong>首先</strong>，推荐使用Scala，因为Spark的底层源码主要是基于Scala编写的，对Scala的支持最友好，而且Scala这种函数式编程语言编写代码比较简洁，建议首选Scala。</p><p><strong>其次</strong>，推荐使用Python编写Spark应用程序，但是性能会稍差，而且Spark的新功能对Python的支持也稍差。如果要用编写机器学习程序，建议使用Python，因为Python提供了丰富的类库。</p><p><strong>最后</strong>，才推荐使用Java，因为代码写起来是在太冗余了，不过Spark对Java的支持要比Python好的多。</p><p>当然你也可以用R语言，不过支持是最差的。</p><h1 id="Scala实现"><a href="#Scala实现" class="headerlink" title="Scala实现"></a>Scala实现</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object ScalaWordCount &#123;</span><br><span class="line">    <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">        <span class="comment">// SparkSession实例, 也就是Spark任务的入口</span></span><br><span class="line">        val spark = SparkSession</span><br><span class="line">            .builder()</span><br><span class="line">            .appName(<span class="string">&quot;ScalaWordCount&quot;</span>)</span><br><span class="line">            .master(<span class="string">&quot;local[4]&quot;</span>) <span class="comment">// 本地模式, 使用4个cpu cores</span></span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取本地的一个txt文件, 里面是随意copy进去的一些英文文章片段</span></span><br><span class="line">        val lines = spark.sparkContext.textFile(<span class="string">&quot;F:\\Files\\WordCount.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 文章中的单词都是以空格分隔的,</span></span><br><span class="line">        <span class="comment">// flatMap函数的作用就是对文件中的每一行单词以空格来分隔,</span></span><br><span class="line">        <span class="comment">// 得到是摊平的每个独立的单词, 如果打印出来的话就是每行一个单词。</span></span><br><span class="line">        val words = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将每个单词转换成key-value对, value都是1,</span></span><br><span class="line">        <span class="comment">// 例如：spark 转换成 (spark, 1)。</span></span><br><span class="line">        val ones = words.map(m =&gt; (m, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据key进行聚合, 对于相同的key, 将其对应的value相加。</span></span><br><span class="line">        val counts = ones.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 打印结果, 可以看到文件中每个单词的个数</span></span><br><span class="line">        counts.foreach(f =&gt; &#123;</span><br><span class="line">            println(f._1 +<span class="string">&quot;: &quot;</span>+ f._2)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 或者也可以保存到一个文件中。</span></span><br><span class="line">        <span class="comment">//counts.coalesce(1).saveAsTextFile(&quot;F:\\Files\\WordCountResult.txt&quot;)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 关闭SparkSession</span></span><br><span class="line">        spark.close()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Python实现"><a href="#Python实现" class="headerlink" title="Python实现"></a>Python实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spark = SparkSession\</span><br><span class="line">        .builder\</span><br><span class="line">        .appName(<span class="string">&quot;python word count&quot;</span>)\</span><br><span class="line">        .master(<span class="string">&#x27;local[4]&#x27;</span>)\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    lines = spark.sparkContext.textFile(<span class="string">&#x27;F:\\Files\\WordCount.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># lambda函数以空格分隔每行单词</span></span><br><span class="line">    words = lines.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Python中也是元组Tuple</span></span><br><span class="line">    ones = words.map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 需要用到add这个求和函数</span></span><br><span class="line">    counts = ones.reduceByKey(add)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印结果</span></span><br><span class="line">    counts.foreach(<span class="keyword">lambda</span> x: print(str(x[<span class="number">0</span>]) + <span class="string">&#x27;: &#x27;</span> + str(x[<span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br></pre></td></tr></table></figure><h1 id="Java实现"><a href="#Java实现" class="headerlink" title="Java实现"></a>Java实现</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JavaWordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SparkSession spark = SparkSession</span><br><span class="line">            .builder()</span><br><span class="line">            .appName(<span class="string">&quot;JavaWordCount&quot;</span>)</span><br><span class="line">            .master(<span class="string">&quot;local[4]&quot;</span>)</span><br><span class="line">            .getOrCreate();</span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;String&gt; lines = spark.sparkContext()</span><br><span class="line">            .textFile(<span class="string">&quot;F:\\Files\\WordCount.txt&quot;</span>, <span class="number">2</span>) <span class="comment">//要给一个分区数</span></span><br><span class="line">            .toJavaRDD();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 以空格分隔每行单词</span></span><br><span class="line">        JavaRDD&lt;String&gt; words = lines</span><br><span class="line">            .flatMap(s -&gt; Arrays.asList(s.split(<span class="string">&quot; &quot;</span>)).iterator());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将每个单词转换成元组Tuple(其实就是key-value对)</span></span><br><span class="line">        JavaPairRDD&lt;String, Integer&gt; ones = words.mapToPair(s -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(s, <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 以key聚合求值</span></span><br><span class="line">        JavaPairRDD&lt;String, Integer&gt; counts = ones.reduceByKey((i1, i2) -&gt; i1 + i2);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 打印结果</span></span><br><span class="line">        counts.foreach(f -&gt; System.out.println(f._1 +<span class="string">&quot;: &quot;</span>+ f._2));</span><br><span class="line"></span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YARN/Hadoop/HDFS常用命令</title>
      <link href="2020/10/11/YARN-Hadoop-HDFS%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>2020/10/11/YARN-Hadoop-HDFS%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>文件系统(FS)shell包含了一系列的与shell相似的命令，很多都对应着Unix中的命令，这些命令可以直接与Hadoop分布式文件系统(HDFS)以及Hadoop支持的其他文件系统，如本地FS、HFTP、S3等进行交互。可以通过如下方法调用：<code>bin/hadoop fs &lt;args&gt;</code>。args都是以路径URI作为参数，本篇文章都是基于HDFS来介绍说明的。</p><a id="more"></a><p>一个HDFS中的文件或目录的格式大致是这样：<code>hdfs://namenodehost/test/a.txt</code>。namenodehost即为NameNode节点的主机名。如果你的Hadoop安装目录../hadoop/etc/hadoop/core-site.xml的文件中有如下配置，那么在操作一个文件或目录时，可以省掉前缀，即/test/a.txt。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://namenodehost<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h1 id="YARN命令"><a href="#YARN命令" class="headerlink" title="YARN命令"></a>YARN命令</h1><h3 id="yarn-application"><a href="#yarn-application" class="headerlink" title="yarn application"></a>yarn application</h3><ul><li>yarn application -list  appID<br>查看正在运行的yarn任务列表</li><li>yarn application -kill  appID<br>kill掉指定id的yarn任务<h3 id="yarn-logs"><a href="#yarn-logs" class="headerlink" title="yarn logs"></a>yarn logs</h3></li><li>yarn logs -applicationId  appID<br>如果日志聚合被开启(yarn.log-aggregation-enable), container日志会被拷贝到HDFS上，同时删除本地的日志。这些日志可以在集群的任何地方使用yarn logs命令查看。注意只有在任务结束之后才能用此命令查看任务日志信息。</li></ul><h1 id="Hadoop命令"><a href="#Hadoop命令" class="headerlink" title="Hadoop命令"></a>Hadoop命令</h1><h3 id="ls"><a href="#ls" class="headerlink" title="ls"></a>ls</h3><p>返回指定文件或指定目录下所有文件的权限、副本数、所属用户组等信息。</p><p>示例：hadoop fs -ls /test/</p><h3 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h3><p>显示文件内容到标准输出。</p><p>示例：<br>hadoop fs -cat /test/src.txt 显示文件src.txt的所有内容<br>hadoop fs -cat /test/src.txt | less 对于内容较多的文件可以用less上下翻页</p><h3 id="cp"><a href="#cp" class="headerlink" title="cp"></a>cp</h3><p>拷贝文件从源地址到目标地址。允许拷贝一次拷贝多个文件到目标目录下。</p><p>示例：hadoop fs -cp /test/src.txt /test/des.txt拷贝时可以改变文件名</p><h3 id="mv"><a href="#mv" class="headerlink" title="mv"></a>mv</h3><p>将文件或目录从源地址移动到目标地址。</p><p>示例：hadoop fs -mv /test/src.txt /test2/</p><h3 id="touchz"><a href="#touchz" class="headerlink" title="touchz"></a>touchz</h3><p>创建一个空文件。</p><p>示例：hadoop fs -touchz /test/src.txt</p><h3 id="mkdir"><a href="#mkdir" class="headerlink" title="mkdir"></a>mkdir</h3><p>创建目录。</p><p>示例：<br>hadoop fs -mkdir /test/dir创建目录dir<br>hadoop fs -mkdir -p /test/dir1/dir2/dir3递归地创建dir1、dir2和dir3</p><h3 id="rm"><a href="#rm" class="headerlink" title="rm"></a>rm</h3><p>删除指定的文件。</p><p>示例：<br>hadoop fs -rm /test/des.txt删除文件des.txt<br>hadoop fs -rm -r /test递归地删除test目录以及test下面的所有目录和文件(<font color=red><strong>慎用啊！</strong></font>)。</p><h3 id="rmr"><a href="#rmr" class="headerlink" title="rmr"></a>rmr</h3><p>递归地删除文件和目录。此命令已被弃用，代替使用-rm -r。</p><h3 id="rmdir"><a href="#rmdir" class="headerlink" title="rmdir"></a>rmdir</h3><p>删除指定目录。</p><p>示例：hadoop fs -rmdir /test</p><h3 id="get"><a href="#get" class="headerlink" title="get"></a>get</h3><p>拷贝HDFS中的文件到本地文件系统。</p><p>示例：hadoop fs -get /test/src.txt . 拷贝src.txt文件到本地文件系统的当前目录下</p><h3 id="put"><a href="#put" class="headerlink" title="put"></a>put</h3><p>拷贝本地文件系统中的文件到HDFS上。</p><p>示例：hadoop fs -put src.txt /test/</p><h3 id="copyFromLocal"><a href="#copyFromLocal" class="headerlink" title="copyFromLocal"></a>copyFromLocal</h3><p>功能和put类似，除了要求源文件一定要是本地文件。</p><h3 id="copyToLocal"><a href="#copyToLocal" class="headerlink" title="copyToLocal"></a>copyToLocal</h3><p>功能和get类似，除了目标文件要是本地文件引用。</p><h3 id="moveFromLocal"><a href="#moveFromLocal" class="headerlink" title="moveFromLocal"></a>moveFromLocal</h3><p>功能类似put，除了拷贝之后会删除源文件。</p><h3 id="moveToLocal"><a href="#moveToLocal" class="headerlink" title="moveToLocal"></a>moveToLocal</h3><p>目前无法使用，执行命令会显示 “Not implemented yet” 。</p><h3 id="df"><a href="#df" class="headerlink" title="df"></a>df</h3><p>显示文件系统空间占用情况。</p><p>示例：hadoop fs -df -h /test/src.txt<br>Filesystem SizeUsedAvailableUse%<br>hdfs://nnhost5T 2.1T 2.9T42%</p><h3 id="du"><a href="#du" class="headerlink" title="du"></a>du</h3><p>显示文件大小，或者目录中所有文件大小。</p><p>示例：<br>hadoop fs -du -h /test/src.txt显示文件大小<br>hadoop fs -du -h /test/依次显示目录下所有文件的大小</p><h3 id="find"><a href="#find" class="headerlink" title="find"></a>find</h3><p>显示指定目录下匹配指定表达式的所有文件。</p><p>示例：hadoop fs -find /test -name “*.txt”显示/test目录下所有以.txt结尾的文件</p><h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><p>依次显示URI下面的总的目录数、文件数和字节数。</p><p>示例：hadoop fs -count /test/<br>目录数文件数字节数<br>50000 10000 409600000</p><h3 id="tail"><a href="#tail" class="headerlink" title="tail"></a>tail</h3><p>显示文件的最后一kb到标准输出。</p><p>示例：hadoop fs -tail /test/src.txt</p><h3 id="chmod"><a href="#chmod" class="headerlink" title="chmod"></a>chmod</h3><p>修改文件的权限。</p><p>示例：<br>hadoop fs -chmod 744 /test/src.txt修改文件src.txt的权限为r+w+x<br>hadoop fs -chmod -R 744 /test/递归地修改test目录及其子目录下所有文件的权限</p><h3 id="chown"><a href="#chown" class="headerlink" title="chown"></a>chown</h3><p>修改文件所属的用户</p><p>示例：hadoop fs -chown [-R] owner:group /test/src.txt</p><h3 id="checksum"><a href="#checksum" class="headerlink" title="checksum"></a>checksum</h3><p>查看文件的校验信息。</p><p>示例：hadoop fs -checksum /test/src.txt</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> Yarn </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark connector连接器之整合读写MySQL及问题汇总</title>
      <link href="2020/10/11/Spark-connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99MySQL%E5%8F%8A%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/"/>
      <url>2020/10/11/Spark-connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99MySQL%E5%8F%8A%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<p>Spark主要是以JDBC驱动的方式读写MySQL的。在提交Spark作业时别忘记了打包驱动包mysql-connector-java-5.1.47.jar(用合适自己项目的版本)。</p><a id="more"></a><h2 id="Spark读取MySQL表可选配置参数"><a href="#Spark读取MySQL表可选配置参数" class="headerlink" title=" Spark读取MySQL表可选配置参数"></a><center> <font color="blue">Spark读取MySQL表可选配置参数</h2><p>详见: <a href="https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html">https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html</a></p><h2 id="Spark读取MySQL表"><a href="#Spark读取MySQL表" class="headerlink" title=" Spark读取MySQL表"></a><center> <font color="blue">Spark读取MySQL表</h2><p>Spark提供了两种(并行)读MySQL的方式：基于整型列并行读取和基于范围并行读取。</p><h5 id="1-基于整型列设置并行度"><a href="#1-基于整型列设置并行度" class="headerlink" title="1. 基于整型列设置并行度"></a>1. 基于整型列设置并行度</h5><p>先上代码，对着代码再做详细的解释：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">readByIntegralColumn</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val options = Map(</span><br><span class="line">        <span class="string">&quot;url&quot;</span> -&gt; <span class="string">&quot;jdbc:mysql://host:3306/dbName&quot;</span>, <span class="comment">// jdbc的url</span></span><br><span class="line">        <span class="string">&quot;dbtable&quot;</span> -&gt; <span class="string">&quot;tableName&quot;</span>, <span class="comment">//MySQL表名</span></span><br><span class="line">        <span class="string">&quot;user&quot;</span> -&gt; <span class="string">&quot;userName&quot;</span>, <span class="comment">//用户名</span></span><br><span class="line">        <span class="string">&quot;password&quot;</span> -&gt; <span class="string">&quot;passwd&quot;</span>, <span class="comment">//密码</span></span><br><span class="line">        <span class="string">&quot;driver&quot;</span> -&gt; <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>, <span class="comment">//驱动</span></span><br><span class="line">        <span class="string">&quot;partitionColumn&quot;</span> -&gt; <span class="string">&quot;id&quot;</span>, <span class="comment">//被用来分区的整型列</span></span><br><span class="line">        <span class="string">&quot;lowerBound&quot;</span> -&gt; <span class="string">&quot;1&quot;</span>, <span class="comment">//要读取的整型列的下界(包含下界)</span></span><br><span class="line">        <span class="string">&quot;UpperBound&quot;</span> -&gt; <span class="string">&quot;400000&quot;</span>, <span class="comment">//要读取的整型列的上界(不包含上界)</span></span><br><span class="line">        <span class="string">&quot;numPartitions&quot;</span> -&gt; <span class="string">&quot;10&quot;</span> <span class="comment">//分区数</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    spark.read</span><br><span class="line">        .format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">        .options(options)</span><br><span class="line">        .load()</span><br><span class="line">        .write</span><br><span class="line">        .saveAsTable(<span class="string">&quot;dbName.tableName&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面代码中，要读取MySQL中id值从1到400000的行，并划分10个分区，每个分区平均读取40000条记录。</p><h5 id="2-基于范围设置并行度"><a href="#2-基于范围设置并行度" class="headerlink" title="2. 基于范围设置并行度"></a>2. 基于范围设置并行度</h5><p>同样先上代码，对着代码再做详细的解释：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">readByRange</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line"><span class="comment">//用户名和密码</span></span><br><span class="line">    val prop = <span class="keyword">new</span> Properties()</span><br><span class="line">    prop.put(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;userName&quot;</span>)</span><br><span class="line">    prop.put(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;passwd&quot;</span>)</span><br><span class="line"></span><br><span class="line">    val url = <span class="string">&quot;jdbc:mysql://host:3306//dbName&quot;</span> <span class="comment">//url</span></span><br><span class="line">    val table = <span class="string">&quot;tableName&quot;</span> <span class="comment">//表名</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//predicates参数就相当于在where表达式中的范围, 有几个范围就有几个分区进行并行读取</span></span><br><span class="line">    val predicates = Array[String](</span><br><span class="line">        <span class="string">&quot;created_at &lt; &#x27;2018-08-01 00:00:00&#x27;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;created_at &gt;= &#x27;2018-08-01 00:00:00&#x27; &amp;&amp; created_at &lt; &#x27;2018-10-01 00:00:00&#x27;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;created_at &gt;= &#x27;2018-10-01 00:00:00&#x27;&quot;</span>)</span><br><span class="line"></span><br><span class="line">    spark.read</span><br><span class="line">        .jdbc(url, table, predicates, prop)</span><br><span class="line">        .write.saveAsTable(<span class="string">&quot;dbName.tableName&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面代码中，基于MySQL中记录的创建时间来划分分区，predicates中设置的范围区间数就是分区数。当然，也可以是使用其他任何可以进行区间查询的列来设置分区数。</p><p>注意：<font color=red><strong>不要在集群上并行创建太多分区，否则可能会给MySQL产生很大的访问压力，甚至可能会导致数据库系统崩溃</strong></font></p><h2 id="Spark写入MySQL表"><a href="#Spark写入MySQL表" class="headerlink" title=" Spark写入MySQL表"></a><center> <font color="blue">Spark写入MySQL表</h2><p>Spark写MySQL比较简单，直接看代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">writeMySQL</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val host = <span class="string">&quot;hostname&quot;</span> <span class="comment">//MySQL服务器地址</span></span><br><span class="line">    val port = <span class="string">&quot;3306&quot;</span> <span class="comment">//端口号</span></span><br><span class="line">    val userName = <span class="string">&quot;userName&quot;</span> <span class="comment">//用户名</span></span><br><span class="line">    val password = <span class="string">&quot;password&quot;</span> <span class="comment">//访问密码</span></span><br><span class="line">    val dbName = <span class="string">&quot;dbName&quot;</span> <span class="comment">//库名</span></span><br><span class="line">    val jdbcUrl = s<span class="string">&quot;jdbc:mysql://$&#123;host&#125;:$&#123;port&#125;/$&#123;dbName &#125;&quot;</span> <span class="comment">//jdbc url</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> java.util.Properties</span><br><span class="line">    val prop = <span class="keyword">new</span> Properties()</span><br><span class="line">    prop.put(<span class="string">&quot;user&quot;</span>, userName)</span><br><span class="line">    prop.put(<span class="string">&quot;password&quot;</span>, password)</span><br><span class="line"></span><br><span class="line">    spark.read.table(<span class="string">&quot;db.test&quot;</span>)</span><br><span class="line">            .coalesce(<span class="number">10</span>) <span class="comment">//调节写入并行度(增加并行度要用repartition(n))</span></span><br><span class="line">            .write</span><br><span class="line">            .mode(<span class="string">&quot;append&quot;</span>) <span class="comment">//追加写入()</span></span><br><span class="line">            .jdbc(jdbcUrl, <span class="string">&quot;test&quot;</span>, prop)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>写入的时候需要注意并行度，以免给MySQL带来太大写入压力。</p><h2 id="Spark读写MySQL问题汇总"><a href="#Spark读写MySQL问题汇总" class="headerlink" title=" Spark读写MySQL问题汇总"></a><center> <font color="blue">Spark读写MySQL问题汇总</h2><h5 id="1-Spark写MySQL覆盖表结构问题"><a href="#1-Spark写MySQL覆盖表结构问题" class="headerlink" title="1. Spark写MySQL覆盖表结构问题"></a>1. Spark写MySQL覆盖表结构问题</h5><p><font color="red"><strong>问题</strong></font>：你在MySQL中创建了一个表user，现在你要通过Spark将DataFrame中的数据写入到表user中，代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df</span><br><span class="line">    .coalesce(<span class="number">2</span>)</span><br><span class="line">    .write</span><br><span class="line">    .mode(<span class="string">&quot;overwrite&quot;</span>)</span><br><span class="line">    .jdbc(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;db.user&quot;</span>, props)</span><br></pre></td></tr></table></figure><p><font color="red"><strong>原因分析</strong></font>：因为每次都是要覆盖之前表中的所有数据，所以写入mode类型为overwrite。而overwrite模式的底层执行机制是，先把之前的表user删掉(drop table db.user)，然后再根据要写入的DataFrame的schema及字段类型创建新的表，这就造成你原来的建表语句就失效了(比如，之前指定的主键、字段类型等都被覆盖了)。</p><p>造成这个问题的原因就是，DataFrame在写MySQL的时候，有一个选项”truncate”, 默认值是false, false情况下覆盖写(overwrite)是执行”drop table”，而true情况下覆盖写才会执行”truncate table”。</p><p><font color="red"><strong>解决方法</strong></font>：将选项”truncate”设置为true即可，代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df</span><br><span class="line">    .coalesce(<span class="number">2</span>)</span><br><span class="line">    .write</span><br><span class="line">    .option(<span class="string">&quot;truncate&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">    .mode(<span class="string">&quot;overwrite&quot;</span>)</span><br><span class="line">    .jdbc(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;db.user&quot;</span>, props)</span><br></pre></td></tr></table></figure><h5 id="2-Spark读MySQL写Hive，boolean-true、false-类型自动转成了0和1"><a href="#2-Spark读MySQL写Hive，boolean-true、false-类型自动转成了0和1" class="headerlink" title="2. Spark读MySQL写Hive，boolean(true、false)类型自动转成了0和1"></a>2. Spark读MySQL写Hive，boolean(true、false)类型自动转成了0和1</h5><p><font color="red"><strong>问题</strong></font>：当MySQL中某个列表示的是tinyint(1)的Boolean类型时(true或false)，写到Hive后，查询显示的是1或0。</p><p><font color="red"><strong>原因分析</strong></font>：因为MySQL没有内建的Boolean类型，而是通过tinyint(1)来代替Boolean类型的，0代表false，1代表true。所以，当你往MySQL中插入true或false的时候，MySQL会自动转换成1或0进行存储。因此，当你从MySQL表读取true和false的时候，其实读取的是1和0，那么写入到Hive之后也是1和0。</p><p><font color="red"><strong>解决方法</strong></font>：在jdbc url中配置参数tinyInt1isBit=false，如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">url=jdbc:mysql:<span class="comment">//hostname:3306/dbname?tinyInt1isBit=false</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Connector连接器之整合读写HBase</title>
      <link href="2020/10/11/Spark-Connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99HBase/"/>
      <url>2020/10/11/Spark-Connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99HBase/</url>
      
        <content type="html"><![CDATA[<p>Spark以不同的方式读写HBase。</p><a id="more"></a><h1 id="Spark读HBase"><a href="#Spark读HBase" class="headerlink" title="Spark读HBase"></a>Spark读HBase</h1><h2 id="1-使用newAPIHadoopRDD-API"><a href="#1-使用newAPIHadoopRDD-API" class="headerlink" title="1. 使用newAPIHadoopRDD API"></a>1. 使用newAPIHadoopRDD API</h2><p>代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Result</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableInputFormat</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.HBaseConfiguration</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;Row, SparkSession&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;IntegerType, StringType, StructField, StructType&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function">def <span class="title">readHBase</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val config = HBaseConfiguration.create()</span><br><span class="line">    config.set(TableInputFormat.INPUT_TABLE, <span class="string">&quot;test&quot;</span>) <span class="comment">//表名</span></span><br><span class="line">    config.set(TableInputFormat.SCAN_ROW_START, <span class="string">&quot;start_key&quot;</span>) <span class="comment">//扫描起始rowKey</span></span><br><span class="line">    config.set(TableInputFormat.SCAN_ROW_STOP, <span class="string">&quot;stop_key&quot;</span>) <span class="comment">//扫描终止rowKey</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//HBase表加载为RDD[(K, V)]</span></span><br><span class="line">    val rdd = spark.sparkContext.newAPIHadoopRDD(config,</span><br><span class="line">        classOf[TableInputFormat],</span><br><span class="line">        classOf[ImmutableBytesWritable],</span><br><span class="line">        classOf[Result]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//从Result中获取指定列最新版本的值</span></span><br><span class="line">    val rdd1 = rdd.map(m =&gt; &#123;</span><br><span class="line">        <span class="comment">//获取一行查询结果</span></span><br><span class="line">        val result: Result = m._2</span><br><span class="line"></span><br><span class="line">        val rowKey = Bytes.toString(result.getRow) <span class="comment">//获取row key</span></span><br><span class="line">        val userId = Bytes.toString(result.getValue(<span class="string">&quot;cf&quot;</span>.getBytes,<span class="string">&quot;user_id&quot;</span>.getBytes))</span><br><span class="line">        val name = Bytes.toString(result.getValue(<span class="string">&quot;cf&quot;</span>.getBytes,<span class="string">&quot;name&quot;</span>.getBytes))</span><br><span class="line">        val age = Bytes.toString(result.getValue(<span class="string">&quot;cf&quot;</span>.getBytes,<span class="string">&quot;age&quot;</span>.getBytes))</span><br><span class="line"></span><br><span class="line">        Row(rowKey, userId, name, age)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建schema</span></span><br><span class="line">    val schema = StructType(</span><br><span class="line">      StructField(<span class="string">&quot;user_id&quot;</span>, IntegerType, <span class="keyword">false</span>) ::</span><br><span class="line">      StructField(<span class="string">&quot;name&quot;</span>, StringType, <span class="keyword">false</span>) ::</span><br><span class="line">      StructField(<span class="string">&quot;age&quot;</span>, IntegerType, <span class="keyword">true</span>) :: Nil)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//RDD转为DataFrame</span></span><br><span class="line">    val df = spark.createDataFrame(rdd1, schema)</span><br><span class="line">    </span><br><span class="line">    df.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Spark写HBase"><a href="#Spark写HBase" class="headerlink" title="Spark写HBase"></a>Spark写HBase</h1><h2 id="1-saveAsNewAPIHadoopFile-API"><a href="#1-saveAsNewAPIHadoopFile-API" class="headerlink" title="1. saveAsNewAPIHadoopFile API"></a>1. saveAsNewAPIHadoopFile API</h2><p>实现代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.&#123;HFileOutputFormat2, TableInputFormat&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.&#123;HBaseConfiguration, KeyValue&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;Dataset, SparkSession&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line"><span class="function">def <span class="title">writeHBase</span><span class="params">(spark: SparkSession, datasetJson: Dataset[String])</span>: Unit </span>= &#123;</span><br><span class="line">    \<span class="comment">//假设datasetJson是一个json字符串类型的Dataset, 其结构为&quot;&#123;&quot;name&quot;:&quot;&quot;, &quot;age&quot;:&quot;&quot;, &quot;phone&quot;:&quot;&quot;, &quot;address&quot;:&quot;&quot;, &#125;&quot;</span></span><br><span class="line">    val ds = spark.read.json(datasetJson)</span><br><span class="line">    </span><br><span class="line">    val rdd = ds.rdd.mapPartitions(iterator =&gt; &#123;</span><br><span class="line">        iterator.map(m =&gt; &#123;</span><br><span class="line">            val json = JSON.parseObject(m.toString()) <span class="comment">//json字符串解析成JSONObject</span></span><br><span class="line">            val phone = json.getString(<span class="string">&quot;phone&quot;</span>) <span class="comment">//phone作为KeyValue的row key</span></span><br><span class="line">            json.remove(<span class="string">&quot;phone&quot;</span>) <span class="comment">//以便遍历其他所有键值对</span></span><br><span class="line"></span><br><span class="line">            val writable = <span class="keyword">new</span> ImmutableBytesWritable(Bytes.toBytes(phone)) <span class="comment">//键值对字节序列</span></span><br><span class="line">            val array = ArrayBuffer[(ImmutableBytesWritable, KeyValue)]() <span class="comment">//初始化数组, 存储JSONObject中的键值对</span></span><br><span class="line">            </span><br><span class="line"><span class="comment">//JSON中的key作为Hbase表中的列名，并按字典序排序</span></span><br><span class="line">            val jsonKeys = json.keySet().toArray.map(_.toString).sortBy(x =&gt; x) </span><br><span class="line"></span><br><span class="line">            val length = jsonKeys.length</span><br><span class="line">            <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until length) &#123;</span><br><span class="line">                val key = jsonKeys(i)</span><br><span class="line">                val value = json.get(jsonKeys(i)).toString</span><br><span class="line">                <span class="comment">//KeyValue为HBase中的基本类型Key/Value。</span></span><br><span class="line">                <span class="comment">//构造函数中的参数依次为：rowkey、列族、列名、值。</span></span><br><span class="line">                <span class="comment">//Json对象中的每个key和其值构成HBase中每条记录的value</span></span><br><span class="line">                val keyValue: KeyValue = <span class="keyword">new</span> KeyValue(</span><br><span class="line">                    Bytes.toBytes(phone),  <span class="comment">//row key</span></span><br><span class="line">                    <span class="string">&quot;cf&quot;</span>.getBytes(), <span class="comment">//列族名</span></span><br><span class="line">                    key.getBytes(), <span class="comment">//列名</span></span><br><span class="line">                    value.getBytes()) <span class="comment">//列的值</span></span><br><span class="line"></span><br><span class="line">                array += ((writable, keyValue))</span><br><span class="line">            &#125;</span><br><span class="line">            array</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">//重新分区，减少保存的文件数</span></span><br><span class="line">        <span class="comment">//展开数组中的元素</span></span><br><span class="line">        <span class="comment">//对rowkey排序</span></span><br><span class="line">    &#125;).repartition(<span class="number">1</span>).flatMap(x =&gt; x).sortByKey()</span><br><span class="line"></span><br><span class="line">    val config = HBaseConfiguration.create()</span><br><span class="line">    config.set(TableInputFormat.INPUT_TABLE, <span class="string">&quot;test&quot;</span>) <span class="comment">//表名</span></span><br><span class="line"></span><br><span class="line">    val job = Job.getInstance(config)</span><br><span class="line">    job.setMapOutputKeyClass(classOf[ImmutableBytesWritable])</span><br><span class="line">    job.setMapOutputValueClass(classOf[KeyValue])</span><br><span class="line"></span><br><span class="line">    <span class="comment">//持久化到HBase表</span></span><br><span class="line">    rdd.saveAsNewAPIHadoopFile(<span class="string">&quot;/tmp/test&quot;</span>,</span><br><span class="line">        classOf[ImmutableBytesWritable],</span><br><span class="line">        classOf[KeyValue],</span><br><span class="line">        classOf[HFileOutputFormat2],</span><br><span class="line">        job.getConfiguration)</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-BulkLoad功能"><a href="#2-BulkLoad功能" class="headerlink" title="2. BulkLoad功能"></a>2. BulkLoad功能</h2><p>在将大批量数据离线导入HBase系统的场景中，不管是使用Put单条插入还是Put批量插入HBase，效率都不理想，还可能占用大量带宽资源，导致其他业务程序运行变慢。这个时候就可以考虑使用Bulk Load，Bulk Load使用MapReduce将待写入数据转换成HFile文件，再直接将这些HFile文件加载到集群中，BulkLoad并没有将写入请求发送给RegionServer，不会占用太多集群资源，效率极高。</p><p>BulkLoad实现代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.ConnectionFactory</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.&#123;HBaseConfiguration, TableName&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.spark.&#123;ByteArrayWrapper, FamiliesQualifiersValues, FamilyHFileWriteOptions, HBaseContext&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.spark.HBaseRDDFunctions._</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function">def <span class="title">handle</span><span class="params">(spark: SparkSession, df: DataFrame, key: String, source: String, stagingDir: String)</span>: Unit </span>= &#123;</span><br><span class="line">val sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">\<span class="comment">//使用HBase资源创建一个Configuration</span></span><br><span class="line">val config = HBaseConfiguration.create()</span><br><span class="line">\<span class="comment">//Zookeeper节点, 端口号</span></span><br><span class="line">config.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;node-3,node-4,node-5&quot;</span>)</span><br><span class="line">config.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, <span class="string">&quot;2181&quot;</span>)</span><br><span class="line"></span><br><span class="line">\<span class="comment">//初始化HbaseContext</span></span><br><span class="line">val hbaseContext = <span class="keyword">new</span> HBaseContext(sc, config)</span><br><span class="line"></span><br><span class="line">\<span class="comment">//DataFrame中的列名</span></span><br><span class="line">val columns = df.columns</span><br><span class="line"></span><br><span class="line">\<span class="comment">//HBase表</span></span><br><span class="line">val tableName = TableName.valueOf(<span class="string">&quot;hbase_table_name&quot;</span>)</span><br><span class="line">val conn = ConnectionFactory.createConnection(config)</span><br><span class="line"></span><br><span class="line">\<span class="comment">//HBase column family</span></span><br><span class="line">val columnFamily: Array[Byte] = Bytes.toBytes(<span class="string">&quot;cf&quot;</span>)</span><br><span class="line"></span><br><span class="line">\<span class="comment">//HBase bulk load对于那些少于1000个列短row的Spark实现</span></span><br><span class="line">df.rdd.hbaseBulkLoadThinRows(</span><br><span class="line">hbaseContext,</span><br><span class="line">tableName, \<span class="comment">//要加载进数据的表</span></span><br><span class="line">t =&gt; &#123; \<span class="comment">//一个要将RDD records转化成key value格式的函数</span></span><br><span class="line">\<span class="comment">//rowKey</span></span><br><span class="line">val rowKey = t.getAs(key).toString</span><br><span class="line"></span><br><span class="line">\<span class="comment">//保存并排序将要bulk load到单个row中的所有cells</span></span><br><span class="line">val familyQualifiersValues = <span class="keyword">new</span> FamiliesQualifiersValues</span><br><span class="line"></span><br><span class="line">\<span class="comment">//遍历所有列</span></span><br><span class="line">columns.foreach(c =&gt; &#123;</span><br><span class="line">val qualifier = Bytes.toBytes(c) \<span class="comment">//HBase column qualifier</span></span><br><span class="line">val value: Array[Byte] = Bytes.toBytes(t.getAs(c).toString) \<span class="comment">//HBase cell value</span></span><br><span class="line"></span><br><span class="line">\<span class="comment">//添加一个新的cell到一个已存在的row</span></span><br><span class="line">familyQualifiersValues.+=(columnFamily, qualifier, value)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">\<span class="comment">//添加字段(字段表示数据来源)</span></span><br><span class="line">familyQualifiersValues.+=(columnFamily, Bytes.toBytes(<span class="string">&quot;source&quot;</span>), Bytes.toBytes(source))</span><br><span class="line"></span><br><span class="line">(<span class="keyword">new</span> ByteArrayWrapper(Bytes.toBytes(rowKey)), familyQualifiersValues)</span><br><span class="line">&#125;,</span><br><span class="line">stagingDir, \<span class="comment">//要load到文件系统的路径</span></span><br><span class="line"><span class="keyword">new</span> util.HashMap[Array[Byte], FamilyHFileWriteOptions], \<span class="comment">//一个选项, 可以设置列族写入HFile的方式(compression, bloomType, blockSize, dataBlockEncoding)</span></span><br><span class="line">compactionExclude = <span class="keyword">false</span> \<span class="comment">//Compaction excluded for the HFiles</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">\<span class="comment">//加载HFileOutputFormat的输出到一个已存在的表</span></span><br><span class="line">val load = <span class="keyword">new</span> LoadIncrementalHFiles(config)</span><br><span class="line"></span><br><span class="line">\<span class="comment">//将指定目录下的数据load到指定的表</span></span><br><span class="line">load.doBulkLoad(<span class="keyword">new</span> Path(stagingDir),</span><br><span class="line">conn.getAdmin,</span><br><span class="line">conn.getTable(tableName),</span><br><span class="line">conn.getRegionLocator(tableName)</span><br><span class="line">)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="依赖包"><a href="#依赖包" class="headerlink" title="依赖包"></a>依赖包</h1><p>spark-submit提交任务时需要依赖的第三方jar包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fastjson-1.2.62.jar</span><br><span class="line">hbase-protocol-1.2.0.jar</span><br><span class="line">hbase-common-1.2.0.jar</span><br><span class="line">hbase-spark-1.2.0-cdh5.16.2.jar</span><br><span class="line">hbase-client-1.2.0.jar</span><br><span class="line">hbase-server-1.2.0.jar</span><br><span class="line">htrace-core-3.1.0-incubating.jar</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark内存管理(静态内存管理和统一内存管理)</title>
      <link href="2020/10/11/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86-%E9%9D%99%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%92%8C%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
      <url>2020/10/11/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86-%E9%9D%99%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%92%8C%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>Spark1.6.0版本之前提供的是静态内存管理，实现类<strong>StaticMemoryManager</strong>；<br>Spark1.6.0版本及之后默认的是统一内存管理，实现类是<strong>UnifiedMemoryManager</strong>。</p><p>不管是静态内存管理还是统一内存管理，Spark内存都是被划分为执行内存区域(execution)和存储内存区域(storage)。execution内存主要用在shuffle、join、sort和aggregation的计算，而storage内存用来缓存和传输集群内部数据。</p><a id="more"></a><h2 id="1-StaticMemoryManager-静态内存管理"><a href="#1-StaticMemoryManager-静态内存管理" class="headerlink" title="1. StaticMemoryManager(静态内存管理)"></a>1. StaticMemoryManager(静态内存管理)</h2><p>在静态内存管理模型中，<font color=blue>Execution和Storage区域都是固定的</font>。Execution区域和Storage区域的大小分别是由’spark.shuffle.memoryFraction’<br>和’spark.storage.memoryFraction’来决定的。这两个区域被清晰的分隔开，任何一方都不能借用另一个区域的内存。</p><p>Storage区域内存划分的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Storage区域可用的总内存</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">private</span> def <span class="title">getMaxStorageMemory</span><span class="params">(conf: SparkConf)</span>: Long </span>= &#123;</span><br><span class="line"> <span class="comment">//生产环境一般不会使用参数&quot;spark.testing.memory&quot;, 那么返回的就是JVM最大内存.</span></span><br><span class="line">   val systemMaxMemory = conf.getLong(<span class="string">&quot;spark.testing.memory&quot;</span>, Runtime.getRuntime.maxMemory)</span><br><span class="line"></span><br><span class="line"><span class="comment">//分给spark内存缓存的内存比率</span></span><br><span class="line">   val memoryFraction = conf.getDouble(<span class="string">&quot;spark.storage.memoryFraction&quot;</span>, <span class="number">0.6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//安全系数</span></span><br><span class="line">   val safetyFraction = conf.getDouble(<span class="string">&quot;spark.storage.safetyFraction&quot;</span>, <span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//最终可分给spark内存缓存的内存大小</span></span><br><span class="line">   (systemMaxMemory * memoryFraction * safetyFraction).toLong</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>Execution区域内存划分的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Execution区域可用的总内存</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> <span class="function"><span class="keyword">private</span> def <span class="title">getMaxExecutionMemory</span><span class="params">(conf: SparkConf)</span>: Long </span>= &#123;</span><br><span class="line"> <span class="comment">//生产环境一般不会使用参数&quot;spark.testing.memory&quot;, 那么返回的就是JVM最大内存.</span></span><br><span class="line">   val systemMaxMemory = conf.getLong(<span class="string">&quot;spark.testing.memory&quot;</span>, Runtime.getRuntime.maxMemory)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"><span class="comment">//分配给shuffle操作(aggregation、cogroups等)的内存比率</span></span><br><span class="line">   val memoryFraction = conf.getDouble(<span class="string">&quot;spark.shuffle.memoryFraction&quot;</span>, <span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//内存安全系数</span></span><br><span class="line">   val safetyFraction = conf.getDouble(<span class="string">&quot;spark.shuffle.safetyFraction&quot;</span>, <span class="number">0.8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//分配给shuffle操作的内存大小</span></span><br><span class="line">   (systemMaxMemory * memoryFraction * safetyFraction).toLong</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>如果spark-submit提交中每个executor的JVM Heap内存为10G(也就是，spark.executor.memory=10G)，那么能分配给Storage区域的内存为10G * 0.6 * 0.9 = 5.4G，分配给Execution区域的内存大小为10G * 0.2 * 0.8 = 1.6G。</p><h2 id="2-UnifiedMemoryManager-统一内存管理"><a href="#2-UnifiedMemoryManager-统一内存管理" class="headerlink" title="2. UnifiedMemoryManager(统一内存管理)"></a>2. UnifiedMemoryManager(统一内存管理)</h2><p>在统一内存管理模型中，Execution区域和Storage区域间的边界线并不是固定死的，可以认为是一个可以滑动的边界线。简单的说就是，<font color=blue>两个区域可以相互借用对方的内存</font>。</p><p>Execution和Storage所共享区域的内存大小由参数’spark.memory.fraction’决定(默认是0.6，即(总的堆大小 - 300M) * 0.6)。*在这个共享区域中，Storage占用的内存大小由’spark.memory.storageFraction’决定(默认是0.5)，也就是说，Storage区域默认为堆空间的0.6 * 0.5 = 0.3。<font color=blue>默认情况下，Execution和Storage两个区域的内存大小是相等的</font>。</p><p>在Execution区域要使用属于自己那部分内存之前，Storage区域可以借用尽可能多的Execution内存。当Execution区域要拿回属于自己的那部分内存时，将会驱逐出这部分内存中缓存的blocks。</p><p>类似地，Execution区域也可以借用尽可能多空闲的Storage 内存。但是，如果Execution区域占用了Storage区域的内存，Storage区域要拿回属于自己的那部分内存是不被允许的，这主要是因为驱逐Execution区域所占用的那部分内存的实现逻辑非常复杂。如果Execution区域占用了大部分Storage内存，新的缓存blocks将会根据对应的storage level进行不同的处理。</p><p>Execution和Storage的共享区域内存划分源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">//预留给系统的内存(300M).</span></span><br><span class="line"> <span class="keyword">private</span> val RESERVED_SYSTEM_MEMORY_BYTES = <span class="number">300</span> * <span class="number">1024</span> * <span class="number">1024</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Execution区域和Storage区域可用的总内存.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">private</span> def <span class="title">getMaxMemory</span><span class="params">(conf: SparkConf)</span>: Long </span>= &#123;</span><br><span class="line"> <span class="comment">//生产环境一般不会使用参数&quot;spark.testing.memory&quot;, 那么返回的就是JVM最大内存.</span></span><br><span class="line">   val systemMemory = conf.getLong(<span class="string">&quot;spark.testing.memory&quot;</span>, Runtime.getRuntime.maxMemory)</span><br><span class="line"></span><br><span class="line"><span class="comment">//系统预留内存</span></span><br><span class="line">   val reservedMemory = conf.getLong(<span class="string">&quot;spark.testing.reservedMemory&quot;</span>,</span><br><span class="line">     <span class="keyword">if</span> (conf.contains(<span class="string">&quot;spark.testing&quot;</span>)) <span class="number">0</span> <span class="keyword">else</span> RESERVED_SYSTEM_MEMORY_BYTES)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 最小系统内存限制</span></span><br><span class="line">   val minSystemMemory = (reservedMemory * <span class="number">1.5</span>).ceil.toLong</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//系统内存systemMemory不能小于minSystemMemory</span></span><br><span class="line">   <span class="keyword">if</span> (systemMemory &lt; minSystemMemory) &#123;</span><br><span class="line">     <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(s<span class="string">&quot;System memory $systemMemory must &quot;</span> +</span><br><span class="line">       s<span class="string">&quot;be at least $minSystemMemory. Please increase heap size using the --driver-memory &quot;</span> +</span><br><span class="line">       s<span class="string">&quot;option or spark.driver.memory in Spark configuration.&quot;</span>)</span><br><span class="line">   &#125;</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//参数&quot;spark.executor.memory&quot;指定的是JVM Heap内存, 不能小于minSystemMemory</span></span><br><span class="line">   <span class="keyword">if</span> (conf.contains(<span class="string">&quot;spark.executor.memory&quot;</span>)) &#123;</span><br><span class="line">     val executorMemory = conf.getSizeAsBytes(<span class="string">&quot;spark.executor.memory&quot;</span>)</span><br><span class="line">     <span class="keyword">if</span> (executorMemory &lt; minSystemMemory) &#123;</span><br><span class="line">       <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(s<span class="string">&quot;Executor memory $executorMemory must be at least &quot;</span> +</span><br><span class="line">         s<span class="string">&quot;$minSystemMemory. Please increase executor memory using the &quot;</span> +</span><br><span class="line">         s<span class="string">&quot;--executor-memory option or spark.executor.memory in Spark configuration.&quot;</span>)</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="comment">//可用内存</span></span><br><span class="line">   val usableMemory = systemMemory - reservedMemory</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//Execution区域和Storage区域占usableMemory的比率</span></span><br><span class="line">   val memoryFraction = conf.getDouble(<span class="string">&quot;spark.memory.fraction&quot;</span>, <span class="number">0.6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Execution区域和Storage区域可使用的总内存大小</span></span><br><span class="line">   (usableMemory * memoryFraction).toLong</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h2 id="3-OFF-HEAP-非堆内存管理"><a href="#3-OFF-HEAP-非堆内存管理" class="headerlink" title="3. OFF_HEAP(非堆内存管理)"></a>3. OFF_HEAP(非堆内存管理)</h2><p>默认情况下，Spark是不使用非堆内存的，如果要开启非堆内存使用，需要设置参数”spark.memory.offHeap.enabled”=true，另外还要设置”spark.memory.offHeap.size”(单位为bytes)来指定要使用的非堆内存的大小。</p><p>StaticMemoryManager支持Execution区域使用非堆内存，不支持Storage区域使用非堆内存的。</p><p>UnifiedMemoryManager即支持Execution区域使用非堆内存，也支持Storage区域使用非堆内存的。</p><p>参考</p><ol><li><a href="https://github.com/apache/spark/blob/branch-1.6/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala">https://github.com/apache/spark/blob/branch-1.6/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala</a></li><li><a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala">https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark内存管理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark性能优化指南(官网文档)</title>
      <link href="2020/10/11/Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97-%E5%AE%98%E7%BD%91%E6%96%87%E6%A1%A3/"/>
      <url>2020/10/11/Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97-%E5%AE%98%E7%BD%91%E6%96%87%E6%A1%A3/</url>
      
        <content type="html"><![CDATA[<p>本篇文章翻译之 <a href="https://spark.apache.org/docs/2.3.1/tuning.html">Tuning Spark</a>。</p><p>由于大多数Spark计算基于内存的特性，Spark程序可能会因为集群中的任何资源而导致出现瓶颈：CPU、网络带宽或内存。通常情况下，如果数据适合于放到内存中，那么瓶颈就是网络带宽，但有时，你还是需要一些调优的，比如以序列化的形式保存RDDs，以便减少内存占用。这篇调优指南主要涵盖两个主题：数据序列化和内存调优。数据序列化对于良好的网络性能是至关重要的，而且还可以减少内存的使用。</p><a id="more"></a><h3 id="数据序列化-Data-Serialization"><a href="#数据序列化-Data-Serialization" class="headerlink" title="数据序列化 - Data Serialization"></a>数据序列化 - Data Serialization</h3><p>序列化在任何的分布式应用中都扮演着重要的角色。将对象序列化的比较慢的格式，或者耗费大量字节的格式，都会大大降低计算速度。通常，这应该是您在优化Spark应用时首先要考虑的事情。Spark旨在在便利性(允许你使用任何Java类型)和性能之间取得平衡。它提供了连个序列化库：</p><ul><li>Java serialization：默认情况下，Spark使用Java的ObjectOutputStream框架来序列化对象，而且可以使用任何你通过实现java.io.Serializable来创建的类。你还可以通过继承java.io.Externalizable来控制序列化的性能。Java序列化是灵活的，但通常很慢，而且对于很多类会导致大的序列化格式。</li><li>Kryo serialization：Spark也可以使用Kryo库(version 4)来更快的序列化对象。Kryo明显要比Java序列化更快，更紧凑，但不支持所有序列化类型，并且要求你提前注册你将在程序中使用的类，以获得最佳性能。</li></ul><p>您可以通过使用SparkConf初始化job，并调用conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”)来切换到使用Kryo。这个设置配置的序列化器不仅可以用于worker节点之间的shuffle数据，还可以用于序列化到磁盘的RDDs。Kryo不是默认值的唯一原因是因为其要自定义注册，但是我们建议在任何网络密集型应用中尝试使用它。从Spark2.0.0开始，我们在基于基本数据类型、基本数据类型或字符串类型的数组来shuffle RDDs时，使用Kyro序列化器。</p><p>Spark对于包含在AllScalaRegistrar(Twitter chill library)中的常用核心Scala类，都自动包含了Kryo序列化器。</p><p>使用registerKryoClasses方法，向Kryo注册您自己的自定义类。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val conf &#x3D; new SparkConf().setMaster(...).setAppName(...)</span><br><span class="line">conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))</span><br><span class="line">val sc &#x3D; new SparkContext(conf)</span><br></pre></td></tr></table></figure><p><a href="https://github.com/EsotericSoftware/kryo">Kryo Document</a>描述了更高级的注册选项，比如添加自定义的序列化代码。</p><p>如果你的对象很大，你可能需要增加配置(spark.kryoserializer.buffer)的值。这个值要足够大到能容纳你要序列化的最大的对象。</p><p>最后，如果你没有注册你的自定义类，Kryo将仍然生效，但是它将不得不存储每个对象的完整类名，那将会非常浪费。</p><h3 id="内存调优-Memory-Tuning"><a href="#内存调优-Memory-Tuning" class="headerlink" title="内存调优 - Memory Tuning"></a>内存调优 - Memory Tuning</h3><p>调优内存使用时需要考虑三个因素：对象占用的内存数(你可能想要将整个dataset放到内存中)，访问这些对象的成本以及垃圾收集的开销。</p><p>默认情况下，Java对象访问速度很快，但是很容易的比对象中存储的数据多消耗2~5倍的空间。这时由于以下几个原因：</p><ul><li>不同的Java对象都有一个”对象头”，大约是16个字节，并包含指向其类的指针等信息。对于一个只有很少数据的对象(比如一个Int字段)，对象头可能会比数据更大。</li><li>Java 字符串在其原始数据上大约有40个字节的开销(因为它们是将原始数据保存在字符数组中的，并且保存长度等额外的数据)，由于字符串内部使用UTF-16编码，所以每个字符都存储为两个字节。因此，一个10字符的字符串可以很容易的消耗60个字节。</li><li>通用集合类，如HashMap和LinkedList，使用链式数据结构，其中每个条目(例如Map.Entry)都有一个”wrapper”对象。这个对象不仅有对象头，还有指向列表中下一个对象的指针(通常每个指针8个字节)。</li><li>基本数据类型的集合通常将它们存储为装箱对象，如java.lang.Integer。</li></ul><p>本节将首先概述Spark的内存管理，然后讨论用户可以采取的具体策略，，以便更有效地使用应用程序中的内存。我们将描述如何确定对象的内存使用，以及如何改进内存使用——通过改变数据结构，或以序列化格式存储数据。然后，我们将概括调优Spark的缓存大小和Java垃圾收集器。</p><h3 id="内存管理-Memory-Management-Overview"><a href="#内存管理-Memory-Management-Overview" class="headerlink" title="内存管理 - Memory Management Overview"></a>内存管理 - Memory Management Overview</h3><p>Spark中的内存使用主要分为两类：execution和storage。Execution memory指的是，在shuffle、join、sort和aggregation中用于计算的内存，而storage memory指的是用来在集群中缓存和传输内部数据的内存。Spark中，execution和storage共享一个统一的区域(M)。当没有execution memory被使用时，storage可以获取所有可用内存，反之，如果没有storage memory被使用时，execution也可以获取所有可用的内存。如果在Execution storage不够用时，可驱逐storage区域占用Execution区域的一部分内存，但仅在总的storage memory占用低于某个阈值(R)之前才会这么做。换句话说，R是M中的一个子区域，是在默认情况下分配给storage的内存，阈值R内缓存的块是永远不会被驱逐的。</p><p>这种设计确保了几个想要的特性。首先，不使用缓存的应用程序可以拿整个内存空间给execution用，从而避免不必要的磁盘溢出。其次，如果应用程序确实要使用缓存，可以保留一个最小的storage空间(R)，这里的数据块不会被驱逐。</p><p>虽然有两个相关的配置，但由于默认值已适用于大多数情况，一般用户是不需要调整这两个参数的：</p><ul><li><p>spark.memory.fraction代表统一共享区域M占Java堆内存-300MB的比例(默认是0.6)。剩余40%的空间是留给用户数据结构、Spark内部元数据和防止OMM用的。</p></li><li><p>spark.memory.storageFraction代表R区域占M区域的比例(默认是0.5)。R中的缓存块时不会被Execution驱逐的。</p><p>spark.memory.fraction的值应满足JVM老年代的堆空间大小。有关详细信息，请参考下面关于高级GC调优的讨论。</p></li></ul><h3 id="确定内存占用-Determining-Memory-Consumption"><a href="#确定内存占用-Determining-Memory-Consumption" class="headerlink" title="确定内存占用 - Determining Memory Consumption"></a>确定内存占用 - Determining Memory Consumption</h3><p>衡量一个dataset所需内存的最好的方法就是创建一个RDD，将其放入缓存中，然后到web UI中查看”Storage”页面。这个页面会告诉你，这个RDD占用了多少内存。</p><p>要估计一个特定对象的内存占用，可以使用SizeEstimator的estimate方法，这对于尝试用不同的数据设计来调整内存使用是非常有用的，还可以确定广播变量在每个executor上占的堆大小。</p><h3 id="数据结构调优-Tuning-Data-Structures"><a href="#数据结构调优-Tuning-Data-Structures" class="headerlink" title="数据结构调优 - Tuning Data Structures"></a>数据结构调优 - Tuning Data Structures</h3><p>减少内存消耗的第一种方法是，避免那些会增加开销的Java特性，比如基于指针的数据结构和包装对象。有几种方式可以做到这一点：</p><ol><li><p>设计你的数据结构以优先选择对象数组和基本类型，而不是标准的Java或Scala集合类型(比如HashMap)。fastutil库为与Java标准库兼容的基本类型提供了方便的集合类。</p></li><li><p>尽可能避免使用包含大量小对象和指针的嵌套结构。</p></li><li><p>对于主键字段，考虑使用数字类型的ID或枚举对象来代替字符串。</p></li><li><p>如果内存少于32GB，可以设置JVM参数-XX:+UseCompressedOops，来使指针由8个字节变为4个字节。您可以在spark-env.sh中添加这个选项。</p></li><li><h3 id="序列化RDD存储-Serialized-RDD-Storage"><a href="#序列化RDD存储-Serialized-RDD-Storage" class="headerlink" title="序列化RDD存储 - Serialized RDD Storage"></a>序列化RDD存储 - Serialized RDD Storage</h3><p>当进行了调优之后，对象太大还是无法有效地存储时，一个更简单的减少内存占用的方式就是使用RDD持久化API中的序列化存储级别(比如MEMORY_ONLY_SER)以序列化形式存储对象。Spark将每个RDD分区存储为一个大的字节数组。以序列化形式存储数据的唯一缺点就是访问时间慢，由于必须动态地反序列化对个对象。我们强烈建议您使用Kryo，如果您想以序列化的形式缓存数据，因为它比Java序列化占用小的多的空间。</p></li></ol><h3 id="垃圾收集调优-Garbage-Cllection-Tuning"><a href="#垃圾收集调优-Garbage-Cllection-Tuning" class="headerlink" title="垃圾收集调优 - Garbage Cllection Tuning"></a>垃圾收集调优 - Garbage Cllection Tuning</h3><p>当您的应用程序存储了大量的RDD时，JVM垃圾收集可能会成为问题。当Java需要驱逐旧对象来为新对象腾出空间时，它将跟踪所有Java对象，并找到未使用的对象。这里要记住的要点是，垃圾收集的成本与Java对象的数据成正比，使用更小对象的数据结构(比如，用int类型的数组代替LinkedList)可以大大降低垃圾收集的成本。一个更好的方法是以序列化的形式持久化对象，如上所述：现在每个RDD分区只有一个对象(一个字节数组)。如果存在GC问题，在尝试使用其他技术之前，首先要尝试使用序列化缓存。</p><p>由于任务工作内存(运行task所需的内存空间)和缓存在节点上的RDD之间存在冲突，也可能会导致GC问题。我们将讨论如何控制分配给RDD的缓存空间来缓解这种问题。</p><h6 id="衡量GC影响-Measuring-the-Impact-of-GC"><a href="#衡量GC影响-Measuring-the-Impact-of-GC" class="headerlink" title="衡量GC影响 - Measuring the Impact of GC"></a>衡量GC影响 - Measuring the Impact of GC</h6><p>GC调优的第一步是收集统计垃圾收集的频率和GC所耗费的时间。这可以通过添加Java gc选项-XX:+PrintGCDetails和-XX:+PrintGCTimeStamps来实现。(有关给Spark job传递Java选项的信息，请查看<a href="http://spark.apache.org/docs/2.3.1/configuration.html#Dynamically-Loading-Spark-Properties">configuration guide</a>)。在下次Spark job运行时，您将在发生垃圾收集时看到被打印到work检点上的日志信息。注意，这些GC日志是打印在集群的worker节点而不是driver节点。</p><h6 id="高级GC调优策略-Advanced-GC-Tuning"><a href="#高级GC调优策略-Advanced-GC-Tuning" class="headerlink" title="高级GC调优策略 - Advanced GC Tuning"></a>高级GC调优策略 - Advanced GC Tuning</h6><p>为了更进一步地调优垃圾收集，我们首先需要了解一些关于JVM内存管理的基本信息：</p><ul><li>Java堆空间被划分为年轻代和年老代两个区域。年轻代用来保存存活时间短的对象，而年老代保存寿命更长的对象。</li><li>年轻代被进一步划分成Eden，Survivor1和Survivor2三个区域。</li><li>垃圾收集过程的简单描述：当Eden空间已满时，会在Eden空间触发一次minor GC，然后将Eden和Survivor1中仍然存活的对象复制到Survivor2区域。如果一个对象达到了所设定的最大年龄或者Survivor2区满了，就会将对象移动到年老代。最终，当年老代空间快要满了时，将会触发一次full GC。</li></ul><p>Spark中进行GC调优的目标是确保只有存活时间长的RDD存储在年老代，年轻代足以存储存活时间短的对象。这将有助于避免full GC去收集任务执行期间创建的临时对象。下面是一些有用的GC调优方法：</p><ul><li><p>通过收集GC统计信息来检查是否有太多的垃圾收集发生。如果在一个task执行完成之前，触发了多次full GC，这意味着没有足够的内存可用来执行tasks。</p></li><li><p>如果触发了太多的minor GC，而没有太多major GC，那么为Eden区分配更多内存将会有所帮助。您可以将Eden区的大小设置为高于每个task预估所占用的内存。如果Eden区的大小被确定为E，那么可以使用选项-Xmn=4/3*E来这是年轻代的大小。</p></li><li><p>在打印的GC统计信息中，如果发现年老代将要满了，则通过降低spark.memory.fraction来减少用于缓存的内存占用；缓存更少的对象比降低task的执行速度要更好。或者，考虑减少年轻代的大小。如果你已经设置了-Xmn的值，这意味着降低它的大小。如果没有设置-Xmn的值，尝试盖面JVM的NewRatio参数的值，许多JVM将这个参数的默认值设为2，这表明年老代占整个堆空间的2/3，它应该足够大，以超过spark.memory.fraction的值。</p></li><li><p>尝试使用G1GC垃圾收集器-XX:+UseG1GC。它可以在垃圾收集成为瓶颈的情况下提高性能。注意，对于那些堆内存大的executor来说，增加G1 的region size(-XX:G1HeapRegionSize)可能很重要。</p></li><li><p>举个例子，如果您的task是从HDFS读取数据，那么就可以使用从HDFS读取数据的block大小来估计这个task所使用的内存。需要注意的是，block解压缩之后的大小通常是原来的2或3倍。因此，如果我们希望有3或4个task的工作空间，并且HDFS block大小为128MB，我们就可以估算Eden区大小为4<em>3</em>128。</p></li><li><p>监视垃圾收集的频率和时间如何随着设置的变化而变化。</p><p>我们的经验表明，GC调优的效果取决于你的应用程序和可用内存的大小。网上有许多调优选项，但是管理full GC发生的频率有助于减少开销。</p></li></ul><h3 id="其他优化技巧-Other-Considerations"><a href="#其他优化技巧-Other-Considerations" class="headerlink" title="其他优化技巧 - Other Considerations"></a>其他优化技巧 - Other Considerations</h3><h6 id="任务并行度-Level-of-Parallelism"><a href="#任务并行度-Level-of-Parallelism" class="headerlink" title="任务并行度 - Level of Parallelism"></a>任务并行度 - Level of Parallelism</h6><p>除非为每个操作设置足够高的并行度，否则集群资源不会得到充分利用。Spark根据每个文件的大小自动设置要在每个文件上运行的map task的数量。对于分布式的reduce操作，例如groupByKey和reduceByKey，它使用最大的父RDD的分区数。你可以将并行度作为第二个参数传递，或设置属性spark.default.parallelism来更改默认值。通常，我们建议集群中每个CPU xore执行2-3个task。</p><h6 id="reduce端task内存占用-Memory-Usage-of-Reduce-Tasks"><a href="#reduce端task内存占用-Memory-Usage-of-Reduce-Tasks" class="headerlink" title="reduce端task内存占用 - Memory Usage of Reduce Tasks"></a>reduce端task内存占用 - Memory Usage of Reduce Tasks</h6><p>有时候，您的应用程序发生OOM错误并不是因为RDD无法放入内存中，而是因为其中一个task的工作集太大，例如groupByKey中的一个reduce task数据太多。Spark的shuffle操作(sortByKey，groupByKey，reduceByKey，join等)在每个task中构建了一个hash table来执行聚合分组，这通常会包含大量的数据。缓解这种情况最简单的方法就是增加并行度，这样每个task的处理的数据就会变少。Spark可以有效地支持短至200ms的task，因为它可以对许多tasks重用一个executor JVM，而且启动task成本很低，因此你可以安全将并行度增加到集群core数量以上。</p><h6 id="广播大变量-Broadcasting-Large-Variables"><a href="#广播大变量-Broadcasting-Large-Variables" class="headerlink" title="广播大变量 - Broadcasting Large Variables"></a>广播大变量 - Broadcasting Large Variables</h6><p>使用SparkContext中的广播功能可以极大地减少每个序列化task的大小和集群启动job的成本。如果你的task使用了driver端任何的大对象，可以考虑将这些对象转换为广播变量。Spark在master节点打印每个task的序列化大小，因此您可以查看来确定task是否太大，一般来说，大于20KB的task值得去优化。</p><h6 id="数据本地性-Data-Locality"><a href="#数据本地性-Data-Locality" class="headerlink" title="数据本地性 - Data Locality"></a>数据本地性 - Data Locality</h6><p>数据所在的位置对Spark作业的性能有很大的影响。如果数据和要处理数据的代码在同一个地方，那么计算速度往往就很快。但是，如果代码和数据不在同一个地方，那么其中一个必须移动到另外一个所在的地方。通常情况下，移动代码比移动数据要快得多，因为代码的大小要比数据小的多。Spark就是根据这种原则来进行调度的。</p><p>数据所在的位置就是指数据与处理数据的代码之间的距离。根据数据当前的位置，有几个级别的距离，按顺序从最近到最远：</p><ul><li>PROCESS_LOCAL  数据和运行代码位于同一个JVM中。这是最好的情况。</li><li>NODE_LOCAL 数据和运行代码位于同一个节点。这会比PROCESS_LOCAL  慢一点，因为数据要在进程之间传输。</li><li>NO_PREF 从任何地方访问数据都是一样快的。</li><li>RACK_LOCAL  数据位于同一个服务器机架上。数据位于同一机架的不同服务器上，因此需要通过网络传输数据，通常是经过一个交换机。</li><li>ANY  数据位于其他机架上。</li></ul><p>Spark会优先调度task在最佳的位置级别，但这并不总是可能的。在任何空闲executor上都没有未处理的数据的情况下，Spark会切换到更低的位置级别。有两种选择：a) 等待CPU空闲下来，在同一服务器上启动一个task，或b) 立即在远端启动一个task，并要求将数据移动到那里。<br>Spark通常的策略就是，先等待一段时间，希望繁忙的CPU能得到释放，一旦超过指定时间，就开始将数据从远端移动到空闲的CPU。每个位置级别之间的超时时间都可以单独配置，也可以全部配置在一个参数中。关于spark.locality参数的详细信息，请查看<a href="http://spark.apache.org/docs/2.3.1/configuration.html#scheduling">configuration page</a>。如果您的tasks运行时间很长并且位置级别很差，那么可以增加配置的值，但是默认的设置通常就能满足多数的情况。</p><h3 id="总结-Summary"><a href="#总结-Summary" class="headerlink" title="总结 - Summary"></a>总结 - Summary</h3><p>这篇简短的调优指南指出了在调优Spark应用程序时，应该关注的主要的点——最重要的是数据序列化和内存调优。对于大多数应用程序，切换到Kryo序列化，并以序列化的形式持久化数据就能解决大多数常见的性能问题。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://spark.apache.org/docs/latest/tuning.html">Tuning Spark</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark性能优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JSON数据转成Spark Dataset/DataFrame</title>
      <link href="2020/10/11/JSON%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%88%90Spark-Dataset-DataFrame/"/>
      <url>2020/10/11/JSON%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%88%90Spark-Dataset-DataFrame/</url>
      
        <content type="html"><![CDATA[<p>在日常使用Spark处理数据时, 半结构化的JSON数据(JSONObject, JSONArray)可能是最常见的一种数据结构，那么能熟练并快速地处理JSON格式数据应该是数据开发人员的必备技能。</p><a id="more"></a><p>接下来我们就看看该如何将各种格式的JSON数据转成DataFrame。</p><h2 id="1-读取JSON文件"><a href="#1-读取JSON文件" class="headerlink" title="1. 读取JSON文件"></a>1. 读取JSON文件</h2><p>读取指定路径下的json文件（或者存放json文件的目录），加载为DataFrame。</p><h6 id="本地文件"><a href="#本地文件" class="headerlink" title="本地文件"></a>本地文件</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//本地路径</span></span><br><span class="line">val path = <span class="string">&quot;file:/opt/package/jar/bank_statement_detail.json&quot;</span></span><br><span class="line">spark.read.json(path).show()</span><br></pre></td></tr></table></figure><h6 id="HDFS文件"><a href="#HDFS文件" class="headerlink" title="HDFS文件"></a>HDFS文件</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//HDFS路径</span></span><br><span class="line">val path = <span class="string">&quot;/file/bank_statement_detail.json&quot;</span></span><br><span class="line">spark.read.json(path).show()</span><br></pre></td></tr></table></figure><h2 id="2-List、Seq"><a href="#2-List、Seq" class="headerlink" title="2. List、Seq"></a>2. List、Seq</h2><p>将一个List或Seq加载为DataFrame。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line">val ds = spark.createDataset(<span class="string">&quot;&quot;</span><span class="string">&quot;[&#123;&quot;</span>name<span class="string">&quot;: &quot;</span>Spark<span class="string">&quot;, &quot;</span>age<span class="string">&quot;: 10&#125;, &#123;&quot;</span>name<span class="string">&quot;: &quot;</span>Flink<span class="string">&quot;, &quot;</span>age<span class="string">&quot;: 5&#125;]&quot;</span><span class="string">&quot;&quot;</span> :: Nil)</span><br><span class="line">spark.read</span><br><span class="line"><span class="comment">//默认的timestampFormat为 yyyy-MM-dd&#x27;T&#x27;HH:mm:ss.SSSXXX, SSS为毫秒数, XXX为非法字符(会报错&quot;java.lang.IllegalArgumentException: Illegal pattern component: XXX&quot;),</span></span><br><span class="line"><span class="comment">//需要手动设置时区为Z(即UTC时间)</span></span><br><span class="line">.option(<span class="string">&quot;timestampFormat&quot;</span>, <span class="string">&quot;yyyy/MM/dd HH:mm:ss.SSSZ&quot;</span>)</span><br><span class="line">.json(ds)</span><br><span class="line">.show()</span><br></pre></td></tr></table></figure><h2 id="3-JSON-String类型的Column"><a href="#3-JSON-String类型的Column" class="headerlink" title="3. JSON String类型的Column"></a>3. JSON String类型的Column</h2><p>JSON String类型的Column, 拿Hive来举例的话，就是Hive表中的某一列，其类型为String，其值是JSONArray类型（”[{‘name’: ‘Spark’, ‘age’: 10}, {‘name’: ‘Flink’, ‘age’: 5}]”），或者是JSONObject类型（”{‘name’: ‘Spark’, ‘age’: 10}”）。</p><p>这种情况可以用以下两种方式处理：</p><h6 id="方式一-map函数处理"><a href="#方式一-map函数处理" class="headerlink" title="方式一(map函数处理)"></a>方式一(map函数处理)</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">//person_info为string类型的JSONArray(&quot;[&#123;&#x27;name&#x27;: &#x27;Spark&#x27;, &#x27;age&#x27;: 10&#125;, &#123;&#x27;name&#x27;: &#x27;Flink&#x27;, &#x27;age&#x27;: 5&#125;]&quot;)</span></span><br><span class="line">val df = spark.read.table(<span class="string">&quot;db_name.table_name&quot;</span>).select(<span class="string">&quot;person_info&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//JSON string类型的Dataset转成DataFrame</span></span><br><span class="line">val ds = df.map(row =&gt; JSON.parseArray(row.toString).get(<span class="number">0</span>).toString) </span><br><span class="line"></span><br><span class="line">ds.show()</span><br></pre></td></tr></table></figure><h6 id="方式二-指定schema"><a href="#方式二-指定schema" class="headerlink" title="方式二(指定schema)"></a>方式二(指定schema)</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">//person_info为string类型的JSONArray(&quot;[&#123;&#x27;name&#x27;: &#x27;Spark&#x27;, &#x27;age&#x27;: 10&#125;, &#123;&#x27;name&#x27;: &#x27;Flink&#x27;, &#x27;age&#x27;: 5&#125;]&quot;)</span></span><br><span class="line">val df = spark.read.table(<span class="string">&quot;db_name.table_name&quot;</span>).select(<span class="string">&quot;person_info&quot;</span>)</span><br><span class="line"></span><br><span class="line">val schema = ArrayType(StructType(StructField(<span class="string">&quot;name&quot;</span>, StringType):: StructField(<span class="string">&quot;age&quot;</span>, IntegerType)::Nil))</span><br><span class="line"></span><br><span class="line">val resultDF = df.select(from_json($<span class="string">&quot;person_info&quot;</span>, schema).as(<span class="string">&quot;info&quot;</span>)) <span class="comment">//将JSONArray列映射到一个schema上</span></span><br><span class="line">.select(explode($<span class="string">&quot;info&quot;</span>).as(<span class="string">&quot;info&quot;</span>)) <span class="comment">//展开JSONArray中的元素(JSONObject)为多行</span></span><br><span class="line">.select(<span class="string">&quot;info.name&quot;</span>, <span class="string">&quot;info.age&quot;</span>) <span class="comment">//获取每个JSONObject中每个key对应的value</span></span><br><span class="line"></span><br><span class="line">resultDF.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> JSON </tag>
            
            <tag> DataFrame </tag>
            
            <tag> Dataset </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Impala与Hive的语法区别(持续更新中...)</title>
      <link href="2020/10/11/Impala%E4%B8%8EHive%E7%9A%84%E8%AF%AD%E6%B3%95%E5%8C%BA%E5%88%AB-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E4%B8%AD/"/>
      <url>2020/10/11/Impala%E4%B8%8EHive%E7%9A%84%E8%AF%AD%E6%B3%95%E5%8C%BA%E5%88%AB-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E4%B8%AD/</url>
      
        <content type="html"><![CDATA[<p>Impala和Hive的区别</p><a id="more"></a><h1 id="使用上的不同"><a href="#使用上的不同" class="headerlink" title="使用上的不同"></a>使用上的不同</h1><ul><li>不支持Date类型<br>Impala中是不支持Date数据类型的</li><li>union时字段类型问题<br>Impala中两表union时，对应字段的数据类型必须一致(比如，int类型不能和空字符串””union)。而Hive中是允许的。</li><li>中文占用的长度<br>Impala中一个中文长度为3，Hive中一个中文长度为1。</li><li>不兼容的函数<br>Impala不支持date_format()、current_date()。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Impala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
            <tag> Impala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java死锁产生的条件及死锁判断排查</title>
      <link href="2020/10/11/Java%E6%AD%BB%E9%94%81%E4%BA%A7%E7%94%9F%E7%9A%84%E6%9D%A1%E4%BB%B6%E5%8F%8A%E6%AD%BB%E9%94%81%E5%88%A4%E6%96%AD%E6%8E%92%E6%9F%A5/"/>
      <url>2020/10/11/Java%E6%AD%BB%E9%94%81%E4%BA%A7%E7%94%9F%E7%9A%84%E6%9D%A1%E4%BB%B6%E5%8F%8A%E6%AD%BB%E9%94%81%E5%88%A4%E6%96%AD%E6%8E%92%E6%9F%A5/</url>
      
        <content type="html"><![CDATA[<h2 id="死锁产生的必要条件"><a href="#死锁产生的必要条件" class="headerlink" title="死锁产生的必要条件"></a>死锁产生的必要条件</h2><p>要达到死锁，必须具备以下4个条件：</p><ol><li>互斥<br>至少有一个被持有的资源处于非共享模式，如果有其他进程/线程请求这个资源，那这个进程/线程必须等待这个资源被释放之后才能获取。</li><li>持有和等待<br>一个进程/线程必须持有至少一个资源，而且同时等待一个被其他进程/线程所持有的资源。</li><li>没有抢占<br>一旦一个进程/线程持有了一个资源，那么这个资源在进程/线程自动释放它之前，都不能被拿走。</li><li>循环等到<br>必须存在一组进程/线程{P0, P1, P2，…，PN}，使得每个P[i]都在等待P[(i + 1) % (N + 1)]。</li></ol><a id="more"></a><h2 id="死锁排查"><a href="#死锁排查" class="headerlink" title="死锁排查"></a>死锁排查</h2><p>我们这里写一个会导致死锁的Java程序</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DeadLockExample</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="keyword">final</span> String s1 = <span class="string">&quot;Flink&quot;</span>;</span><br><span class="line"><span class="keyword">final</span> String s2 = <span class="string">&quot;Spark&quot;</span>;</span><br><span class="line"></span><br><span class="line">Thread t1 = <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line"><span class="keyword">synchronized</span> (s1) &#123;</span><br><span class="line">System.out.println(<span class="string">&quot;Thread 1: locked s1&quot;</span>);</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">synchronized</span> (s2) &#123;</span><br><span class="line">System.out.println(<span class="string">&quot;Thread 1: locked s2&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">Thread t2 = <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line"><span class="keyword">synchronized</span> (s2) &#123;</span><br><span class="line">System.out.println(<span class="string">&quot;Thread 2: locked s2&quot;</span>);</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">synchronized</span> (s1) &#123;</span><br><span class="line">System.out.println(<span class="string">&quot;Thread 2: locked s1&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">t1.start();</span><br><span class="line">t2.start();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>程序运行之后，我们可以通过jps命令找到程序对应的PID，之后通过jstack PID获取堆栈信息，如果程序中产生死锁，那么堆栈信息中会出现如下类似信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Java stack information <span class="keyword">for</span> the threads listed above:</span><br><span class="line">===================================================</span><br><span class="line"><span class="string">&quot;Thread-1&quot;</span>:</span><br><span class="line">        at demo.DeadLockExample<span class="variable">$2</span>.run(DeadLockExample.java:30)</span><br><span class="line">        - waiting to lock &lt;0x000000076b431a88&gt; (a java.lang.String)</span><br><span class="line">        - locked &lt;0x000000076b431ab8&gt; (a java.lang.String)</span><br><span class="line"><span class="string">&quot;Thread-0&quot;</span>:</span><br><span class="line">        at demo.DeadLockExample<span class="variable">$1</span>.run(DeadLockExample.java:16)</span><br><span class="line">        - waiting to lock &lt;0x000000076b431ab8&gt; (a java.lang.String)</span><br><span class="line">        - locked &lt;0x000000076b431a88&gt; (a java.lang.String)</span><br><span class="line"></span><br><span class="line">Found 1 deadlock.</span><br></pre></td></tr></table></figure><p>堆栈信息中很清楚明白的提示了”Found 1 deadlock”，可以看到线程Thread-0和Thread-1相互持有对方的资源，并等待对方释放资源，但是双方永远不会获取到要请求的资源，就造成了死锁。</p><p>接下来就是根据堆栈信息去排查程序中造成死锁的代码行了。</p><p>参考</p><ol><li><a href="https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/7_Deadlocks.html">https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/7_Deadlocks.html</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> 死锁 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark DataFrame导出为Excel文件</title>
      <link href="2020/10/11/Spark-DataFrame%E5%AF%BC%E5%87%BA%E4%B8%BAExcel%E6%96%87%E4%BB%B6/"/>
      <url>2020/10/11/Spark-DataFrame%E5%AF%BC%E5%87%BA%E4%B8%BAExcel%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>有时候我们在进行一些表的计算之后，会生成一些指标，需要导出来给其它同事用，虽说可以将DataFrame直接写成表，然后通过工具(比如Hue)导出为Excel，但是步骤就多了，而且如果要导出的表比较多的话，就更浪费时间了，那么这时候调用第三方插件就可以直接导出为Excel。</p><a id="more"></a><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><h2 id="1-Maven依赖"><a href="#1-Maven依赖" class="headerlink" title="1. Maven依赖"></a>1. Maven依赖</h2><p>需添加的第三方依赖：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&lt;dependency&gt;</span></span><br><span class="line">   <span class="string">&lt;groupId&gt;com.crealytics&lt;/groupId&gt;</span></span><br><span class="line">   <span class="string">&lt;artifactId&gt;spark-excel_2.11&lt;/artifactId&gt;</span></span><br><span class="line">   <span class="string">&lt;version&gt;0.12.5&lt;/version&gt;</span></span><br><span class="line"><span class="string">&lt;/dependency&gt;</span></span><br></pre></td></tr></table></figure><h2 id="2-代码实现"><a href="#2-代码实现" class="headerlink" title="2. 代码实现"></a>2. 代码实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">df.write</span><br><span class="line">.format(<span class="string">&quot;com.crealytics.spark.excel&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;dataAddress&quot;</span>, <span class="string">&quot;&#x27;My Sheet&#x27;!B3:C35&quot;</span>) <span class="comment">// 指定sheet名称和要开始写入的cell位置或cell范围</span></span><br><span class="line">.option(<span class="string">&quot;useHeader&quot;</span>, <span class="string">&quot;false&quot;</span>) <span class="comment">//是否输出表头</span></span><br><span class="line">.option(<span class="string">&quot;dateFormat&quot;</span>, <span class="string">&quot;yyyy-mm-dd hh:mm:ss&quot;</span>) <span class="comment">// Optional, default: yy-m-d h:mm</span></span><br><span class="line">.option(<span class="string">&quot;timestampFormat&quot;</span>, <span class="string">&quot;yyyy-mm-dd hh:mm:ss&quot;</span>) <span class="comment">// Optional, default: yyyy-mm-dd hh:mm:ss.000</span></span><br><span class="line">.mode(<span class="string">&quot;append&quot;</span>) <span class="comment">// Optional, default: overwrite. &quot;append&quot;模式下可以在一个Excel文件中追加多个sheet</span></span><br><span class="line">.save(<span class="string">&quot;/file.xlsx&quot;</span>) <span class="comment">// 要输出的HDFS文件路径. </span></span><br></pre></td></tr></table></figure><h2 id="3-参数详解"><a href="#3-参数详解" class="headerlink" title="3. 参数详解"></a>3. 参数详解</h2><p>下面就拿两种最常用的参数举例：</p><h3 id="1-‘sheet-name’-B3-C35。"><a href="#1-‘sheet-name’-B3-C35。" class="headerlink" title="1. ‘sheet_name’!B3:C35。"></a>1. ‘sheet_name’!B3:C35。</h3><ul><li><strong>sheet_name</strong>：表示要指定的sheet名称，如果不指定sheet，系统会给一个默认名称。另外，在追加模式下(mode=”append”)，一个Excel文件可以写入多个sheet；</li><li><strong>B3:C35</strong>：B3为起始位置，表示数据从B3这个cell位置开始写；C35为终止位置，表示数据写到C35这个位置为止。当然，可以只指定起始位置，那会从起始位置写入尽可能多的数据；也可以只指定终止位置，那会默认从表格的第一个cell也就是A1开始写数据，到指定的终止位置。</li></ul><h3 id="2-sheet-name-All"><a href="#2-sheet-name-All" class="headerlink" title="2. sheet_name[#All]"></a>2. sheet_name[#All]</h3><ul><li>**[#All]**：表示从表格的第一个cell也就是A1开始写尽可能多的数据。</li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://github.com/crealytics/spark-excel">1. https://github.com/crealytics/spark-excel</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> DataFrame </tag>
            
            <tag> Excel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>15分钟搞定 Spark集群安装</title>
      <link href="2020/10/11/15%E5%88%86%E9%92%9F%E6%90%9E%E5%AE%9A-Spark%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"/>
      <url>2020/10/11/15%E5%88%86%E9%92%9F%E6%90%9E%E5%AE%9A-Spark%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<p>快速安装Spark集群。</p><a id="more"></a><p><font color=red>注：以下步骤中，从第1步到第8步，除了元数据库的安装之外，其他步骤都是要在集群所有节点上执行的！</font></p><h1 id="1-官网下载Spark安装包"><a href="#1-官网下载Spark安装包" class="headerlink" title="1. 官网下载Spark安装包"></a>1. 官网下载Spark安装包</h1><p>下载要安装版本的安装包：<a href="https://archive.apache.org/dist/spark/">https://archive.apache.org/dist/spark/</a>。</p><p>我这里下载的是：<a href="https://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.6.tgz%E3%80%82%E5%BB%BA%E8%AE%AE%E7%94%A8%E8%BF%85%E9%9B%B7%E4%B8%8B%E8%BD%BD%EF%BC%8C%E4%BC%9A%E6%AF%94%E8%BE%83%E5%BF%AB%E3%80%82">https://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.6.tgz。建议用迅雷下载，会比较快。</a></p><h1 id="2-解压安装包"><a href="#2-解压安装包" class="headerlink" title="2. 解压安装包"></a>2. 解压安装包</h1><p>将下载好的安装包上传到集群某个节点的/opt目录，解压<br>tar -zxvf spark-2.3.1-bin-hadoop2.6.tgz spark</p><h1 id="3-Spark配置"><a href="#3-Spark配置" class="headerlink" title="3. Spark配置"></a>3. Spark配置</h1><p>进入到Spark安装路径conf目录下：cd ../spark/conf/。复制以下3个文件：<br>cp spark-env.sh.template spark-env.sh<br>cp spark-defaults.conf.template spark-defaults.conf<br>cp log4j.properties.template log4j.properties(INFO改为WARN)</p><h3 id="1-配置spark-env-sh"><a href="#1-配置spark-env-sh" class="headerlink" title="1. 配置spark-env.sh"></a>1. 配置spark-env.sh</h3><p>这个文件主要是配置Spark的环境变量。</p><p>文件中添加以下环境变量，vi spark-env.sh：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/etc/hadoop/conf <span class="comment"># Hadoop配置文件所在目录</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop <span class="comment"># Hadoop安装路径</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=\<span class="variable">$HADOOP_HOME</span>/lib/native:\<span class="variable">$LD_LIBRARY_PATH</span></span><br></pre></td></tr></table></figure><p>主要让Spark找到Hadoop的安装目录。</p><h3 id="2-配置spark-defaults-conf"><a href="#2-配置spark-defaults-conf" class="headerlink" title="2. 配置spark-defaults.conf"></a>2. 配置spark-defaults.conf</h3><p>这个文件主要是一些Spark相关的配置项。<br>以下以Spark on YARN为例，vi spark-defaults.conf：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.masteryarn <span class="comment"># 指定yarn作为资源调度框架</span></span><br><span class="line">spark.eventLog.enabled<span class="literal">true</span> <span class="comment"># 启动事件日志, 以便后续日志分析</span></span><br><span class="line">spark.eventLog.dirhdfs://master-1:8021/spark_eventlog</span><br><span class="line">spark.serializerorg.apache.spark.serializer.KryoSerializer <span class="comment"># Kryo序列化性能要好于默认的Java序列化</span></span><br><span class="line">spark.driver.memory2g <span class="comment"># 指定默认Driver端内存</span></span><br><span class="line">spark.yarn.archivehdfs://master-1:8021/archive <span class="comment"># 将Spark程序运行所需的jar打包放到这个目录，避免每次提交任务都要从本地Spark安装目录下拷贝jar包</span></span><br></pre></td></tr></table></figure><p>Spark有不计其数的配置项，可根据个人需求进行配置。具体可参考官网<a href="https://spark.apache.org/docs/latest/configuration.html">https://spark.apache.org/docs/latest/configuration.html</a>。</p><h3 id="3-配置log4j-properties"><a href="#3-配置log4j-properties" class="headerlink" title="3. 配置log4j.properties"></a>3. 配置log4j.properties</h3><p>可以配置Spark任务的日志级别。</p><h1 id="4-scp安装文件"><a href="#4-scp安装文件" class="headerlink" title="4. scp安装文件"></a>4. scp安装文件</h1><p>将解压配置好的Spark安装文件拷贝到其他节点：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp username@hostname:/opt/spark username@hostname:/opt</span><br></pre></td></tr></table></figure><h1 id="5-测试"><a href="#5-测试" class="headerlink" title="5. 测试"></a>5. 测试</h1><ul><li><p>启动spark shell命令行，写一段简单的Spark程序：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark<span class="literal">-shell</span> -<span class="literal">-master</span> yarn -<span class="literal">-num</span><span class="literal">-executors</span> <span class="number">2</span> -<span class="literal">-executor</span><span class="literal">-cores</span> <span class="number">1</span> -<span class="literal">-executor</span><span class="literal">-memory</span> <span class="number">512</span>M</span><br></pre></td></tr></table></figure></li><li><p>或启动spark sql命令行，执行一些简单的SQL语句：</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark<span class="literal">-sql</span> -<span class="literal">-master</span> yarn -<span class="literal">-num</span><span class="literal">-executors</span> <span class="number">2</span> -<span class="literal">-executor</span><span class="literal">-cores</span> <span class="number">1</span> -<span class="literal">-executor</span><span class="literal">-memory</span> <span class="number">512</span>M</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vim编辑器常用操作</title>
      <link href="2020/10/11/Vim%E7%BC%96%E8%BE%91%E5%99%A8%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"/>
      <url>2020/10/11/Vim%E7%BC%96%E8%BE%91%E5%99%A8%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p>Vim常用命令</p><a id="more"></a><h1 id="光标移动"><a href="#光标移动" class="headerlink" title="光标移动"></a>光标移动</h1><ul><li>快速移动到行首：^</li><li>快速移动到行尾：$</li><li>快速移动到文件第一行：连按两下g键</li><li>快速移动到文件最后一行：shift+g组合键</li></ul><h1 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h1><ol><li>非编辑模式下输入斜杠(/)，之后紧挨着输入要查找的内容</li><li>按回车键定位到要查找的内容</li><li>按n键查找下一个；shift+n键查找上一个</li></ol><h1 id="行号"><a href="#行号" class="headerlink" title="行号"></a>行号</h1><ul><li><h2 id="显示行号"><a href="#显示行号" class="headerlink" title="显示行号"></a>显示行号</h2></li></ul><ol><li>vim进入待编辑文件</li><li>输入英文的冒号</li><li>输入set number，然后按回车键即会显示行号</li></ol><ul><li><h2 id="取消行号"><a href="#取消行号" class="headerlink" title="取消行号"></a>取消行号</h2></li></ul><ol><li>非编辑模式下输入英文的冒号</li><li>输入set nonumber，然后按回车键即会取消行号</li></ol><h1 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h1><ul><li><h2 id="批量注释"><a href="#批量注释" class="headerlink" title="批量注释"></a>批量注释</h2></li></ul><ol><li>vim进入待编辑文件</li><li>键盘上下键移动到要注释文本的第一行</li><li>组合键ctrl + v 进入 VISUAL BLOCK 模式</li><li>键盘上下键移动到要注释文本的最后一行</li><li>组合键shift + i 进入编辑模式</li><li>输入注释符号(#或者//等)</li><li>连续按两次ESC键，此时会看到选中的行被注释</li></ol><ul><li><h2 id="批量取消注释"><a href="#批量取消注释" class="headerlink" title="批量取消注释"></a>批量取消注释</h2></li></ul><ol><li>vim进入待编辑文件</li><li>键盘上下键移动到要取消注释文本的第一行</li><li>组合键ctrl + v 进入 VISUAL BLOCK 模式</li><li>键盘上下键移动到要取消注释文本的最后一行</li><li>按下键盘的d键，此时会看到选中的行被取消注释</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Vim </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux Sed命令使用指南</title>
      <link href="2020/10/11/Linux-Sed%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
      <url>2020/10/11/Linux-Sed%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<h1 id="Sed命令"><a href="#Sed命令" class="headerlink" title="Sed命令"></a>Sed命令</h1><p>sed编辑器被称为流编辑器，它会执行下列操作：</p><ol><li>一次从输入读取一行数据，</li><li>根据所提供的编辑器命令匹配数据，</li><li>按照命令修改流中的数据，</li><li>将新的数据输出到STDOUT。</li></ol><p>sed命令的格式：sed options [script] [file]</p><a id="more"></a><h1 id="常用options"><a href="#常用options" class="headerlink" title="常用options"></a>常用options</h1><ul><li><p><strong>-i</strong><br>替换文件中每一行的aa为a，默认情况下它只替换每行中出现的第一处:</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">&#x27;s/aa/a/g&#x27;</span> data.txt</span><br></pre></td></tr></table></figure></li><li><p><strong>-e</strong><br>此选项可执行多个命令:</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -e <span class="string">&#x27;s/brown/green/; s/dog/cat/&#x27;</span> data.txt</span><br><span class="line">或sed -e <span class="string">&#x27;s/brown/green/ s/dog/cat/&#x27;</span> data.txt</span><br></pre></td></tr></table></figure></li><li><p><strong>-f</strong><br>此选项在sed命令中指定文件。如果有大量要处理的sed命令，那么可以将它们放进一个单独的文件中：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -f script.sed data.txt </span><br></pre></td></tr></table></figure></li></ul><h1 id="替换标记"><a href="#替换标记" class="headerlink" title="替换标记"></a>替换标记</h1><p>让替换命令能够替换一行中不同地方出现的文本，格式：s/pattern/replacement/flags。<br>有4种可用的替换标记：</p><ol><li><p><strong>数字</strong>：表明新文本将替换第几处模式匹配的地方。</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">&#x27;s/test/trial/2&#x27;</span> data.txt</span><br></pre></td></tr></table></figure></li><li><p><strong>g</strong>：表明新文本将会替换所有匹配的文本。</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">&#x27;s/test/trial/g&#x27;</span> data.txt</span><br></pre></td></tr></table></figure></li><li><p><strong>p</strong>：表明原先行的内容要打印出来。</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sed -n <span class="string">&#x27;s/test/trial/p&#x27;</span> data.txt</span><br><span class="line"></span><br><span class="line">-n表示禁止sed编辑器输出，但p替换标记会输出修改过的行，将二者配合使用的效果就是只输出被替换命令修改过的行。</span><br></pre></td></tr></table></figure></li><li><p><strong>w file</strong>：将替换的结果写带文件中。</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">&#x27;s/test/trial/w out.txt&#x27;</span> data.txt</span><br></pre></td></tr></table></figure></li></ol><h1 id="行寻址"><a href="#行寻址" class="headerlink" title="行寻址"></a>行寻址</h1><p>sed编辑器中使用的命令会作用于文本数据的所有行，如果只想讲命令作用于特定行，则必须用行寻址。</p><p>有以下两种形式：</p><ol><li>以数字形式表示行区间<ul><li>替换第二行：sed ‘2s/dog/cat/‘ data.txt</li><li>替换第二行到第三行：sed ‘2,3s/dog/cat/‘ data.txt</li><li>替换某行开始的所有行：sed ‘2,$s/dog/cat/‘ data.txt</li></ul></li><li>用文本模式来过滤行<br>sed ‘/Samantha/s/bash/csh/‘ /etc/passwd命令组合，如果需要在单行上执行多条命令，可以用花括号将多余命令组合在一起: sed  ‘2{s/fox/elephant/ s/dog/cat/}’ data.txt</li></ol><h1 id="删除操作"><a href="#删除操作" class="headerlink" title="删除操作"></a>删除操作</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">&#x27;d&#x27;</span> data.txt   流中的所有文本行都会被删除</span><br><span class="line">sed <span class="string">&#x27;3d&#x27;</span> data.txt删除第三行</span><br><span class="line">sed <span class="string">&#x27;2,3d&#x27;</span> data.txt删除第2到第三行</span><br><span class="line">sed <span class="string">&#x27;3,\$d&#x27;</span> data.txt删除第三及之后所有行</span><br><span class="line">sed <span class="string">&#x27;/dog/d&#x27;</span> data.txt删除包含匹配指定模式的行</span><br><span class="line">sed <span class="string">&#x27;/^\$/d&#x27;</span> data.txt删除空白行</span><br></pre></td></tr></table></figure><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ul><li><p>打印匹配行到最后一行的内容</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -n <span class="string">&#x27;/c/,$&#123;//!p&#125;&#x27;</span> file</span><br></pre></td></tr></table></figure></li><li><p>多个空格替换为指定字符(^)</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">&#x27;s/[[:space:]][[:space:]]*/^/g&#x27;</span> data.txt</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> sed </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Connector连接器之整合读写Phoenix</title>
      <link href="2020/10/11/Spark-Connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99Phoenix/"/>
      <url>2020/10/11/Spark-Connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99Phoenix/</url>
      
        <content type="html"><![CDATA[<p>Phoenix为NoSQL数据库HBase提供了标准SQL和JDBC API的强大功能，且具备完整的ACID事务处理能力。对于小数据量的查询，其性能可以达到毫秒级别；对于数千万行的数据，其性能也可以达到秒级。</p><a id="more"></a><p>要使用phoenix-spark插件，需要在项目中添加如下依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-spark<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.1-HBase-1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Spark版本为2.3.1，HBase版本为1.2.0，Phoenix版本为4.14.1。</p><h2 id="Spark加载Phoenix表"><a href="#Spark加载Phoenix表" class="headerlink" title="Spark加载Phoenix表"></a>Spark加载Phoenix表</h2><p>方法一：使用数据源API加载Phoenix表为一个DataFrame</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">readPhoenix</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val df = spark.read</span><br><span class="line">        .format(<span class="string">&quot;org.apache.phoenix.spark&quot;</span>)</span><br><span class="line">        .options(Map(<span class="string">&quot;table&quot;</span> -&gt; <span class="string">&quot;TEST&quot;</span>, <span class="string">&quot;zkUrl&quot;</span> -&gt; <span class="string">&quot;host:2181&quot;</span>))</span><br><span class="line">        .load()</span><br><span class="line"></span><br><span class="line">    df.show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>方法二：使用Configuration对象加载Phoenix表为一个DataFrame</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.phoenix.spark._</span><br><span class="line"></span><br><span class="line"><span class="function">def <span class="title">readPhoenix</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val conf = <span class="keyword">new</span> Configuration()</span><br><span class="line">    conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hostname:2181&quot;</span>)</span><br><span class="line">    val df = spark.sqlContext.phoenixTableAsDataFrame(</span><br><span class="line">        <span class="string">&quot;test&quot;</span>, <span class="comment">//表名</span></span><br><span class="line">        Seq(<span class="string">&quot;NAME&quot;</span>, <span class="string">&quot;AGE&quot;</span>), <span class="comment">//指定要加载的列名</span></span><br><span class="line">        predicate = Some(<span class="string">&quot;PHONE = 13012340000&quot;</span>), <span class="comment">//可设置where条件</span></span><br><span class="line">        conf = conf)</span><br><span class="line"></span><br><span class="line">    df.show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>方法三：使用Zookeeper URL加载Phoenix表为一个RDD</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.phoenix.spark._</span><br><span class="line"></span><br><span class="line"><span class="function">def <span class="title">readPhoenix</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val rdd =spark.sparkContext.phoenixTableAsRDD(</span><br><span class="line">        <span class="string">&quot;TEST&quot;</span>,</span><br><span class="line">        Seq(<span class="string">&quot;NAME&quot;</span>, <span class="string">&quot;AGE&quot;</span>),</span><br><span class="line">        predicate = Some(<span class="string">&quot;PHONE = 13012340000&quot;</span>),</span><br><span class="line">        zkUrl = Some(<span class="string">&quot;hostname:2181&quot;</span>) <span class="comment">//Zookeeper URL来连接Phoenix</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    rdd.map(_.get(<span class="string">&quot;&quot;</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Spark持久化数据到Phoenix"><a href="#Spark持久化数据到Phoenix" class="headerlink" title="Spark持久化数据到Phoenix"></a>Spark持久化数据到Phoenix</h2><p>创建要导入的Phoenix表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">TEST</span>(<span class="keyword">id</span> <span class="built_in">BIGINT</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> PRIMARY <span class="keyword">KEY</span>, col1 <span class="built_in">VARCHAR</span>, col2 <span class="built_in">INTEGER</span>);</span><br></pre></td></tr></table></figure><h3 id="保存RDD到Phoenix"><a href="#保存RDD到Phoenix" class="headerlink" title="保存RDD到Phoenix"></a>保存RDD到Phoenix</h3><p>代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.phoenix.spark._</span><br><span class="line"></span><br><span class="line"><span class="function">def <span class="title">writePhoenix</span><span class="params">(spark: SparkSession)</span>: Unit </span>= &#123;</span><br><span class="line">    val dataSet = List((<span class="number">1L</span>, <span class="string">&quot;1&quot;</span>, <span class="number">1</span>), (<span class="number">2L</span>, <span class="string">&quot;2&quot;</span>, <span class="number">2</span>), (<span class="number">3L</span>, <span class="string">&quot;3&quot;</span>, <span class="number">3</span>))</span><br><span class="line">    spark.sparkContext.parallelize(dataSet)</span><br><span class="line">        .saveToPhoenix(</span><br><span class="line">            <span class="string">&quot;TEST&quot;</span>, <span class="comment">//表名</span></span><br><span class="line">            Seq(<span class="string">&quot;ID&quot;</span>,<span class="string">&quot;COL1&quot;</span>,<span class="string">&quot;COL2&quot;</span>), <span class="comment">//列命名</span></span><br><span class="line">            zkUrl = Some(<span class="string">&quot;host:2181&quot;</span>) <span class="comment">//Zookeeper URL</span></span><br><span class="line">        )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="保存DataFrame到Phoenix"><a href="#保存DataFrame到Phoenix" class="headerlink" title="保存DataFrame到Phoenix"></a>保存DataFrame到Phoenix</h3><p>方法一：使用saveToPhoenix()方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val df: DataFrame = ...</span><br><span class="line"></span><br><span class="line">df.saveToPhoenix(Map(<span class="string">&quot;table&quot;</span> -&gt; <span class="string">&quot;TEST&quot;</span>, <span class="string">&quot;zkUrl&quot;</span> -&gt; <span class="string">&quot;host:2181&quot;</span>))</span><br></pre></td></tr></table></figure><p>方法二：使用format()方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val df: DataFrame = ...</span><br><span class="line"></span><br><span class="line">df.write</span><br><span class="line">    .format(<span class="string">&quot;org.apache.phoenix.spark&quot;</span>)</span><br><span class="line">    .mode(<span class="string">&quot;overwrite&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;table&quot;</span>, <span class="string">&quot;OUTPUT_TABLE&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;zkUrl&quot;</span>, <span class="string">&quot;host:2181&quot;</span>)</span><br><span class="line">    .save()</span><br></pre></td></tr></table></figure><p>参考</p><ol><li><a href="http://phoenix.apache.org/phoenix_spark.html">http://phoenix.apache.org/phoenix_spark.html</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> HBase </tag>
            
            <tag> Phoenix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Connector连接器之整合读写Elasticsearch</title>
      <link href="2020/10/11/Spark-Connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99Elasticsearch/"/>
      <url>2020/10/11/Spark-Connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99Elasticsearch/</url>
      
        <content type="html"><![CDATA[<p>Elasticsearch提供了对Spark的支持，可以将ES中的索引（一行行的JSON数据）加载为RDD或DataFrame。</p><a id="more"></a><p>在使用elasticsearch-spark插件之前，需要在项目中添加依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.elasticsearch<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>elasticsearch-spark-20_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>7.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h1 id="1-ES索引加载为RDD"><a href="#1-ES索引加载为RDD" class="headerlink" title="1. ES索引加载为RDD"></a>1. ES索引加载为RDD</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.elasticsearch.spark._</span><br><span class="line"></span><br><span class="line">val spark = SparkSession</span><br><span class="line">.builder()</span><br><span class="line">.config(<span class="string">&quot;hive.metastore.uris&quot;</span>, <span class="string">&quot;thrift://master-1:9083&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//es配置</span></span><br><span class="line">.config(<span class="string">&quot;es.nodes&quot;</span>, <span class="string">&quot;node-1&quot;</span>) </span><br><span class="line">.config(<span class="string">&quot;es.port&quot;</span>, <span class="string">&quot;9200&quot;</span>)</span><br><span class="line">.config(<span class="string">&quot;pushdown&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line"></span><br><span class="line">.appName(<span class="string">&quot;Spark ES&quot;</span>)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line">val rdd = spark.sparkContext.esJsonRDD(<span class="string">&quot;index_name&quot;</span>)</span><br><span class="line"></span><br><span class="line">spark.close()</span><br></pre></td></tr></table></figure><p>配置项”es.nodes”的值只要给出ES集群中的一个数据节点即可，只要给出一个节点，ES就能找到集群中的其他节点。数据节点就是ES安装目录下配置文件(../elasticsearch-7.1.0/config/elasticsearch.yml)中”node.master”为true的节点。</p><p>Spark从ES加载出来的数据是JSON String类型的RDD，根据请求体的结构就可以取出来具体的数据。</p><h1 id="2-ES索引加载为DataFrame"><a href="#2-ES索引加载为DataFrame" class="headerlink" title="2. ES索引加载为DataFrame"></a>2. ES索引加载为DataFrame</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">val spark = SparkSession</span><br><span class="line">.builder()</span><br><span class="line">.config(<span class="string">&quot;hive.metastore.uris&quot;</span>, <span class="string">&quot;thrift://master-1:9083&quot;</span>)</span><br><span class="line">.appName(<span class="string">&quot;Spark ES&quot;</span>)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line">val options = Map(</span><br><span class="line"><span class="string">&quot;es.nodes&quot;</span> -&gt; <span class="string">&quot;node-1&quot;</span>,</span><br><span class="line"><span class="string">&quot;es.port&quot;</span> -&gt; <span class="string">&quot;9200&quot;</span>,</span><br><span class="line"><span class="string">&quot;pushdown&quot;</span> -&gt; <span class="string">&quot;true&quot;</span>)</span><br><span class="line"></span><br><span class="line">val df = spark.read.format(<span class="string">&quot;org.elasticsearch.spark.sql&quot;</span>)</span><br><span class="line">.options(options)</span><br><span class="line">.load(<span class="string">&quot;index_name&quot;</span>)</span><br><span class="line">.where(<span class="string">&quot;col = 100&quot;</span>)</span><br><span class="line">.select(<span class="string">&quot;col1&quot;</span>, <span class="string">&quot;col2&quot;</span>, <span class="string">&quot;col3&quot;</span>)</span><br><span class="line"></span><br><span class="line">spark.close()</span><br></pre></td></tr></table></figure><p>将ES索引加载成DataFrame后便可以方便地进行表操作了。</p><p>最后注意提交Spark任务时加上第三方依赖包elasticsearch-spark-20_2.11-7.1.0.jar。</p><p>参考</p><ol><li><a href="https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html">https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python使用POP3协议客户端poplib登录邮箱并解压缩zip、rar压缩包</title>
      <link href="2020/10/11/Python%E4%BD%BF%E7%94%A8POP3%E5%8D%8F%E8%AE%AE%E5%AE%A2%E6%88%B7%E7%AB%AFpoplib%E7%99%BB%E5%BD%95%E9%82%AE%E7%AE%B1%E5%B9%B6%E8%A7%A3%E5%8E%8B%E7%BC%A9zip%E3%80%81rar%E5%8E%8B%E7%BC%A9%E5%8C%85/"/>
      <url>2020/10/11/Python%E4%BD%BF%E7%94%A8POP3%E5%8D%8F%E8%AE%AE%E5%AE%A2%E6%88%B7%E7%AB%AFpoplib%E7%99%BB%E5%BD%95%E9%82%AE%E7%AE%B1%E5%B9%B6%E8%A7%A3%E5%8E%8B%E7%BC%A9zip%E3%80%81rar%E5%8E%8B%E7%BC%A9%E5%8C%85/</url>
      
        <content type="html"><![CDATA[<h1 id="1-项目背景"><a href="#1-项目背景" class="headerlink" title="1. 项目背景"></a>1. 项目背景</h1><p>目前在做的一个小项目，需要到登录到邮箱获取压缩包，解压压缩包获取文件，并从文件中抽取出有效数据入库，后续会做一些二次加工，最后用到业务风控中。</p><a id="more"></a><h1 id="2-poplib模块"><a href="#2-poplib模块" class="headerlink" title="2. poplib模块"></a>2. poplib模块</h1><p>Python内建了poplib模块来实现登录邮箱，这个模块提供了一个类POP3_SSL，它支持使用SSL(Secure Socket Layer，安全套阶层，在OSI模型处于会话层)作为底层协议连接POP3服务器。</p><h1 id="3-邮箱登录"><a href="#3-邮箱登录" class="headerlink" title="3. 邮箱登录"></a>3. 邮箱登录</h1><p>邮箱登录代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> poplib</span><br><span class="line"></span><br><span class="line">\<span class="comment"># 通过主机名、端口生成POP3对象</span></span><br><span class="line">pop = poplib.POP3_SSL(host=<span class="string">&quot;&quot;</span>, port=<span class="string">&quot;&quot;</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    \<span class="comment"># 通过用户名、密码登录邮箱</span></span><br><span class="line">    pop.user(<span class="string">&quot;&quot;</span>)</span><br><span class="line">    pop.pass_(<span class="string">&quot;&quot;</span>)</span><br><span class="line"><span class="keyword">except</span> poplib.error_proto <span class="keyword">as</span> e:</span><br><span class="line">    logger.error(<span class="string">&quot;Login failed: &quot;</span> + e)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    parse(pop, sys.argv)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    \<span class="comment"># 退出邮箱</span></span><br><span class="line">    pop.quit()</span><br></pre></td></tr></table></figure><p>注意，最后程序不管是正常结束还是非正常结束都一定要退出邮箱。</p><h1 id="4-获取邮件内容"><a href="#4-获取邮件内容" class="headerlink" title="4. 获取邮件内容"></a>4. 获取邮件内容</h1><p>获取指定邮件有多种方式，可以通过邮件发件人、邮件发件邮箱地址、邮件主题，还可以用邮件索引，使用邮件索引速度最快，这种获取方式就跟在散列表中根据key获取对应value很相似。</p><h2 id="1-获取邮件基本信息"><a href="#1-获取邮件基本信息" class="headerlink" title="1.  获取邮件基本信息"></a>1.  获取邮件基本信息</h2><p>登录邮箱之后，可以获取很多与邮件相关的基本信息，比如邮箱邮件列表、邮件主题、邮件内容、邮件发件人及发件地址、邮件接收时间等等</p><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> email.parser <span class="keyword">import</span> Parser</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_mail_list</span>(<span class="params">pop</span>):</span></span><br><span class="line">    <span class="comment"># 查找附件, 并返回下载文件路径</span></span><br><span class="line">    emails = pop.list()[<span class="number">1</span>]  <span class="comment"># 邮箱邮件列表</span></span><br><span class="line">    email_num = len(emails)  <span class="comment"># 邮件数量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>]:</span><br><span class="line">        lines = pop.retr(i)[<span class="number">1</span>]  <span class="comment"># 获取指定的邮件</span></span><br><span class="line">        line_bytes = <span class="string">b&#x27;\r\n&#x27;</span>.join(lines)  <span class="comment"># 换行符分割邮件信息</span></span><br><span class="line">        msg_content = line_bytes.decode(<span class="string">&#x27;utf-8&#x27;</span>)  <span class="comment"># 解码</span></span><br><span class="line">        msg = Parser().parsestr(msg_content)  <span class="comment"># 每封邮件信息</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 发件人和邮箱地址</span></span><br><span class="line">        header, address = parseaddr(msg.get(<span class="string">&quot;From&quot;</span>, <span class="string">&quot;&quot;</span>)) </span><br><span class="line">        name, address = decode_str(header), decode_str(address)</span><br><span class="line">        <span class="comment"># 邮件主题</span></span><br><span class="line">        email_subject = decode_str(msg.get(<span class="string">&#x27;Subject&#x27;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 邮件接收时间</span></span><br><span class="line">        date_tuple = parsedate_tz(msg.get(<span class="string">&quot;Date&quot;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line">        date_formatted = datetime.fromtimestamp(mktime_tz(date_tuple)).strftime(<span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        compressed_files = list()</span><br><span class="line">        <span class="keyword">for</span> compressed_file <span class="keyword">in</span> get_attachment(msg, compressed_files):</span><br><span class="line">            <span class="keyword">yield</span> compressed_file</span><br></pre></td></tr></table></figure><h2 id="2-获取邮件中的附件"><a href="#2-获取邮件中的附件" class="headerlink" title="2. 获取邮件中的附件"></a>2. 获取邮件中的附件</h2><p>获取邮件附件中的zip或rar压缩包，<font color=red>注意：这里要考虑到一封邮件中有多个压缩包的情况。</font></p><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_attachment</span>(<span class="params">msg, ompressed_files: list</span>):</span></span><br><span class="line">    <span class="keyword">if</span> msg.is_multipart() <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">        \<span class="comment"># 分层信息</span></span><br><span class="line">        parts = msg.get_payload()</span><br><span class="line">        <span class="keyword">for</span> n, part <span class="keyword">in</span> enumerate(parts):</span><br><span class="line">            res = get_attachment(part, compressed_files)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        content_type = msg.get_content_type()</span><br><span class="line">        <span class="keyword">if</span> content_type == <span class="string">&#x27;application/octet-stream&#x27;</span>:</span><br><span class="line">            <span class="keyword">for</span> subpart <span class="keyword">in</span> msg.walk():  \<span class="comment"># 遍历消息树(深度优先搜索算法), 获取每个子节点</span></span><br><span class="line">                file_name_encoder = subpart.get_filename()</span><br><span class="line">                file_name = decode_str(file_name_encoder)  \<span class="comment"># 解码获取文件名</span></span><br><span class="line">                \<span class="comment"># 判断是否是zip或rar格式文件</span></span><br><span class="line">                <span class="keyword">if</span> file_name.split(<span class="string">&quot;.&quot;</span>)[<span class="number">-1</span>] <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;zip&#x27;</span>, <span class="string">&quot;rar&quot;</span>]:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                data = msg.get_payload(decode=<span class="literal">True</span>)  \<span class="comment"># 附件二进制</span></span><br><span class="line">                file_data = base64.b64encode(data).decode()  \<span class="comment"># base64编码</span></span><br><span class="line">                compressed_files.append(&#123;<span class="string">&quot;file_data&quot;</span>: file_data, <span class="string">&quot;name&quot;</span>: file_name&#125;)  \<span class="comment"># 保存到列表</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> compressed_files</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode_str</span>(<span class="params">s</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    解码</span></span><br><span class="line"><span class="string">    :param s:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    value, charset = decode_header(s)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> charset:</span><br><span class="line">        value = value.decode(charset)</span><br><span class="line">    <span class="keyword">return</span> value</span><br></pre></td></tr></table></figure><h1 id="5-解压zip-rar压缩包"><a href="#5-解压zip-rar压缩包" class="headerlink" title="5. 解压zip/rar压缩包"></a>5. 解压zip/rar压缩包</h1><p>在获取到邮件附件的二进制内容之后，就可以使用zipfile模块和rarfile模块解解压zip和rar压缩包了。</p><h2 id="1-打开zip-rar压缩包"><a href="#1-打开zip-rar压缩包" class="headerlink" title="1. 打开zip/rar压缩包"></a>1. 打开zip/rar压缩包</h2><p>处理步骤4中获取的邮件附件，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> rarfile</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">pop</span>):</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> get_mail_list(pop):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> data[<span class="string">&#x27;name&#x27;</span>].endswith(<span class="string">&quot;.zip&quot;</span>):  <span class="comment"># zip格式的压缩文件</span></span><br><span class="line">                zip_obj = zipfile.ZipFile(BytesIO(base64.b64decode(data[<span class="string">&#x27;file_data&#x27;</span>].encode())))</span><br><span class="line">                <span class="keyword">for</span> record <span class="keyword">in</span> get_files_in_zip(zip_obj, item):</span><br><span class="line">                    <span class="keyword">pass</span>  <span class="comment"># 数据处理</span></span><br><span class="line">            <span class="keyword">elif</span> data[<span class="string">&#x27;name&#x27;</span>].endswith(<span class="string">&quot;.rar&quot;</span>):  <span class="comment"># rar格式的压缩文件</span></span><br><span class="line">                rar_obj = rarfile.RarFile(BytesIO(base64.b64decode(data[<span class="string">&#x27;file_data&#x27;</span>].encode())))</span><br><span class="line">                <span class="keyword">for</span> record <span class="keyword">in</span> get_files_in_rar(rar_obj, item):</span><br><span class="line"><span class="keyword">pass</span>  <span class="comment"># 数据处理</span></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            logger.error(traceback.format_exc())</span><br></pre></td></tr></table></figure><h2 id="2-获取压缩包中文件"><a href="#2-获取压缩包中文件" class="headerlink" title="2. 获取压缩包中文件"></a>2. 获取压缩包中文件</h2><p>打开压缩包之后，便可迭代获取压缩包中文件。不过这里要注意的是，zip/rar压缩中可能会嵌套压缩包，比如zip压缩包中嵌套zip压缩包或rar压缩包，那么就需要使用递归进行解压缩。</p><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> rarfile</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_files_in_zip</span>(<span class="params">zip_obj</span>):</span></span><br><span class="line">    <span class="comment"># 获取压缩包中文件列表, 同时过滤一些空目录</span></span><br><span class="line">    files = [file <span class="keyword">for</span> file <span class="keyword">in</span> zip_obj.namelist() <span class="keyword">if</span> re.search(<span class="string">&quot;\\..*$&quot;</span>, file)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历每个压缩包中的所有文件</span></span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">        <span class="comment"># 压缩包中嵌套压缩包</span></span><br><span class="line">        <span class="keyword">if</span> file.endswith(<span class="string">&quot;zip&quot;</span>):</span><br><span class="line">            inner_zip_obj = zipfile.ZipFile(BytesIO(base64.b64decode(base64.b64encode(zip_obj.read(file)).decode().encode())))</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> get_files_in_zip(inner_zip_obj)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">elif</span> file.endswith(<span class="string">&quot;rar&quot;</span>):</span><br><span class="line">            inner_rar_obj = rarfile.RarFile(BytesIO(base64.b64decode(base64.b64encode(zip_obj.read(file)).decode().encode())))</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> get_files_in_rar(inner_rar_obj)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line"><span class="keyword">pass</span>  <span class="comment"># 压缩中文件处理</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_files_in_rar</span>(<span class="params">rar_obj</span>):</span></span><br><span class="line">    <span class="comment"># 获取压缩包中文件列表, 同时过滤一些空目录</span></span><br><span class="line">    files = [file <span class="keyword">for</span> file <span class="keyword">in</span> rar_obj.namelist() <span class="keyword">if</span> re.search(<span class="string">&quot;\\..*$&quot;</span>, file)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析每个文件内容</span></span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">        <span class="comment"># 压缩包中嵌套压缩包</span></span><br><span class="line">        <span class="keyword">if</span> file.endswith(<span class="string">&quot;rar&quot;</span>):</span><br><span class="line">            inner_rar_obj = rarfile.RarFile(BytesIO(base64.b64decode(base64.b64encode(rar_obj.read(file)).decode().encode())))</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> get_files_in_rar(inner_rar_obj)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">elif</span> file.endswith(<span class="string">&quot;zip&quot;</span>):</span><br><span class="line">            inner_zip_obj = zipfile.ZipFile(BytesIO(base64.b64decode(base64.b64encode(rar_obj.read(file)).decode().encode())))</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> get_files_in_zip(inner_zip_obj)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line"><span class="keyword">pass</span>  <span class="comment"># 压缩中文件处理</span></span><br></pre></td></tr></table></figure><p>解压获取压缩包中文件之后，就可以从文件(这里主要是pdf、html、excel、csv等格式的文件)中提取数据了。</p><p>不同格式的文件如何解析，请参考我的<a href="https://blog.csdn.net/lovetechlovelife/article/details/107499638">另一篇文章</a>。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 邮箱 </tag>
            
            <tag> POP </tag>
            
            <tag> ZIP </tag>
            
            <tag> RAR </tag>
            
            <tag> 压缩包 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python解析Excel、PDF、HTML、CSV...文件</title>
      <link href="2020/10/11/Python%E8%A7%A3%E6%9E%90Excel%E3%80%81PDF%E3%80%81HTML%E3%80%81CSV-%E6%96%87%E4%BB%B6/"/>
      <url>2020/10/11/Python%E8%A7%A3%E6%9E%90Excel%E3%80%81PDF%E3%80%81HTML%E3%80%81CSV-%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<p>Python在文件解析方面有很多丰富的三方模块，接下来我们就看如何使用Python解析Excel、PDF、HTML、CSV这几种常见格式的文件。</p><a id="more"></a><h1 id="1-Excel文件解析"><a href="#1-Excel文件解析" class="headerlink" title="1. Excel文件解析"></a>1. Excel文件解析</h1><p>标准的Excel文件其实就相当于一张表，有表头和对应的数据。一般常见的Excel文件都以.xls、.xlsx和.et结尾。</p><p>以下是使用xlrd模块实现解析Excel文件的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> xlrd</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xls_parse</span>(<span class="params">file_data</span>):</span></span><br><span class="line">    <span class="comment"># 打开一个电子表格文件进行数据提取</span></span><br><span class="line">    data = xlrd.open_workbook(file_contents=a bytes object, encoding_override=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="comment"># 一个Excel文件中所有sheet的名称</span></span><br><span class="line">    sheets = data.sheet_names()</span><br><span class="line">    <span class="comment"># 通过sheet名获取对应的sheet表内容</span></span><br><span class="line">    table = data.sheet_by_name(sheets[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    start_row = <span class="number">0</span>  <span class="comment"># 起始行</span></span><br><span class="line">    rows = table.nrows  <span class="comment"># 表格总行数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假如第一行是表头</span></span><br><span class="line">header = table.row_values(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历表记录</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, rows):</span><br><span class="line">        <span class="comment"># 表记录值与对应的表头组成字典</span></span><br><span class="line">        col_dic = dict(zip(header, table.row_values(i)))</span><br><span class="line">        </span><br><span class="line">        raw_item = json.dumps(col_dic, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">yield</span> raw_item  <span class="comment"># 返回json格式</span></span><br></pre></td></tr></table></figure><p>&nbsp;</p><h1 id="2-PDF文件解析"><a href="#2-PDF文件解析" class="headerlink" title="2. PDF文件解析"></a>2. PDF文件解析</h1><p>PDF相对来说是比较难的解析，除非PDF中有类似Excel格式的数据，否则解析出来的数据很难进行规范化。Python中也有很多解析pdf文件的第三方库。比如tabula等</p><p>tabula的解析代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> tabula</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pdf_parse</span>(<span class="params">file_data</span>):</span></span><br><span class="line">    <span class="comment"># 提取PDF文件中的表格</span></span><br><span class="line">    tables = tabula.read_pdf(file_data, pages=<span class="string">&quot;all&quot;</span>)  <span class="comment"># file_data为BytesIO对象</span></span><br><span class="line">    <span class="comment"># 遍历提取出的所有表格</span></span><br><span class="line">    <span class="keyword">for</span> df <span class="keyword">in</span> tables:</span><br><span class="line">        <span class="comment"># 提取表头</span></span><br><span class="line">        columns = list(df.columns)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 按列切分表格记录</span></span><br><span class="line">        df_new = pd.DataFrame(columns=columns)</span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> columns:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment"># DataFrame转JSON</span></span><br><span class="line">        json_array = json.loads(df_new.to_json(orient=<span class="string">&quot;records&quot;</span>, force_ascii=<span class="literal">False</span>))</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> json_array:</span><br><span class="line">            <span class="keyword">yield</span> json.dumps(item, ensure_ascii=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>除了tabula之外，本人还是测试了很多其他的第三方模块，比如pdfminer.six、pdfminer3k、pdfplumber、PyPDF2、slate3k、camelot。要使用哪个模块，主要依赖于pdf文件内容的规范程度，总的来说推荐使用pdfplumber或者tabula。</p><p>&nbsp;</p><h1 id="3-HTML文件解析"><a href="#3-HTML文件解析" class="headerlink" title="3. HTML文件解析"></a>3. HTML文件解析</h1><p>HTML格式的文件相当于是半结构的数据，解析相对来说比较简单。</p><p>html文件的解析代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> lxml.etree <span class="keyword">import</span> HTML</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">html_parse</span>(<span class="params">file_data</span>):</span></span><br><span class="line">    <span class="comment"># 从一个字符串常量中解析HTML文档</span></span><br><span class="line">    html = HTML(file_data.read())</span><br><span class="line">    html_tables = html.findall(<span class="string">&quot;body/table&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将HTML中的所有&lt;table&gt;都放到一个列表中, 以便后续统一处理</span></span><br><span class="line">    table_rows = []</span><br><span class="line">    <span class="keyword">for</span> html_table <span class="keyword">in</span> html_tables:</span><br><span class="line">        table_rows.extend(list(html_table))</span><br><span class="line"></span><br><span class="line">    rows = len(table_rows)  <span class="comment"># 表格总行数</span></span><br><span class="line">    start_row = <span class="number">0</span>  <span class="comment"># 起始行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设第一行是表头</span></span><br><span class="line">    header = [col.text <span class="keyword">for</span> col <span class="keyword">in</span> table_rows[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历表记录</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(start_row, rows):</span><br><span class="line">        values = [col.text <span class="keyword">for</span> col <span class="keyword">in</span> table_rows[i]]</span><br><span class="line">        <span class="comment"># 表记录值与对应的列名组成字典</span></span><br><span class="line">        col_dic = dict(zip(header, values))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 字典转json</span></span><br><span class="line">        raw_item = json.dumps(col_dic, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">yield</span> raw_item</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>&nbsp;</p><h1 id="4-CSV文件解析"><a href="#4-CSV文件解析" class="headerlink" title="4. CSV文件解析"></a>4. CSV文件解析</h1><p>csv是默认以逗号(,)分割的一行行数据组成的文件。通常情况，csv文件都可以通过Excel软件打开，正常显示为表格。</p><p>csv文件解析代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">csv_parse</span>(<span class="params">file_data</span>):</span></span><br><span class="line">    file_bytes_obj = file_data.read()  <span class="comment"># file_data为BytesIO对象</span></span><br><span class="line"><span class="comment"># 根据文件内容检测编码格式有时候会不准确, 这时可以尝试更范围的编码格式</span></span><br><span class="line"><span class="comment"># 范围: gb18030 &gt; gb2312 &gt; gbk</span></span><br><span class="line">    encode = chardet.detect(file_bytes_obj)[<span class="string">&quot;encoding&quot;</span>]</span><br><span class="line"></span><br><span class="line">    file_string_io = StringIO(file_bytes_obj.decode(encode).replace(<span class="string">&#x27;\r&#x27;</span>, <span class="string">&#x27;\r\n&#x27;</span>))</span><br><span class="line">    csv_data = list(csv.reader(file_string_io))</span><br><span class="line"></span><br><span class="line">    start_row = <span class="number">-1</span>  <span class="comment"># 起始行</span></span><br><span class="line">    rows = len(csv_data)  <span class="comment"># 表格总行数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 表头</span></span><br><span class="line">    header = csv_data[i]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历表记录</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(start_row, rows):</span><br><span class="line">        col_dic = dict(zip(header, csv_data[i]))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 字典转json</span></span><br><span class="line">        raw_item = json.dumps(col_dic, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">yield</span> raw_item</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如有遇到其他格式的文件，之后会持续更新到这篇文章中！</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python读写Elasticsearch、HBase、Hive</title>
      <link href="2020/10/11/Python%E8%AF%BB%E5%86%99Elasticsearch%E3%80%81HBase%E3%80%81Hive/"/>
      <url>2020/10/11/Python%E8%AF%BB%E5%86%99Elasticsearch%E3%80%81HBase%E3%80%81Hive/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h2 id="1-Elasticsearch"><a href="#1-Elasticsearch" class="headerlink" title="1. Elasticsearch"></a>1. Elasticsearch</h2><p>安装Elasticsearch模块：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install elasticsearch==7.1.0</span><br></pre></td></tr></table></figure><p>读取ES索引的代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> elasticsearch <span class="keyword">import</span> Elasticsearch</span><br><span class="line"></span><br><span class="line">es = Elasticsearch(hosts=[&#123;<span class="string">&quot;host&quot;</span>: <span class="string">&quot;host&quot;</span>, <span class="string">&quot;port&quot;</span>: <span class="number">9200</span>&#125;])</span><br><span class="line">result = es.search(index=<span class="string">&quot;index_name&quot;</span>, body=&#123;<span class="string">&quot;query&quot;</span>: &#123;<span class="string">&quot;match_all&quot;</span>: &#123;&#125;&#125;&#125;)</span><br><span class="line"><span class="keyword">for</span> hit <span class="keyword">in</span> result[<span class="string">&#x27;hits&#x27;</span>][<span class="string">&#x27;hits&#x27;</span>]:</span><br><span class="line">    print(hit[<span class="string">&quot;_source&quot;</span>])</span><br></pre></td></tr></table></figure><h2 id="2-HBase"><a href="#2-HBase" class="headerlink" title="2. HBase"></a>2. HBase</h2><p>安装Python连接HBase的模块：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install happybase</span><br></pre></td></tr></table></figure><p>读取HBase表的代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> happybase</span><br><span class="line"></span><br><span class="line">connection = happybase.Connection(<span class="string">&#x27;host&#x27;</span>)</span><br><span class="line">connection.open()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印所有的表</span></span><br><span class="line">print(connection.tables())</span><br><span class="line"></span><br><span class="line">table = connection.table(<span class="string">&#x27;table_name&#x27;</span>)</span><br><span class="line">print(table.families())</span><br><span class="line"></span><br><span class="line">row = table.row(<span class="string">&#x27;row_key&#x27;</span>, columns=[<span class="string">b&#x27;cf:col1&#x27;</span>, <span class="string">b&#x27;cf:col2&#x27;</span>, <span class="string">b&#x27;cf:col3&#x27;</span>])</span><br><span class="line">print(bytes(row[<span class="string">b&#x27;cf:col1&#x27;</span>]).decode(<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line"></span><br><span class="line">connection.close()</span><br></pre></td></tr></table></figure><h2 id="3-Hive"><a href="#3-Hive" class="headerlink" title="3. Hive"></a>3. Hive</h2><p>安装Python连接Hive的模块：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install pyhive</span><br></pre></td></tr></table></figure><p>如果安装pyhive过程出错，请参考<a href="https://blog.csdn.net/lovetechlovelife/article/details/97128463">pyhive依赖模块sasl安装(pip install sasl)报错Microsoft Visual C++ 14.0 is required</a>。</p><p>读取Hive表的代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyhive <span class="keyword">import</span> hive</span><br><span class="line"></span><br><span class="line">conn = hive.Connection(host=<span class="string">&quot;master-1&quot;</span>, port=<span class="number">10000</span>, username=<span class="string">&quot;root&quot;</span>, auth=<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">cursor.execute(<span class="string">&quot;select * from db.table limit 10&quot;</span>)</span><br><span class="line">print(cursor.fetchall())</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
            <tag> Python </tag>
            
            <tag> Elasticsearch </tag>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pymysql + DBUtils实现数据库连接池及数据批量读写</title>
      <link href="2020/10/11/pymysql-DBUtils%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E6%B1%A0%E5%8F%8A%E6%95%B0%E6%8D%AE%E6%89%B9%E9%87%8F%E8%AF%BB%E5%86%99/"/>
      <url>2020/10/11/pymysql-DBUtils%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E6%B1%A0%E5%8F%8A%E6%95%B0%E6%8D%AE%E6%89%B9%E9%87%8F%E8%AF%BB%E5%86%99/</url>
      
        <content type="html"><![CDATA[<h2 id="1-安装pymysql、DBUtils"><a href="#1-安装pymysql、DBUtils" class="headerlink" title="1. 安装pymysql、DBUtils"></a>1. 安装pymysql、DBUtils</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple pymysql</span><br><span class="line">pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple DBUtils</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="2-创建连接池"><a href="#2-创建连接池" class="headerlink" title="2. 创建连接池"></a>2. 创建连接池</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> DBUtils.PooledDB <span class="keyword">import</span> PooledDB, PooledDBError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_pool</span>():</span></span><br><span class="line">pool = PooledDB(</span><br><span class="line">    creator=pymysql,  <span class="comment"># 数据库连接对象</span></span><br><span class="line">    host=<span class="string">&quot;host&quot;</span>,</span><br><span class="line">    user=<span class="string">&quot;user&quot;</span>,</span><br><span class="line">    password=<span class="string">&quot;password&quot;</span>,</span><br><span class="line">    database=<span class="string">&quot;database&quot;</span>,</span><br><span class="line">    charset=<span class="string">&quot;charset&quot;</span>,</span><br><span class="line">    blocking=<span class="literal">True</span>,  <span class="comment"># 超过最大连接数时该如何处理后来的任务。设置为true就表示阻塞等待直达有空闲连接</span></span><br><span class="line">    maxconnections=<span class="number">2</span>,  <span class="comment"># 连接池所能允许的最大连接数</span></span><br><span class="line">    mincached=<span class="number">1</span>,  <span class="comment"># 最小初始化空闲连接数</span></span><br><span class="line">    autocommit=<span class="literal">True</span>  <span class="comment"># 是否要自动提交</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">return</span> pool.connection()</span><br></pre></td></tr></table></figure><h2 id="3-读数据"><a href="#3-读数据" class="headerlink" title="3. 读数据"></a>3. 读数据</h2><h3 id="1-只读一行"><a href="#1-只读一行" class="headerlink" title="1. 只读一行"></a>1. 只读一行</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_one</span>(<span class="params">self, conn, sql</span>):</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line">        cursor.execute(sql)</span><br><span class="line">        <span class="keyword">return</span> cursor.fetchone()[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        conn.close()</span><br><span class="line">        cursor.close()</span><br></pre></td></tr></table></figure><h3 id="2-批量读取"><a href="#2-批量读取" class="headerlink" title="2. 批量读取"></a>2. 批量读取</h3><p>批量读取的场景，如果要返回的数据量比较大，为了避免给数据库造成太大负载和占用过多带宽资源，可以选择分批读取数据。比如要读取10万条数据，每批只读2000条。</p><p>思路：</p><ol><li>先通过一条SQL获取要读取的数据行数</li><li>每读取一批之后，自增主键减去2000</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">sql_count = <span class="string">&quot;select id from db.table where id between 25000 and 125000&quot;</span></span><br><span class="line">sql = <span class="string">&quot;select col1, col2, col3 from db.table&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_many</span>(<span class="params">self, conn, sql_count, sql, size</span>):</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算要读取的数据行数</span></span><br><span class="line">        cursor.execute(sql_count)</span><br><span class="line">        cursor.fetchall()</span><br><span class="line">        row_count = cursor.rowcount</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分配读取数据</span></span><br><span class="line">        cursor.execute(sql)</span><br><span class="line">        <span class="keyword">while</span> row_count &gt; <span class="number">0</span>:</span><br><span class="line">            records = cursor.fetchmany(size)</span><br><span class="line">            row_count -= size</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> records</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        conn.close()</span><br><span class="line">        cursor.close()</span><br></pre></td></tr></table></figure><h2 id="4-写数据"><a href="#4-写数据" class="headerlink" title="4. 写数据"></a>4. 写数据</h2><p>如果可以，尽量进行批量写，因为executemany()方法在多行插入和更新的场景下提升写性能。否则，就等于是循环执行execute()方法一条条的写，性能较差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">execute_many</span>(<span class="params">self, conn, sql, args</span>):</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 从连接池中获取一个连接</span></span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line">        cursor.executemany(sql, args)</span><br><span class="line">    <span class="keyword">except</span> PooledDBError <span class="keyword">as</span> e:</span><br><span class="line">        logger.error(<span class="string">&quot;Failed to insert records into MySQL table &#123;&#125;&quot;</span>.format(e))</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        conn.close()</span><br><span class="line">        cursor.close()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
            <tag> Python </tag>
            
            <tag> 连接池 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL SELECT查询非分组聚合列（sql_mode=only_full_group_by）</title>
      <link href="2020/10/11/MySQL-SELECT%E6%9F%A5%E8%AF%A2%E9%9D%9E%E5%88%86%E7%BB%84%E8%81%9A%E5%90%88%E5%88%97%EF%BC%88sql-mode-only-full-group-by%EF%BC%89/"/>
      <url>2020/10/11/MySQL-SELECT%E6%9F%A5%E8%AF%A2%E9%9D%9E%E5%88%86%E7%BB%84%E8%81%9A%E5%90%88%E5%88%97%EF%BC%88sql-mode-only-full-group-by%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>业务中有这样的逻辑：通过某一列col1分组之后查询col1，col2，col3这三个列，SQL语句如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> company_name, department, employee <span class="keyword">from</span> company <span class="keyword">group</span> <span class="keyword">by</span> company_name</span><br></pre></td></tr></table></figure><p>如果是MySQL 5.7以前的版本，是可以执行成功的；如果是MySQL 5.7及以上版本，就会报错：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR 1140 (42000): In aggregated query without GROUP BY, expression... of <span class="keyword">SELECT</span> <span class="keyword">list</span> contains nonaggregated <span class="keyword">column</span> <span class="string">&#x27;col_name&#x27;</span>; this is incompatible <span class="keyword">with</span> sql_mode=only_full_group_by</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h1><p>在标准的 SQL-92中，上述的查询是不被支持的。也就是说，select列表、having条件和order by列表中是不允许出现非聚合列的，所谓非聚合列就是group by子句中的列。换句话说，就是select中查询的列，必须出现在group by子句中，否则就必须在非聚合列上使用聚合函数（比如sum()、max()等）。</p><p>为什么不允许这样查询呢？主要是因为分组之后，要查询非聚合列，其实是不知道该取那个值的。</p><p>但是MySQL对标准的SQL-92做了扩展，使得select列表、having条件和order by列表中可以引用非聚合列，这个功能可以带来更好的性能，因为它需要对非聚合列进行分组排序。不过在分组之后，查询非聚合列时，其取值是随机的，也就是组内随机取一个，就算是先通过order by进行排序之后，也是随机取值的。</p><p>因为MySQL 5.7之前，配置项ONLY_FULL_GROUP_BY是默认禁用的，也就是支持在select列表、having条件和order by列表中引用非聚合列且不适用聚合函数；而MySQL 5.7及之后的版本中，配置项ONLY_FULL_GROUP_BY默认是开启的，则不支持上述功能。</p><p>可以通过下面SQL查看ONLY_FULL_GROUP_BY是否启用：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询的结果中包含ONLY_FULL_GROUP_BY就说明是启用的，否则就是被禁用的</span></span><br><span class="line"><span class="keyword">select</span> @@global.sql_mode;</span><br></pre></td></tr></table></figure><p>由于本人使用的是MySQL 5.7，所以在select中查询非聚合列就出现了上述的报错</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>了解了问题原因和原理之后，解决起来就比较简单了。</p><p>通过下面SQL将sql_mode中的ONLY_FULL_GROUP_BY替换成空即可：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> <span class="keyword">global</span> sql_mode=(<span class="keyword">select</span> <span class="keyword">replace</span>(@@sql_mode, <span class="string">&#x27;ONLY_FULL_GROUP_BY&#x27;</span>, <span class="string">&#x27;&#x27;</span>));</span><br></pre></td></tr></table></figure><p>当然也可以设置会话级别的：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> <span class="keyword">session</span> sql_mode=(<span class="keyword">select</span> <span class="keyword">replace</span>(@@sql_mode, <span class="string">&#x27;ONLY_FULL_GROUP_BY&#x27;</span>, <span class="string">&#x27;&#x27;</span>));</span><br></pre></td></tr></table></figure><p>或者是通过MySQL配置文件my.cnf进行修改（需重启mysql实例）。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://dev.mysql.com/doc/refman/5.6/en/group-by-handling.html">https://dev.mysql.com/doc/refman/5.6/en/group-by-handling.html</a></li><li><a href="https://dev.mysql.com/doc/refman/5.7/en/group-by-handling.html">https://dev.mysql.com/doc/refman/5.7/en/group-by-handling.html</a></li><li><a href="https://dev.mysql.com/doc/refman/5.7/en/sql-mode.html#sqlmode_only_full_group_by">https://dev.mysql.com/doc/refman/5.7/en/sql-mode.html#sqlmode_only_full_group_by</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python for循环中添加字典到列表，结果列表中全是循环中的最后一个值</title>
      <link href="2020/10/11/Python-for%E5%BE%AA%E7%8E%AF%E4%B8%AD%E6%B7%BB%E5%8A%A0%E5%AD%97%E5%85%B8%E5%88%B0%E5%88%97%E8%A1%A8%EF%BC%8C%E7%BB%93%E6%9E%9C%E5%88%97%E8%A1%A8%E4%B8%AD%E5%85%A8%E6%98%AF%E5%BE%AA%E7%8E%AF%E4%B8%AD%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AA%E5%80%BC/"/>
      <url>2020/10/11/Python-for%E5%BE%AA%E7%8E%AF%E4%B8%AD%E6%B7%BB%E5%8A%A0%E5%AD%97%E5%85%B8%E5%88%B0%E5%88%97%E8%A1%A8%EF%BC%8C%E7%BB%93%E6%9E%9C%E5%88%97%E8%A1%A8%E4%B8%AD%E5%85%A8%E6%98%AF%E5%BE%AA%E7%8E%AF%E4%B8%AD%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AA%E5%80%BC/</url>
      
        <content type="html"><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>问题的是在Python解析Excel文件出现的。</p><a id="more"></a><p>下面是出现问题的一段代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此函数实现了解析Excel文件，并将文件中的每行数据以字典的形式返回(key为列名，value为对应的值)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span>():</span></span><br><span class="line">...</span><br><span class="line"><span class="keyword">yield</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将解析返回的每行数据追加到一个列表中</span></span><br><span class="line">list = []</span><br><span class="line"><span class="keyword">for</span> dic <span class="keyword">in</span> func():</span><br><span class="line">list.append(dic)</span><br><span class="line"></span><br><span class="line">print(list)</span><br></pre></td></tr></table></figure><p>上面代码执行之后，输出的list中，发现所有元素的内容都是相同的，再去对比源文件，发现列表的元素都为Excel文件中最后一行内容。很明显，这肯定是不对的！</p><h1 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h1><p>出现这个问题之后，首先想到的就是逐步调试这段循环代码，同时观察dic的值和list中元素的值。</p><ul><li>第一次循环时，list中对应的确实是Excel中第一行的内容，</li><li>第二次循环时，在dic已改变，但还没往list中追加第二个元素之前，发现第一次追加到list中的的元素的值已经发生了改变，而且改变之后的内容就和第二次循环时dic的内容相同。</li></ul><p>到这里，就恍然大悟了，原来每次往列表中追加的都是这个字典的引用，而不是字典的内容，所以才会在每次循环时，出现只要dic的内容发生了变化，list中之前追加进去的dic也随着发生改变的情况。</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>知道了这个原因，我们就知道在追加列表时，针对Excel的每行数据，都应该追加不同的引用。</p><p>那么具体的实现就是用到字典的copy()方法，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">list = []</span><br><span class="line"><span class="keyword">for</span> dic <span class="keyword">in</span> func():</span><br><span class="line">list.append(dic.copy())</span><br></pre></td></tr></table></figure><p>copy()方法对字典dic进行了一个浅拷贝，也就是说对字典创建了一个新的引用。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://docs.python.org/3.7/library/copy.html">https://docs.python.org/3.7/library/copy.html</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Excel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL是如何通过变量实现组内排序的(MySQL 8.0之前版本，非row_number函数)</title>
      <link href="2020/10/11/MySQL%E6%98%AF%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E5%8F%98%E9%87%8F%E5%AE%9E%E7%8E%B0%E7%BB%84%E5%86%85%E6%8E%92%E5%BA%8F%E7%9A%84-MySQL-8-0%E4%B9%8B%E5%89%8D%E7%89%88%E6%9C%AC%EF%BC%8C%E9%9D%9Erow-number%E5%87%BD%E6%95%B0/"/>
      <url>2020/10/11/MySQL%E6%98%AF%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E5%8F%98%E9%87%8F%E5%AE%9E%E7%8E%B0%E7%BB%84%E5%86%85%E6%8E%92%E5%BA%8F%E7%9A%84-MySQL-8-0%E4%B9%8B%E5%89%8D%E7%89%88%E6%9C%AC%EF%BC%8C%E9%9D%9Erow-number%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>业务中要实现这样的逻辑：有一张用户表，每个用户有一个或多个银行账户，每个账户有对应的流水数据，现在想要获取每个用户的每个账户下，交易金额最大的前10条流水数据，该如何实现？</p><a id="more"></a><h1 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h1><p>组内排序取Top N在业务当中是一个会经常用到的功能。如果你是在大数据框架中取组内Top N，一般的大数据框架都提供了现成的函数，比如Hive中的row_number() + partitionBy()，实现起来会比较简单。而如果你用的是关系型数据库，比如MySQL，就没有类似现成的函数来实现这个功能了(MySQL 8.0开始也提供row_number函数，但是很多公司都还是使用的MySQL5.6/5.7)。</p><p>那么在MySQL 8.0 之前如何实现组内排序呢？答案就是，借助变量。</p><p>表(bank_transaction)中部分列：<br>|列名| 类型 | 含义 |<br>|–|–|–|<br>| user_id | varchar(64) | 用户ID |<br>| bank_account | varchar(64) | 银行账户 |<br>| transaction_amount | decimal(17, 2) | 交易金额 |</p><p>SQL实现组内求Top N：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">user_id,</span><br><span class="line">bank_account,</span><br><span class="line">transaction_amount</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">user_id,</span><br><span class="line">bank_account,</span><br><span class="line">transaction_amount,</span><br><span class="line">@row_num := <span class="keyword">if</span>(@group01 = user_id <span class="keyword">and</span> @group02 = bank_account, @row_num + <span class="number">1</span>, <span class="number">1</span>) <span class="keyword">as</span> rn,</span><br><span class="line">@group01 := user_id,</span><br><span class="line">@group02 := bank_account</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">user_id,</span><br><span class="line">bank_account,</span><br><span class="line">transaction_amount</span><br><span class="line"><span class="keyword">from</span> bank_transaction </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> user_id, bank_account, transaction_amount <span class="keyword">desc</span></span><br><span class="line">) t1, (<span class="keyword">select</span> @row_num := <span class="number">0</span>, @group01 := <span class="string">&quot;&quot;</span>, @group02 := <span class="string">&quot;&quot;</span>)t2</span><br><span class="line">) t</span><br><span class="line"><span class="keyword">where</span> t.rn &lt;= <span class="number">6</span></span><br></pre></td></tr></table></figure><p>实现思路：</p><ol><li>第一步：对数据进行组内排序，t1这个子查询里面就是对每个用户的每个账户下的所有交易金额进行降序排列</li><li>第二步：设置变量，@row_num变量保存的是组内排序时的编号，@group01和@group02保存的是要分组列的临时值</li><li>第三步：遍历已排序数据，每次拿后一行与前一行进行比较，如果后一行分组列的值和前一行的相等（对应上面SQL语句中的if条件），就表明后一行与前一行属于同一个组，那么变量@row_num的值就加一；如果不相等，就从1重新开始。以此，给所有数据编号。</li><li>第四步：给所有数据编号之后，通过where条件即可取出Top N的记录。</li></ol><p>另外，SQL也可以这样写，主要是变量的使用方式不同：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> @row_num := <span class="number">0</span>, @group01 := <span class="string">&quot;&quot;</span>, @group02 := <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">user_id,</span><br><span class="line">bank_account,</span><br><span class="line">transaction_amount</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">user_id,</span><br><span class="line">bank_account,</span><br><span class="line">transaction_amount,</span><br><span class="line">@row_num := <span class="keyword">if</span>(@group01 = user_id <span class="keyword">and</span> @group02 = bank_account, @row_num + <span class="number">1</span>, <span class="number">1</span>) <span class="keyword">as</span> rn,</span><br><span class="line">@group01 := user_id,</span><br><span class="line">@group02 := bank_account</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">user_id,</span><br><span class="line">bank_account,</span><br><span class="line">transaction_amount</span><br><span class="line"><span class="keyword">from</span> bank_transaction </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> user_id, bank_account, transaction_amount <span class="keyword">desc</span></span><br><span class="line">) t1</span><br><span class="line">) t</span><br><span class="line"><span class="keyword">where</span> t.rn &lt;= <span class="number">6</span></span><br></pre></td></tr></table></figure><p>Hive中如何实现组内排序求Top N，可以参考<a href="https://editor.csdn.net/md/?articleId=107349381">这里</a>。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>本地Spark连接远程集群Hive(Scala/Python)</title>
      <link href="2020/10/11/%E6%9C%AC%E5%9C%B0Spark%E8%BF%9E%E6%8E%A5%E8%BF%9C%E7%A8%8B%E9%9B%86%E7%BE%A4Hive-Scala-Python/"/>
      <url>2020/10/11/%E6%9C%AC%E5%9C%B0Spark%E8%BF%9E%E6%8E%A5%E8%BF%9C%E7%A8%8B%E9%9B%86%E7%BE%A4Hive-Scala-Python/</url>
      
        <content type="html"><![CDATA[<h1 id="为什么要在本地使用Spark连接Hive？"><a href="#为什么要在本地使用Spark连接Hive？" class="headerlink" title="为什么要在本地使用Spark连接Hive？"></a>为什么要在本地使用Spark连接Hive？</h1><p>很多时候，我们在编写好Spark应用程序之后，想要读取Hive表测试一下，但又不想进行打jar包、上传集群、spark-submit这一系列麻烦的操作时，那我们就可以在本地运行Spark应用的main函数直接读取远程的Hive。</p><a id="more"></a><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p>Spark版本：2.0+</p><h2 id="1-Scala实现"><a href="#1-Scala实现" class="headerlink" title="1. Scala实现"></a>1. Scala实现</h2><p>代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object SparkDemo &#123;</span><br><span class="line">val spark = SparkSession</span><br><span class="line">.builder()</span><br><span class="line">.appName(<span class="string">&quot;Spark to Hive&quot;</span>)</span><br><span class="line">.master(<span class="string">&quot;local[4]&quot;</span>)</span><br><span class="line">.config(<span class="string">&quot;hive.metastore.uris&quot;</span>, <span class="string">&quot;thrift://bigdata01:9083,thrift://bigdata02:9083&quot;</span>)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line">val df = spark.read.table(<span class="string">&quot;db.table&quot;</span>)</span><br><span class="line">df.show(<span class="keyword">false</span>)</span><br><span class="line"></span><br><span class="line">spark.close()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码中在创建SparkSession实例时的配置项 “hive.metastore.uris” 的值以个人情况而定，具体配置对应的是你集群中Hive安装目录下的这个文件 ../conf/hive-site.xml 中的 hive.metastore.uris，这个配置表示Hive的Metastore Server所在的节点。</p><p><font color=Red>注意</font>：一般来讲，bigdata01，bigdata02都是集群中节点的别名，并不是具体的IP地址，所以你还需要把集群某个节点下文件 /etc/hosts 中的配置拷贝到你Windows电脑(本人用的Windows)的这个 C:\Windows\System32\drivers\etc\hosts 文件中，这样才能在本地访问集群Hive的元数据服务器节点。具体配置就长下面这个样子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">10.1.11.10           bigdata01</span><br><span class="line">10.1.11.11           bigdata02</span><br><span class="line">10.1.11.12           bigdata03</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="2-pyspark实现"><a href="#2-pyspark实现" class="headerlink" title="2. pyspark实现"></a>2. pyspark实现</h2><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&quot;SPARK_HOME&quot;</span>] = <span class="string">&quot;F:\App\spark-2.3.1-bin-hadoop2.6&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">spark = SparkSession\</span><br><span class="line">        .builder \</span><br><span class="line">        .appName(<span class="string">&quot;Spark to Hive&quot;</span>) \</span><br><span class="line">        .master(<span class="string">&quot;local[4]&quot;</span>) \</span><br><span class="line">        .config(<span class="string">&quot;hive.metastore.uris&quot;</span>, <span class="string">&quot;thrift://bigdata01:9083,thrift://bigdata02:9083&quot;</span>) \</span><br><span class="line">        .enableHiveSupport()\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">df = spark.read.table(<span class="string">&quot;db.table&quot;</span>)</span><br><span class="line">df.show(truncate=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure><p>pyspark版本跟Scala不同的地方在于，pyspark运行时会依赖Spark的一些文件，因此需要设置环境变量SPARK_HOME，你可以到<a href="http://spark.apache.org/downloads.html">Spark官网</a>下载相应版本的Spark，文件名称spark-2.3.3-bin-hadoop2.6.tgz；下载之后，解压到指定目录下；最后，配置环境变量SPARK_HOME的值为之前解压文件的根路径即可。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark中一行代码转换DataFrame所有列的类型</title>
      <link href="2020/10/10/Spark%E4%B8%AD%E4%B8%80%E8%A1%8C%E4%BB%A3%E7%A0%81%E8%BD%AC%E5%8C%96DataFrame%E6%89%80%E6%9C%89%E5%88%97%E7%9A%84%E7%B1%BB%E5%9E%8B/"/>
      <url>2020/10/10/Spark%E4%B8%AD%E4%B8%80%E8%A1%8C%E4%BB%A3%E7%A0%81%E8%BD%AC%E5%8C%96DataFrame%E6%89%80%E6%9C%89%E5%88%97%E7%9A%84%E7%B1%BB%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>当需要把DataFrame中所有列的类型转换成另外一种类型，并且这个DataFrame中字段很多，一个一个地转换要写很多冗余代码，那么就可以使用如下这两种转换方式。</p><a id="more"></a><h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><h2 id="1-foldLeft-函数"><a href="#1-foldLeft-函数" class="headerlink" title="1. foldLeft() 函数"></a>1. foldLeft() 函数</h2><p>代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.DataFrame</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.col</span><br><span class="line"></span><br><span class="line">val df: DataFrame = ...</span><br><span class="line">val columns: Array[String] = df.columns</span><br><span class="line">val df2: DataFrame = columns.foldLeft(df)&#123;(currentDF, column) =&gt; currentDF.withColumn(column, col(column).cast(<span class="string">&quot;string&quot;</span>))&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>变量columns</strong>：是一个String类型的数组，数组中的元素为df中的列名。</li><li><strong>foldLeft函数</strong>：此函数以df为初始值，从左向右遍历columns数组，并把df的每一行和columns的每个元素作为参数传入foldLeft后面的函数中(也就是foldLeft后面的大括号中)。</li><li>**withColumn()**：将每一列转换成String类型并赋值给当前列。如果存在同名的列，withColumn函数默认会进行覆盖。</li></ul><h2 id="2-map-函数"><a href="#2-map-函数" class="headerlink" title="2. map() 函数"></a>2. map() 函数</h2><p>代码如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;Column, DataFrame&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.col</span><br><span class="line"></span><br><span class="line">val df: DataFrame = ...</span><br><span class="line">val columns: Array[String] = df.columns</span><br><span class="line">val arrayColumn: Array[Column] = columns.map(column =&gt; col(column).cast(<span class="string">&quot;string&quot;</span>))</span><br><span class="line">val df2: DataFrame = df.select(arrayColumn :_*)</span><br></pre></td></tr></table></figure><p>通过map函数将columns中的每一列转换成String类型，并返回一个Column类型的数组，然后，将arrayColumn数组中的每个元素作为参数传入select函数中，就相当于df.select(col1, col2, col3, …)。</p><p>除此之外，这种写法还有一个很有用的场景：比如要在一个DataFrame中select出很多列(假如有几十个几百个)，如果要一个个显示写出来，既不方便又会让代码显得很冗余，那么就可以使用这种写法。</p><ol><li>先通过df.columns得到这个DataFrame中的所有列，返回一个包含所有列的数组；</li><li>再使用Scala中的这种语法进行查询df.select(arrayColumn :_*)，非常简洁明了。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL行记录大小超过限制 Row size too large</title>
      <link href="2020/10/10/MySQL%E8%A1%8C%E8%AE%B0%E5%BD%95%E5%A4%A7%E5%B0%8F%E8%B6%85%E8%BF%87%E9%99%90%E5%88%B6%20Row%20size%20too%20large/"/>
      <url>2020/10/10/MySQL%E8%A1%8C%E8%AE%B0%E5%BD%95%E5%A4%A7%E5%B0%8F%E8%B6%85%E8%BF%87%E9%99%90%E5%88%B6%20Row%20size%20too%20large/</url>
      
        <content type="html"><![CDATA[<h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>问题出现在Spark写MySQL的场景：要写入MySQL的DataFrame中有90多个列，其中有10多个列为字符串类型，且长度较长(大于1000)；对应的要写入的MySQL表使用的是InnoDB引擎，这些较大的字符串所对应的列在MySQL中设置为text类型。最终在写MySQL的时候，出现这样的报错：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: <span class="function">Row size too <span class="title">large</span> <span class="params">(&gt; <span class="number">8126</span>)</span>. Changing some columns to TEXT or BLOB or using ROW_FORMAT</span>=DYNAMIC or ROW_FORMAT=COMPRESSED may help. In current row format, BLOB prefix of <span class="number">768</span> bytes is stored inline.</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="MySQL版本"><a href="#MySQL版本" class="headerlink" title="MySQL版本"></a>MySQL版本</h1><p>本人使用的MySQL版本为5.6、InnoDB引擎，以下内容也是以这个前提来展开的，MyISAM暂不做介绍。</p><p>注：</p><ul><li>MySQL 5.6版本默认InnoDB文件格式为Antelope，相应配置innodb_file_format=Antelope，此种文件格式不支持COMPRESSED和DYNAMIC的行格式。</li><li>MySQL 5.7版本默认InnoDB文件格式为Barracuda，相应配置innodb_file_format=Barracuda，此种文件格式支持COMPRESSED和DYNAMIC的行格式。</li></ul><h1 id="名词术语"><a href="#名词术语" class="headerlink" title="名词术语"></a>名词术语</h1><p>要理解上面报错的本质，首先要了解以下概念：</p><ol><li>MySQL中的可变长度类型</li><li>页(page)、页大小(page size)、off-page column、overflow page？</li><li>页（Page）、行大小、行格式三种之间的关系</li></ol><h2 id="1-可变长度类型"><a href="#1-可变长度类型" class="headerlink" title="1. 可变长度类型"></a>1. 可变长度类型</h2><p>MySQL中的可变长度类型：VARCHAR、VARBINARY、BLOB和TEXT类型。</p><p>InnoDB将长度大于等于768字节的fixed-length字段当作可变长度字段，可以存储在off-page。</p><h2 id="2-页-page-、页大小-Page-size-、off-page-column、overflow-page"><a href="#2-页-page-、页大小-Page-size-、off-page-column、overflow-page" class="headerlink" title="2. 页(page)、页大小(Page size)、off-page column、overflow page"></a>2. 页(page)、页大小(Page size)、off-page column、overflow page</h2><h3 id="i-页-Page"><a href="#i-页-Page" class="headerlink" title="i. 页(Page)"></a>i. 页(Page)</h3><p>page代表InnoDB每次在磁盘和内存之间传输多少数据的一个单元。一个page可以包含一行或多行数据，这主要取决于每行数据的大小。如果一行记录不能全部放入到一个page中，InnoDB会用一个指针来引用这行数据。</p><p>可以使用COMPRESSED格式来使每个page容纳更多的数据。对于blob或者text类型的字段，COMPACT格式允许大长度的列和其他列分开存储，以便减少查询时的I/O负载和内存占用。</p><p>当InnoDB以批处理的方式读写一组page以增加I/O吞吐量时，它会一次读写一个区段的page。</p><h3 id="ii-页大小-Page-size"><a href="#ii-页大小-Page-size" class="headerlink" title="ii. 页大小(Page size)"></a>ii. 页大小(Page size)</h3><p>在MySQL 5.6版本之前，每个InnoDB page的大小都是固定的16KB，这是一个各方面取舍平衡的值：16KB能足以容纳大多数的行数据，同时也足够小到可以最小化将不必要的数据传输到内存的性能开销。</p><p>从MySQL 5.6开始，InnoDB page的大小可以是4KB、8KB或16KB，可通过innodb_page_size配置进行设置。在MySQL5.7.6中，InnoDB支持更大的page size(32KB和64KB)，但是这两种page size并不支持ROW_FORMAT=COMPRESSED， 并且最大记录大小为16KB。</p><h3 id="iii-off-page-column"><a href="#iii-off-page-column" class="headerlink" title="iii. off-page column"></a>iii. off-page column</h3><p>一个可变长度列(比如BLOB和VARCHAR)中的数据因为太大而不能放入一个B-tree page中，那么数据就会存储在overflow pages中。</p><h3 id="iiii-overflow-page"><a href="#iiii-overflow-page" class="headerlink" title="iiii. overflow page"></a>iiii. overflow page</h3><p>专门分配的磁盘pages，用来存储那些因为数据太长而不能放入B-tree page的可变长度列，这些可变长度列就是上面提到的off-page column。</p><h2 id="3-行格式"><a href="#3-行格式" class="headerlink" title="3. 行格式"></a>3. 行格式</h2><p>表的行格式决定了表中行是如何在物理层面上被存储的，这反过来又会影响增删查改操作的性能。当越多的行能被存储在单个page中时，那查询操作和索引的查找都会更高效，buffer pool就需要越少的缓存，更新操作就需要越少的I/O。</p><p>每个表中的数据都是被划分为很多个page的，这些page都是保存在B-tree这种数据结构中的，表中的数据和二级索引都是使用的这种数据结构。</p><p>长度较长的可变长度列由于无法存储到单个B-tree page中，只能存储到单独分配的磁盘页(overflow pagess)上。这些列也被称为off-page column。off-page columns的值存储在overflow pages的单链表中，而且每一列都有自己的列表，从这个列表中可以知道这一列的值都存储在哪些overflow page中。根据列长度的不同，会将变长列的全部值或前缀存储在B-tree中，这样就能避免page的浪费，也避免了要读取多个page的情况。</p><p>MySQL中常用的InnoDB引擎支持4中行格式：</p><ol><li>REDUNDANT</li><li>COMPACT </li><li>DYNAMIC</li><li>COMPRESSED</li></ol><p>更多关于InnoDB Row Formats的细节，参考<a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-row-format.html">这里</a>。</p><h2 id="4-页（Page）、行大小、行格式三种之间的关系"><a href="#4-页（Page）、行大小、行格式三种之间的关系" class="headerlink" title="4. 页（Page）、行大小、行格式三种之间的关系"></a>4. 页（Page）、行大小、行格式三种之间的关系</h2><p>MySQL表中行的最大长度被限制为65535字节，即使使用的存储引擎能够支持更大的行，也不能超过这个限制。</p><p>表中行的最大长度略少于数据库page大小的一半，例如，对于默认的InnoDB page大小16KB，所对应的行最大长度为略小于8KB，这个值是通过配置项innodb_page_size来设定的。</p><p>如果表中一行没有超过半个page的限制，那么整行数据都是存储在page中的；如果超过了半个page大小，那么对于可变长度列，超过限制的数据会被存储在外部off-page storage(就是上面提到的overflow page)。</p><p>而可变长度列是如何存储在off-page storage中的，又跟行格式的不同而不同：</p><ul><li>COMPACT 和 REDUNDANT行格式<br>在使用这两种行格式的情况下，当一个可变长度列被存储到外部的off-page storage中时，InnoDB引擎会把这一列的前768个字节存储在page中，剩下的数据存储在overflow pages中。每一个存储在overflow pages中的可变长度列都有一个自己的overflow pages列表。这768个字节中，有20字节用来存储这个列的真实长度和指向包含指向overflow list的指针。</li><li>DYNAMIC和COMPRESSED行格式<br>在使用这两种行格式的情况下，当一个可变长度列被存储到外部的off-page storage中时，InnoDB引擎会在page中存储一个20字节的指针，列中的剩余数据会全部存储到overflow pages中。</li></ul><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>在使用InnoDB建表时，默认的行格式为COMPACT（可通过show variables like “table_name”查看），这种行格式对应的默认page大小为16KB，那么相应每行的大小不能超过8KB。如果表中有20个列都为text类型，而且每个text类型列的值都超过了768字节，那么20 * 768字节=15360字节=15KB远大于8KB，所以必然会报错！<br>那么解决这个问题的方法就是修改行格式，以下是启用DYNAMIC行格式的步骤：</p><ol><li><p>首先是MySQL配置文件my.cnf中添加两个配置项：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">innodb_file_per_table=1 //</span><br><span class="line">innodb_file_format = Barracuda //DYNAMIC行格式只有在Barracuda文件格式下才支持</span><br></pre></td></tr></table></figure></li><li><p>修改表行格式ROW_FORMAT</p> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name ROW_FORMAT=DYNAMIC;</span><br></pre></td></tr></table></figure><p>修改之后，执行 show table status like ‘table_name’，可以看到Row_format这一列对应的值已经变成了dynamic，再写入数据的时候就不会报错了。</p></li></ol><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://dev.mysql.com/doc/mysql-reslimits-excerpt/5.7/en/column-count-limit.html">https://dev.mysql.com/doc/mysql-reslimits-excerpt/5.7/en/column-count-limit.html</a></li><li><a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-row-format.html">https://dev.mysql.com/doc/refman/5.7/en/innodb-row-format.html</a></li><li><a href="https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_dynamic_row_format">https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_dynamic_row_format</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
