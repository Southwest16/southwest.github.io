<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>15分钟搞定 Spark集群安装</title>
    <url>/2020/10/11/15%E5%88%86%E9%92%9F%E6%90%9E%E5%AE%9A-Spark%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Flink和Spark Streaming背压</title>
    <url>/2020/10/11/Flink%E5%92%8CSpark-Streaming%E8%83%8C%E5%8E%8B/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>CDH集群安装</title>
    <url>/2020/10/11/CDH%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Hadoop集群数据迁移</title>
    <url>/2020/10/11/Hadoop%E9%9B%86%E7%BE%A4%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Hive/Spark SQL常用函数(窗口分析函数、行列转换、JSON处理)</title>
    <url>/2020/10/11/Hive-Spark-SQL%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3%E5%88%86%E6%9E%90%E5%87%BD%E6%95%B0%E3%80%81%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2%E3%80%81JSON%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>JSON数据转成Spark Dataset/DataFrame</title>
    <url>/2020/10/11/JSON%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%88%90Spark-Dataset-DataFrame/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Linux Sed命令使用指南</title>
    <url>/2020/10/11/Linux-Sed%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Impala与Hive的语法区别(持续更新中...)</title>
    <url>/2020/10/11/Impala%E4%B8%8EHive%E7%9A%84%E8%AF%AD%E6%B3%95%E5%8C%BA%E5%88%AB-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E4%B8%AD/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Java死锁产生的条件及死锁判断排查</title>
    <url>/2020/10/11/Java%E6%AD%BB%E9%94%81%E4%BA%A7%E7%94%9F%E7%9A%84%E6%9D%A1%E4%BB%B6%E5%8F%8A%E6%AD%BB%E9%94%81%E5%88%A4%E6%96%AD%E6%8E%92%E6%9F%A5/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>MySQL SELECT查询非分组聚合列（sql_mode=only_full_group_by）</title>
    <url>/2020/10/11/MySQL-SELECT%E6%9F%A5%E8%AF%A2%E9%9D%9E%E5%88%86%E7%BB%84%E8%81%9A%E5%90%88%E5%88%97%EF%BC%88sql-mode-only-full-group-by%EF%BC%89/</url>
    <content><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>业务中有这样的逻辑：通过某一列col1分组之后查询col1，col2，col3这三个列，SQL语句如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> company_name, department, employee <span class="keyword">from</span> company <span class="keyword">group</span> <span class="keyword">by</span> company_name</span><br></pre></td></tr></table></figure>
<p>如果是MySQL 5.7以前的版本，是可以执行成功的；如果是MySQL 5.7及以上版本，就会报错：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ERROR 1140 (42000): In aggregated query without GROUP BY, expression... of <span class="keyword">SELECT</span> <span class="keyword">list</span> contains nonaggregated <span class="keyword">column</span> <span class="string">&#x27;col_name&#x27;</span>; this is incompatible <span class="keyword">with</span> sql_mode=only_full_group_by</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h1 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h1><p>在标准的 SQL-92中，上述的查询是不被支持的。也就是说，select列表、having条件和order by列表中是不允许出现非聚合列的，所谓非聚合列就是group by子句中的列。换句话说，就是select中查询的列，必须出现在group by子句中，否则就必须在非聚合列上使用聚合函数（比如sum()、max()等）。</p>
<p>为什么不允许这样查询呢？主要是因为分组之后，要查询非聚合列，其实是不知道该取那个值的。</p>
<p>但是MySQL对标准的SQL-92做了扩展，使得select列表、having条件和order by列表中可以引用非聚合列，这个功能可以带来更好的性能，因为它需要对非聚合列进行分组排序。不过在分组之后，查询非聚合列时，其取值是随机的，也就是组内随机取一个，就算是先通过order by进行排序之后，也是随机取值的。</p>
<p>因为MySQL 5.7之前，配置项ONLY_FULL_GROUP_BY是默认禁用的，也就是支持在select列表、having条件和order by列表中引用非聚合列且不适用聚合函数；而MySQL 5.7及之后的版本中，配置项ONLY_FULL_GROUP_BY默认是开启的，则不支持上述功能。</p>
<p>可以通过下面SQL查看ONLY_FULL_GROUP_BY是否启用：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询的结果中包含ONLY_FULL_GROUP_BY就说明是启用的，否则就是被禁用的</span></span><br><span class="line"><span class="keyword">select</span> @@global.sql_mode;</span><br></pre></td></tr></table></figure>

<p>由于本人使用的是MySQL 5.7，所以在select中查询非聚合列就出现了上述的报错</p>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>了解了问题原因和原理之后，解决起来就比较简单了。</p>
<p>通过下面SQL将sql_mode中的ONLY_FULL_GROUP_BY替换成空即可：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> <span class="keyword">global</span> sql_mode=(<span class="keyword">select</span> <span class="keyword">replace</span>(@@sql_mode, <span class="string">&#x27;ONLY_FULL_GROUP_BY&#x27;</span>, <span class="string">&#x27;&#x27;</span>));</span><br></pre></td></tr></table></figure>

<p>当然也可以设置会话级别的：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> <span class="keyword">session</span> sql_mode=(<span class="keyword">select</span> <span class="keyword">replace</span>(@@sql_mode, <span class="string">&#x27;ONLY_FULL_GROUP_BY&#x27;</span>, <span class="string">&#x27;&#x27;</span>));</span><br></pre></td></tr></table></figure>
<p>或者是通过MySQL配置文件my.cnf进行修改（需重启mysql实例）。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="https://dev.mysql.com/doc/refman/5.6/en/group-by-handling.html">https://dev.mysql.com/doc/refman/5.6/en/group-by-handling.html</a></li>
<li><a href="https://dev.mysql.com/doc/refman/5.7/en/group-by-handling.html">https://dev.mysql.com/doc/refman/5.7/en/group-by-handling.html</a></li>
<li><a href="https://dev.mysql.com/doc/refman/5.7/en/sql-mode.html#sqlmode_only_full_group_by">https://dev.mysql.com/doc/refman/5.7/en/sql-mode.html#sqlmode_only_full_group_by</a></li>
</ol>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL行记录大小超过限制 Row size too large</title>
    <url>/2020/10/10/MySQL%E8%A1%8C%E8%AE%B0%E5%BD%95%E5%A4%A7%E5%B0%8F%E8%B6%85%E8%BF%87%E9%99%90%E5%88%B6%20Row%20size%20too%20large/</url>
    <content><![CDATA[<h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>问题出现在Spark写MySQL的场景：要写入MySQL的DataFrame中有90多个列，其中有10多个列为字符串类型，且长度较长(大于1000)；对应的要写入的MySQL表使用的是InnoDB引擎，这些较大的字符串所对应的列在MySQL中设置为text类型。最终在写MySQL的时候，出现这样的报错：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: <span class="function">Row size too <span class="title">large</span> <span class="params">(&gt; <span class="number">8126</span>)</span>. Changing some columns to TEXT or BLOB or using ROW_FORMAT</span>=DYNAMIC or ROW_FORMAT=COMPRESSED may help. In current row format, BLOB prefix of <span class="number">768</span> bytes is stored inline.</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h1 id="MySQL版本"><a href="#MySQL版本" class="headerlink" title="MySQL版本"></a>MySQL版本</h1><p>本人使用的MySQL版本为5.6、InnoDB引擎，以下内容也是以这个前提来展开的，MyISAM暂不做介绍。</p>
<p>注：</p>
<ul>
<li>MySQL 5.6版本默认InnoDB文件格式为Antelope，相应配置innodb_file_format=Antelope，此种文件格式不支持COMPRESSED和DYNAMIC的行格式。</li>
<li>MySQL 5.7版本默认InnoDB文件格式为Barracuda，相应配置innodb_file_format=Barracuda，此种文件格式支持COMPRESSED和DYNAMIC的行格式。</li>
</ul>
<h1 id="名词术语"><a href="#名词术语" class="headerlink" title="名词术语"></a>名词术语</h1><p>要理解上面报错的本质，首先要了解以下概念：</p>
<ol>
<li>MySQL中的可变长度类型</li>
<li>页(page)、页大小(page size)、off-page column、overflow page？</li>
<li>页（Page）、行大小、行格式三种之间的关系</li>
</ol>
<h2 id="1-可变长度类型"><a href="#1-可变长度类型" class="headerlink" title="1. 可变长度类型"></a>1. 可变长度类型</h2><p>MySQL中的可变长度类型：VARCHAR、VARBINARY、BLOB和TEXT类型。</p>
<p>InnoDB将长度大于等于768字节的fixed-length字段当作可变长度字段，可以存储在off-page。</p>
<h2 id="2-页-page-、页大小-Page-size-、off-page-column、overflow-page"><a href="#2-页-page-、页大小-Page-size-、off-page-column、overflow-page" class="headerlink" title="2. 页(page)、页大小(Page size)、off-page column、overflow page"></a>2. 页(page)、页大小(Page size)、off-page column、overflow page</h2><h3 id="i-页-Page"><a href="#i-页-Page" class="headerlink" title="i. 页(Page)"></a>i. 页(Page)</h3><p>page代表InnoDB每次在磁盘和内存之间传输多少数据的一个单元。一个page可以包含一行或多行数据，这主要取决于每行数据的大小。如果一行记录不能全部放入到一个page中，InnoDB会用一个指针来引用这行数据。</p>
<p>可以使用COMPRESSED格式来使每个page容纳更多的数据。对于blob或者text类型的字段，COMPACT格式允许大长度的列和其他列分开存储，以便减少查询时的I/O负载和内存占用。</p>
<p>当InnoDB以批处理的方式读写一组page以增加I/O吞吐量时，它会一次读写一个区段的page。</p>
<h3 id="ii-页大小-Page-size"><a href="#ii-页大小-Page-size" class="headerlink" title="ii. 页大小(Page size)"></a>ii. 页大小(Page size)</h3><p>在MySQL 5.6版本之前，每个InnoDB page的大小都是固定的16KB，这是一个各方面取舍平衡的值：16KB能足以容纳大多数的行数据，同时也足够小到可以最小化将不必要的数据传输到内存的性能开销。</p>
<p>从MySQL 5.6开始，InnoDB page的大小可以是4KB、8KB或16KB，可通过innodb_page_size配置进行设置。在MySQL5.7.6中，InnoDB支持更大的page size(32KB和64KB)，但是这两种page size并不支持ROW_FORMAT=COMPRESSED， 并且最大记录大小为16KB。</p>
<h3 id="iii-off-page-column"><a href="#iii-off-page-column" class="headerlink" title="iii. off-page column"></a>iii. off-page column</h3><p>一个可变长度列(比如BLOB和VARCHAR)中的数据因为太大而不能放入一个B-tree page中，那么数据就会存储在overflow pages中。</p>
<h3 id="iiii-overflow-page"><a href="#iiii-overflow-page" class="headerlink" title="iiii. overflow page"></a>iiii. overflow page</h3><p>专门分配的磁盘pages，用来存储那些因为数据太长而不能放入B-tree page的可变长度列，这些可变长度列就是上面提到的off-page column。</p>
<h2 id="3-行格式"><a href="#3-行格式" class="headerlink" title="3. 行格式"></a>3. 行格式</h2><p>表的行格式决定了表中行是如何在物理层面上被存储的，这反过来又会影响增删查改操作的性能。当越多的行能被存储在单个page中时，那查询操作和索引的查找都会更高效，buffer pool就需要越少的缓存，更新操作就需要越少的I/O。</p>
<p>每个表中的数据都是被划分为很多个page的，这些page都是保存在B-tree这种数据结构中的，表中的数据和二级索引都是使用的这种数据结构。</p>
<p>长度较长的可变长度列由于无法存储到单个B-tree page中，只能存储到单独分配的磁盘页(overflow pagess)上。这些列也被称为off-page column。off-page columns的值存储在overflow pages的单链表中，而且每一列都有自己的列表，从这个列表中可以知道这一列的值都存储在哪些overflow page中。根据列长度的不同，会将变长列的全部值或前缀存储在B-tree中，这样就能避免page的浪费，也避免了要读取多个page的情况。</p>
<p>MySQL中常用的InnoDB引擎支持4中行格式：</p>
<ol>
<li>REDUNDANT</li>
<li>COMPACT </li>
<li>DYNAMIC</li>
<li>COMPRESSED</li>
</ol>
<p>更多关于InnoDB Row Formats的细节，参考<a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-row-format.html">这里</a>。</p>
<h2 id="4-页（Page）、行大小、行格式三种之间的关系"><a href="#4-页（Page）、行大小、行格式三种之间的关系" class="headerlink" title="4. 页（Page）、行大小、行格式三种之间的关系"></a>4. 页（Page）、行大小、行格式三种之间的关系</h2><p>MySQL表中行的最大长度被限制为65535字节，即使使用的存储引擎能够支持更大的行，也不能超过这个限制。</p>
<p>表中行的最大长度略少于数据库page大小的一半，例如，对于默认的InnoDB page大小16KB，所对应的行最大长度为略小于8KB，这个值是通过配置项innodb_page_size来设定的。</p>
<p>如果表中一行没有超过半个page的限制，那么整行数据都是存储在page中的；如果超过了半个page大小，那么对于可变长度列，超过限制的数据会被存储在外部off-page storage(就是上面提到的overflow page)。</p>
<p>而可变长度列是如何存储在off-page storage中的，又跟行格式的不同而不同：</p>
<ul>
<li>COMPACT 和 REDUNDANT行格式<br>在使用这两种行格式的情况下，当一个可变长度列被存储到外部的off-page storage中时，InnoDB引擎会把这一列的前768个字节存储在page中，剩下的数据存储在overflow pages中。每一个存储在overflow pages中的可变长度列都有一个自己的overflow pages列表。这768个字节中，有20字节用来存储这个列的真实长度和指向包含指向overflow list的指针。</li>
<li>DYNAMIC和COMPRESSED行格式<br>在使用这两种行格式的情况下，当一个可变长度列被存储到外部的off-page storage中时，InnoDB引擎会在page中存储一个20字节的指针，列中的剩余数据会全部存储到overflow pages中。</li>
</ul>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>在使用InnoDB建表时，默认的行格式为COMPACT（可通过show variables like “table_name”查看），这种行格式对应的默认page大小为16KB，那么相应每行的大小不能超过8KB。如果表中有20个列都为text类型，而且每个text类型列的值都超过了768字节，那么20 * 768字节=15360字节=15KB远大于8KB，所以必然会报错！<br>那么解决这个问题的方法就是修改行格式，以下是启用DYNAMIC行格式的步骤：</p>
<ol>
<li><p>首先是MySQL配置文件my.cnf中添加两个配置项：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">innodb_file_per_table=1 //</span><br><span class="line">innodb_file_format = Barracuda //DYNAMIC行格式只有在Barracuda文件格式下才支持</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改表行格式ROW_FORMAT</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name ROW_FORMAT=DYNAMIC;</span><br></pre></td></tr></table></figure>
<p>修改之后，执行 show table status like ‘table_name’，可以看到Row_format这一列对应的值已经变成了dynamic，再写入数据的时候就不会报错了。</p>
</li>
</ol>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://dev.mysql.com/doc/mysql-reslimits-excerpt/5.7/en/column-count-limit.html">https://dev.mysql.com/doc/mysql-reslimits-excerpt/5.7/en/column-count-limit.html</a></li>
<li><a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-row-format.html">https://dev.mysql.com/doc/refman/5.7/en/innodb-row-format.html</a></li>
<li><a href="https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_dynamic_row_format">https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_dynamic_row_format</a></li>
</ol>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL是如何通过变量实现组内排序的(MySQL 8.0之前版本，非row_number函数)</title>
    <url>/2020/10/11/MySQL%E6%98%AF%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E5%8F%98%E9%87%8F%E5%AE%9E%E7%8E%B0%E7%BB%84%E5%86%85%E6%8E%92%E5%BA%8F%E7%9A%84-MySQL-8-0%E4%B9%8B%E5%89%8D%E7%89%88%E6%9C%AC%EF%BC%8C%E9%9D%9Erow-number%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>业务中要实现这样的逻辑：有一张用户表，每个用户有一个或多个银行账户，每个账户有对应的流水数据，现在想要获取每个用户的每个账户下，交易金额最大的前10条流水数据，该如何实现？</p>
<a id="more"></a>

<h1 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h1><p>组内排序取Top N在业务当中是一个会经常用到的功能。如果你是在大数据框架中取组内Top N，一般的大数据框架都提供了现成的函数，比如Hive中的row_number() + partitionBy()，实现起来会比较简单。而如果你用的是关系型数据库，比如MySQL，就没有类似现成的函数来实现这个功能了(MySQL 8.0开始也提供row_number函数，但是很多公司都还是使用的MySQL5.6/5.7)。</p>
<p>那么在MySQL 8.0 之前如何实现组内排序呢？答案就是，借助变量。</p>
<p>表(bank_transaction)中部分列：<br>|列名| 类型 | 含义 |<br>|–|–|–|<br>| user_id | varchar(64) | 用户ID |<br>| bank_account | varchar(64) | 银行账户 |<br>| transaction_amount | decimal(17, 2) | 交易金额 |</p>
<p>SQL实现组内求Top N：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	user_id,</span><br><span class="line">	bank_account,</span><br><span class="line">	transaction_amount</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">	<span class="keyword">select</span></span><br><span class="line">		user_id,</span><br><span class="line">		bank_account,</span><br><span class="line">		transaction_amount,</span><br><span class="line">		@row_num := <span class="keyword">if</span>(@group01 = user_id <span class="keyword">and</span> @group02 = bank_account, @row_num + <span class="number">1</span>, <span class="number">1</span>) <span class="keyword">as</span> rn,</span><br><span class="line">		@group01 := user_id,</span><br><span class="line">		@group02 := bank_account	</span><br><span class="line">	<span class="keyword">from</span> (</span><br><span class="line">		<span class="keyword">select</span></span><br><span class="line">			user_id,</span><br><span class="line">			bank_account,</span><br><span class="line">			transaction_amount</span><br><span class="line">		<span class="keyword">from</span> bank_transaction </span><br><span class="line">		<span class="keyword">order</span> <span class="keyword">by</span> user_id, bank_account, transaction_amount <span class="keyword">desc</span></span><br><span class="line">	) t1, (<span class="keyword">select</span> @row_num := <span class="number">0</span>, @group01 := <span class="string">&quot;&quot;</span>, @group02 := <span class="string">&quot;&quot;</span>)t2</span><br><span class="line">) t</span><br><span class="line"><span class="keyword">where</span> t.rn &lt;= <span class="number">6</span></span><br></pre></td></tr></table></figure>
<p>实现思路：</p>
<ol>
<li>第一步：对数据进行组内排序，t1这个子查询里面就是对每个用户的每个账户下的所有交易金额进行降序排列</li>
<li>第二步：设置变量，@row_num变量保存的是组内排序时的编号，@group01和@group02保存的是要分组列的临时值</li>
<li>第三步：遍历已排序数据，每次拿后一行与前一行进行比较，如果后一行分组列的值和前一行的相等（对应上面SQL语句中的if条件），就表明后一行与前一行属于同一个组，那么变量@row_num的值就加一；如果不相等，就从1重新开始。以此，给所有数据编号。</li>
<li>第四步：给所有数据编号之后，通过where条件即可取出Top N的记录。</li>
</ol>
<p>另外，SQL也可以这样写，主要是变量的使用方式不同：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> @row_num := <span class="number">0</span>, @group01 := <span class="string">&quot;&quot;</span>, @group02 := <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">	user_id,</span><br><span class="line">	bank_account,</span><br><span class="line">	transaction_amount</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">	<span class="keyword">select</span></span><br><span class="line">		user_id,</span><br><span class="line">		bank_account,</span><br><span class="line">		transaction_amount,</span><br><span class="line">		@row_num := <span class="keyword">if</span>(@group01 = user_id <span class="keyword">and</span> @group02 = bank_account, @row_num + <span class="number">1</span>, <span class="number">1</span>) <span class="keyword">as</span> rn,</span><br><span class="line">		@group01 := user_id,</span><br><span class="line">		@group02 := bank_account	</span><br><span class="line">	<span class="keyword">from</span> (</span><br><span class="line">		<span class="keyword">select</span></span><br><span class="line">			user_id,</span><br><span class="line">			bank_account,</span><br><span class="line">			transaction_amount</span><br><span class="line">		<span class="keyword">from</span> bank_transaction </span><br><span class="line">		<span class="keyword">order</span> <span class="keyword">by</span> user_id, bank_account, transaction_amount <span class="keyword">desc</span></span><br><span class="line">	) t1</span><br><span class="line">) t</span><br><span class="line"><span class="keyword">where</span> t.rn &lt;= <span class="number">6</span></span><br></pre></td></tr></table></figure>

<p>Hive中如何实现组内排序求Top N，可以参考<a href="https://editor.csdn.net/md/?articleId=107349381">这里</a>。</p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL表同步到Hive</title>
    <url>/2020/10/11/MySQL%E8%A1%A8%E5%90%8C%E6%AD%A5%E5%88%B0Hive/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Python for循环中添加字典到列表，结果列表中全是循环中的最后一个值</title>
    <url>/2020/10/11/Python-for%E5%BE%AA%E7%8E%AF%E4%B8%AD%E6%B7%BB%E5%8A%A0%E5%AD%97%E5%85%B8%E5%88%B0%E5%88%97%E8%A1%A8%EF%BC%8C%E7%BB%93%E6%9E%9C%E5%88%97%E8%A1%A8%E4%B8%AD%E5%85%A8%E6%98%AF%E5%BE%AA%E7%8E%AF%E4%B8%AD%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AA%E5%80%BC/</url>
    <content><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>问题的是在Python解析Excel文件出现的。</p>
<a id="more"></a>

<p>下面是出现问题的一段代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 此函数实现了解析Excel文件，并将文件中的每行数据以字典的形式返回(key为列名，value为对应的值)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span>():</span></span><br><span class="line">	...</span><br><span class="line">	<span class="keyword">yield</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将解析返回的每行数据追加到一个列表中</span></span><br><span class="line">list = []</span><br><span class="line"><span class="keyword">for</span> dic <span class="keyword">in</span> func():</span><br><span class="line">	list.append(dic)</span><br><span class="line"></span><br><span class="line">print(list)</span><br></pre></td></tr></table></figure>
<p>上面代码执行之后，输出的list中，发现所有元素的内容都是相同的，再去对比源文件，发现列表的元素都为Excel文件中最后一行内容。很明显，这肯定是不对的！</p>
<h1 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h1><p>出现这个问题之后，首先想到的就是逐步调试这段循环代码，同时观察dic的值和list中元素的值。</p>
<ul>
<li>第一次循环时，list中对应的确实是Excel中第一行的内容，</li>
<li>第二次循环时，在dic已改变，但还没往list中追加第二个元素之前，发现第一次追加到list中的的元素的值已经发生了改变，而且改变之后的内容就和第二次循环时dic的内容相同。</li>
</ul>
<p>到这里，就恍然大悟了，原来每次往列表中追加的都是这个字典的引用，而不是字典的内容，所以才会在每次循环时，出现只要dic的内容发生了变化，list中之前追加进去的dic也随着发生改变的情况。</p>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>知道了这个原因，我们就知道在追加列表时，针对Excel的每行数据，都应该追加不同的引用。</p>
<p>那么具体的实现就是用到字典的copy()方法，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list = []</span><br><span class="line"><span class="keyword">for</span> dic <span class="keyword">in</span> func():</span><br><span class="line">	list.append(dic.copy())</span><br></pre></td></tr></table></figure>
<p>copy()方法对字典dic进行了一个浅拷贝，也就是说对字典创建了一个新的引用。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://docs.python.org/3.7/library/copy.html">https://docs.python.org/3.7/library/copy.html</a></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title>Python解析Excel、PDF、HTML、CSV...文件</title>
    <url>/2020/10/11/Python%E8%A7%A3%E6%9E%90Excel%E3%80%81PDF%E3%80%81HTML%E3%80%81CSV-%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Python使用POP3协议客户端poplib登录邮箱并解压缩zip、rar压缩包</title>
    <url>/2020/10/11/Python%E4%BD%BF%E7%94%A8POP3%E5%8D%8F%E8%AE%AE%E5%AE%A2%E6%88%B7%E7%AB%AFpoplib%E7%99%BB%E5%BD%95%E9%82%AE%E7%AE%B1%E5%B9%B6%E8%A7%A3%E5%8E%8B%E7%BC%A9zip%E3%80%81rar%E5%8E%8B%E7%BC%A9%E5%8C%85/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Python读写Phoenix</title>
    <url>/2020/10/11/Python%E8%AF%BB%E5%86%99Phoenix/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Python读写Elasticsearch、HBase、Hive</title>
    <url>/2020/10/11/Python%E8%AF%BB%E5%86%99Elasticsearch%E3%80%81HBase%E3%80%81Hive/</url>
    <content><![CDATA[<h2 id="1-Elasticsearch"><a href="#1-Elasticsearch" class="headerlink" title="1. Elasticsearch"></a>1. Elasticsearch</h2><p>安装Elasticsearch模块：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip3 install elasticsearch==7.1.0</span><br></pre></td></tr></table></figure>

<p>读取ES索引的代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> elasticsearch <span class="keyword">import</span> Elasticsearch</span><br><span class="line"></span><br><span class="line">es = Elasticsearch(hosts=[&#123;<span class="string">&quot;host&quot;</span>: <span class="string">&quot;host&quot;</span>, <span class="string">&quot;port&quot;</span>: <span class="number">9200</span>&#125;])</span><br><span class="line">result = es.search(index=<span class="string">&quot;index_name&quot;</span>, body=&#123;<span class="string">&quot;query&quot;</span>: &#123;<span class="string">&quot;match_all&quot;</span>: &#123;&#125;&#125;&#125;)</span><br><span class="line"><span class="keyword">for</span> hit <span class="keyword">in</span> result[<span class="string">&#x27;hits&#x27;</span>][<span class="string">&#x27;hits&#x27;</span>]:</span><br><span class="line">    print(hit[<span class="string">&quot;_source&quot;</span>])</span><br></pre></td></tr></table></figure>

<h2 id="2-HBase"><a href="#2-HBase" class="headerlink" title="2. HBase"></a>2. HBase</h2><p>安装Python连接HBase的模块：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip3 install happybase</span><br></pre></td></tr></table></figure>

<p>读取HBase表的代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> happybase</span><br><span class="line"></span><br><span class="line">connection = happybase.Connection(<span class="string">&#x27;host&#x27;</span>)</span><br><span class="line">connection.open()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印所有的表</span></span><br><span class="line">print(connection.tables())</span><br><span class="line"></span><br><span class="line">table = connection.table(<span class="string">&#x27;table_name&#x27;</span>)</span><br><span class="line">print(table.families())</span><br><span class="line"></span><br><span class="line">row = table.row(<span class="string">&#x27;row_key&#x27;</span>, columns=[<span class="string">b&#x27;cf:col1&#x27;</span>, <span class="string">b&#x27;cf:col2&#x27;</span>, <span class="string">b&#x27;cf:col3&#x27;</span>])</span><br><span class="line">print(bytes(row[<span class="string">b&#x27;cf:col1&#x27;</span>]).decode(<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line"></span><br><span class="line">connection.close()</span><br></pre></td></tr></table></figure>

<h2 id="3-Hive"><a href="#3-Hive" class="headerlink" title="3. Hive"></a>3. Hive</h2><p>安装Python连接Hive的模块：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip3 install pyhive</span><br></pre></td></tr></table></figure>
<p>如果安装pyhive过程出错，请参考<a href="https://blog.csdn.net/lovetechlovelife/article/details/97128463">pyhive依赖模块sasl安装(pip install sasl)报错Microsoft Visual C++ 14.0 is required</a>。</p>
<p>读取Hive表的代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyhive <span class="keyword">import</span> hive</span><br><span class="line"></span><br><span class="line">conn = hive.Connection(host=<span class="string">&quot;master-1&quot;</span>, port=<span class="number">10000</span>, username=<span class="string">&quot;root&quot;</span>, auth=<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line">cursor.execute(<span class="string">&quot;select * from db.table limit 10&quot;</span>)</span><br><span class="line">print(cursor.fetchall())</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Elasticsearch</tag>
        <tag>HBase</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Connector连接器之整合读写Elasticsearch</title>
    <url>/2020/10/11/Spark-Connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99Elasticsearch/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Spark Connector连接器之整合读写HBase</title>
    <url>/2020/10/11/Spark-Connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99HBase/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Spark Connector连接器之整合读写Phoenix</title>
    <url>/2020/10/11/Spark-Connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99Phoenix/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Spark connector连接器之整合读写MySQL及问题汇总</title>
    <url>/2020/10/11/Spark-connector%E8%BF%9E%E6%8E%A5%E5%99%A8%E4%B9%8B%E6%95%B4%E5%90%88%E8%AF%BB%E5%86%99MySQL%E5%8F%8A%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Spark Dataframe转成HashMap</title>
    <url>/2020/10/11/Spark-Dataframe%E8%BD%AC%E6%88%90HashMap/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Spark DataFrame导出为Excel文件</title>
    <url>/2020/10/11/Spark-DataFrame%E5%AF%BC%E5%87%BA%E4%B8%BAExcel%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Spark中一行代码转换DataFrame所有列的类型</title>
    <url>/2020/10/10/Spark%E4%B8%AD%E4%B8%80%E8%A1%8C%E4%BB%A3%E7%A0%81%E8%BD%AC%E5%8C%96DataFrame%E6%89%80%E6%9C%89%E5%88%97%E7%9A%84%E7%B1%BB%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>当需要把DataFrame中所有列的类型转换成另外一种类型，并且这个DataFrame中字段很多，一个一个地转换要写很多冗余代码，那么就可以使用如下这两种转换方式。</p>
<a id="more"></a>

<h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><h2 id="1-foldLeft-函数"><a href="#1-foldLeft-函数" class="headerlink" title="1. foldLeft() 函数"></a>1. foldLeft() 函数</h2><p>代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.DataFrame</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.col</span><br><span class="line"></span><br><span class="line">val df: DataFrame = ...</span><br><span class="line">val columns: Array[String] = df.columns</span><br><span class="line">val df2: DataFrame = columns.foldLeft(df)&#123;(currentDF, column) =&gt; currentDF.withColumn(column, col(column).cast(<span class="string">&quot;string&quot;</span>))&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>变量columns</strong>：是一个String类型的数组，数组中的元素为df中的列名。</li>
<li><strong>foldLeft函数</strong>：此函数以df为初始值，从左向右遍历columns数组，并把df的每一行和columns的每个元素作为参数传入foldLeft后面的函数中(也就是foldLeft后面的大括号中)。</li>
<li>**withColumn()**：将每一列转换成String类型并赋值给当前列。如果存在同名的列，withColumn函数默认会进行覆盖。</li>
</ul>
<h2 id="2-map-函数"><a href="#2-map-函数" class="headerlink" title="2. map() 函数"></a>2. map() 函数</h2><p>代码如下</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;Column, DataFrame&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.col</span><br><span class="line"></span><br><span class="line">val df: DataFrame = ...</span><br><span class="line">val columns: Array[String] = df.columns</span><br><span class="line">val arrayColumn: Array[Column] = columns.map(column =&gt; col(column).cast(<span class="string">&quot;string&quot;</span>))</span><br><span class="line">val df2: DataFrame = df.select(arrayColumn :_*)</span><br></pre></td></tr></table></figure>
<p>通过map函数将columns中的每一列转换成String类型，并返回一个Column类型的数组，然后，将arrayColumn数组中的每个元素作为参数传入select函数中，就相当于df.select(col1, col2, col3, …)。</p>
<p>除此之外，这种写法还有一个很有用的场景：比如要在一个DataFrame中select出很多列(假如有几十个几百个)，如果要一个个显示写出来，既不方便又会让代码显得很冗余，那么就可以使用这种写法。</p>
<ol>
<li>先通过df.columns得到这个DataFrame中的所有列，返回一个包含所有列的数组；</li>
<li>再使用Scala中的这种语法进行查询df.select(arrayColumn :_*)，非常简洁明了。</li>
</ol>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark内存管理(静态内存管理和统一内存管理)</title>
    <url>/2020/10/11/Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86-%E9%9D%99%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%92%8C%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Spark性能优化指南(官网文档)</title>
    <url>/2020/10/11/Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97-%E5%AE%98%E7%BD%91%E6%96%87%E6%A1%A3/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>YARN/Hadoop/HDFS常用命令</title>
    <url>/2020/10/11/YARN-Hadoop-HDFS%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Vim编辑器常用操作</title>
    <url>/2020/10/11/Vim%E7%BC%96%E8%BE%91%E5%99%A8%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>pyhive依赖模块sasl安装(pip install sasl)报错Microsoft Visual C++ 14.0 is required</title>
    <url>/2020/10/11/pyhive%E4%BE%9D%E8%B5%96%E6%A8%A1%E5%9D%97sasl%E5%AE%89%E8%A3%85-pip-install-sasl-%E6%8A%A5%E9%94%99Microsoft-Visual-C-14-0-is-required/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>pymysql + DBUtils实现数据库连接池及数据批量读写</title>
    <url>/2020/10/11/pymysql-DBUtils%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E6%B1%A0%E5%8F%8A%E6%95%B0%E6%8D%AE%E6%89%B9%E9%87%8F%E8%AF%BB%E5%86%99/</url>
    <content><![CDATA[<h2 id="1-安装pymysql、DBUtils"><a href="#1-安装pymysql、DBUtils" class="headerlink" title="1. 安装pymysql、DBUtils"></a>1. 安装pymysql、DBUtils</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple pymysql</span><br><span class="line">pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple DBUtils</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h2 id="2-创建连接池"><a href="#2-创建连接池" class="headerlink" title="2. 创建连接池"></a>2. 创建连接池</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> DBUtils.PooledDB <span class="keyword">import</span> PooledDB, PooledDBError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_pool</span>():</span></span><br><span class="line">	pool = PooledDB(</span><br><span class="line">	    creator=pymysql,  <span class="comment"># 数据库连接对象</span></span><br><span class="line">	    host=<span class="string">&quot;host&quot;</span>,</span><br><span class="line">	    user=<span class="string">&quot;user&quot;</span>,</span><br><span class="line">	    password=<span class="string">&quot;password&quot;</span>,</span><br><span class="line">	    database=<span class="string">&quot;database&quot;</span>,</span><br><span class="line">	    charset=<span class="string">&quot;charset&quot;</span>,</span><br><span class="line">	    blocking=<span class="literal">True</span>,  <span class="comment"># 超过最大连接数时该如何处理后来的任务。设置为true就表示阻塞等待直达有空闲连接</span></span><br><span class="line">	    maxconnections=<span class="number">2</span>,  <span class="comment"># 连接池所能允许的最大连接数</span></span><br><span class="line">	    mincached=<span class="number">1</span>,  <span class="comment"># 最小初始化空闲连接数</span></span><br><span class="line">	    autocommit=<span class="literal">True</span>  <span class="comment"># 是否要自动提交</span></span><br><span class="line">	)</span><br><span class="line">	<span class="keyword">return</span> pool.connection()</span><br></pre></td></tr></table></figure>

<h2 id="3-读数据"><a href="#3-读数据" class="headerlink" title="3. 读数据"></a>3. 读数据</h2><h3 id="1-只读一行"><a href="#1-只读一行" class="headerlink" title="1. 只读一行"></a>1. 只读一行</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_one</span>(<span class="params">self, conn, sql</span>):</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line">        cursor.execute(sql)</span><br><span class="line">        <span class="keyword">return</span> cursor.fetchone()[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        conn.close()</span><br><span class="line">        cursor.close()</span><br></pre></td></tr></table></figure>

<h3 id="2-批量读取"><a href="#2-批量读取" class="headerlink" title="2. 批量读取"></a>2. 批量读取</h3><p>批量读取的场景，如果要返回的数据量比较大，为了避免给数据库造成太大负载和占用过多带宽资源，可以选择分批读取数据。比如要读取10万条数据，每批只读2000条。</p>
<p>思路：</p>
<ol>
<li>先通过一条SQL获取要读取的数据行数</li>
<li>每读取一批之后，自增主键减去2000</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sql_count = <span class="string">&quot;select id from db.table where id between 25000 and 125000&quot;</span></span><br><span class="line">sql = <span class="string">&quot;select col1, col2, col3 from db.table&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_many</span>(<span class="params">self, conn, sql_count, sql, size</span>):</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 计算要读取的数据行数</span></span><br><span class="line">        cursor.execute(sql_count)</span><br><span class="line">        cursor.fetchall()</span><br><span class="line">        row_count = cursor.rowcount</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 分配读取数据</span></span><br><span class="line">        cursor.execute(sql)</span><br><span class="line">        <span class="keyword">while</span> row_count &gt; <span class="number">0</span>:</span><br><span class="line">            records = cursor.fetchmany(size)</span><br><span class="line">            row_count -= size</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> records</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        conn.close()</span><br><span class="line">        cursor.close()</span><br></pre></td></tr></table></figure>


<h2 id="4-写数据"><a href="#4-写数据" class="headerlink" title="4. 写数据"></a>4. 写数据</h2><p>如果可以，尽量进行批量写，因为executemany()方法在多行插入和更新的场景下提升写性能。否则，就等于是循环执行execute()方法一条条的写，性能较差。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">execute_many</span>(<span class="params">self, conn, sql, args</span>):</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 从连接池中获取一个连接</span></span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line">        cursor.executemany(sql, args)</span><br><span class="line">    <span class="keyword">except</span> PooledDBError <span class="keyword">as</span> e:</span><br><span class="line">        logger.error(<span class="string">&quot;Failed to insert records into MySQL table &#123;&#125;&quot;</span>.format(e))</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        conn.close()</span><br><span class="line">        cursor.close()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>Python</tag>
        <tag>连接池</tag>
      </tags>
  </entry>
  <entry>
    <title>pyspark中遇到的坑 (持续更新)</title>
    <url>/2020/10/11/pyspark%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>图解正则表达式中的贪婪模式和非贪婪模式</title>
    <url>/2020/10/11/%E5%9B%BE%E8%A7%A3%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%B8%AD%E7%9A%84%E8%B4%AA%E5%A9%AA%E6%A8%A1%E5%BC%8F%E5%92%8C%E9%9D%9E%E8%B4%AA%E5%A9%AA%E6%A8%A1%E5%BC%8F/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>单词统计WordCount (Scala/Python/Java)</title>
    <url>/2020/10/11/%E5%8D%95%E8%AF%8D%E7%BB%9F%E8%AE%A1WordCount-Scala-Python-Java/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>手把手教你维度数据建模</title>
    <url>/2020/10/11/%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E7%BB%B4%E5%BA%A6%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>数据仓库架构、维度数据建模、雪花模型和星型模型</title>
    <url>/2020/10/11/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84%E3%80%81%E7%BB%B4%E5%BA%A6%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E3%80%81%E9%9B%AA%E8%8A%B1%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%98%9F%E5%9E%8B%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>本地Spark连接远程集群Hive(Scala/Python)</title>
    <url>/2020/10/11/%E6%9C%AC%E5%9C%B0Spark%E8%BF%9E%E6%8E%A5%E8%BF%9C%E7%A8%8B%E9%9B%86%E7%BE%A4Hive-Scala-Python/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark Hive</tag>
      </tags>
  </entry>
</search>
